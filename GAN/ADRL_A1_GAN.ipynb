{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lusLqM2-z6e1"
   },
   "source": [
    "**GROUP 1 : ASSIGNMENT 1 : GAN**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8FdijRaTJLmO"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import zipfile\n",
    "import random\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader,random_split,Tensor_Dataset\n",
    "from scipy.linalg import sqrtm\n",
    "from torchvision import transforms, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FEbNfAgxacvW",
    "outputId": "a3c7d15b-b535-4876-d62e-ed25a8ff687e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ESyLfvysTjyE"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4z_H0WhjnK9T"
   },
   "source": [
    "**Organising Data for Butterfly and Animal dataset and also Data Augmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ero1T2bEJWFP"
   },
   "outputs": [],
   "source": [
    "# We convert the entire dataset into a torch tensor of shape (90,60,3,128,128) and work with this file\n",
    "# transforms.ToTensor also scales pixel values to be in [0,1]\n",
    "folder_path = \"/content/drive/MyDrive/adrl/training_images.pt\"\n",
    "transform = transforms.Compose([transforms.Resize((128, 128)),transforms.ToTensor()])\n",
    "\n",
    "# Function to load images as tensors and resize as well\n",
    "def load_images_as_tensors(folder):\n",
    "    all_images = []\n",
    "    i = 0\n",
    "    # Iterate over subfolders\n",
    "    for subdir, _, files in os.walk(folder):\n",
    "        print(i)\n",
    "        i += 1\n",
    "        subfolder_images = []\n",
    "        # Sort files to maintain consistent order\n",
    "        files.sort()\n",
    "        for file in files:\n",
    "            file_path = os.path.join(subdir, file)\n",
    "            try:\n",
    "                # Load a single image,resize and then and convert to tensor\n",
    "                img = Image.open(file_path)\n",
    "                img_tensor = transform(img)\n",
    "                subfolder_images.append(img_tensor)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path}: {e}\")\n",
    "        if subfolder_images:\n",
    "            # Broadcast across all the classes of images\n",
    "            all_images.append(torch.stack(subfolder_images))\n",
    "    return torch.stack(all_images)\n",
    "    # Expected shape is (90, 60, 3, 128, 128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FtwLGLYPtoiV"
   },
   "outputs": [],
   "source": [
    "# Butterfly dataset\n",
    "# train has 6499 images from 75 classes\n",
    "# test has 2786 images\n",
    "\n",
    "# Convert the image to a tensor and normalize it between 0 and 1\n",
    "transform = transforms.Compose([transforms.Resize((128, 128)),transforms.ToTensor()])\n",
    "# Load CSV containing filenames and labels\n",
    "df = pd.read_csv(r\"C:\\ADRL data\\Butterfly dataset\\Training_set.csv\")\n",
    "N = len(df)\n",
    "# Create a mapping of distinct labels to indices\n",
    "class_names = sorted(df['label'].unique())\n",
    "class_to_idx = {class_name: idx for idx, class_name in enumerate(class_names)}\n",
    "\n",
    "# Save the class_to_idx dictionary to a JSON file\n",
    "with open(r\"C:\\ADRL data\\Butterfly dataset\\class_to_idx.json\", 'w') as f:\n",
    "    json.dump(class_to_idx, f)\n",
    "with open(r\"C:\\ADRL data\\Butterfly dataset\\class_to_idx.json\", 'r') as f:\n",
    "    loaded_class_to_idx = json.load(f)\n",
    "print(loaded_class_to_idx)\n",
    "\n",
    "# Initialize a tensor to store the images\n",
    "# Shape: (N, 3 (channels), 128 (H), 128 (W))\n",
    "image_tensor = torch.zeros((N, 3, 128, 128))\n",
    "# Initialize a tensor to store class indices\n",
    "# Shape: (N,)\n",
    "class_indices = torch.zeros(N, dtype=torch.long)  # Store class indices\n",
    "\n",
    "# Process each image and store it in the appropriate class index\n",
    "for i, row in df.iterrows():\n",
    "    print(i)\n",
    "    img_path = os.path.join(r\"C:\\ADRL data\\Butterfly dataset\\train\", row['filename'])  # Assuming images are in the 'train' folder\n",
    "    label = row['label']\n",
    "\n",
    "    # Load the image\n",
    "    img = Image.open(img_path).convert('RGB')  # Ensure image is in RGB mode\n",
    "    # (3,224,224)\n",
    "\n",
    "    # Apply transformations\n",
    "    img_tensor = transform(img)\n",
    "    # Get class index\n",
    "    class_idx = class_to_idx[label]\n",
    "\n",
    "    # Store the image tensor and class index\n",
    "    image_tensor[i] = img_tensor\n",
    "    class_indices[i] = class_idx\n",
    "\n",
    "torch.save(image_tensor, r\"C:\\ADRL data\\Butterfly dataset\\butterfly_training_images.pth\")\n",
    "torch.save(class_indices, r\"C:\\ADRL data\\Butterfly dataset\\butterfly_training_classindices.pth\")\n",
    "# Now, image_tensor contains the (N, 3, 128, 128) tensor.\n",
    "print(f'Successfully created tensor with shape: {image_tensor.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NA1DPashANNZ"
   },
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "# Note that variable name change wrt A1\n",
    "animal_images = torch.load(\"/home/sahapthank/saha_adrl/training_images.pt\")\n",
    "butterfly_images = torch.load(\"/home/sahapthank/saha_adrl/butterfly_training_images.pth\")\n",
    "butterfly_labels = torch.load(\"/home/sahapthank/saha_adrl/butterfly_training_classindices.pth\")\n",
    "anime_images = torch.load(\"/home/sahapthank/saha_adrl/anime_faces_images.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6TbbBzwsyoUO"
   },
   "outputs": [],
   "source": [
    "# DATA AUGMENTATION FOR ANIMAL\n",
    "horizontal_flip = transforms.RandomHorizontalFlip(p=1)    # Randomly flip the image horizontally\n",
    "rotation = transforms.RandomRotation(degrees=20)    # Random rotation with expanding size\n",
    "color_jitter = transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)  # Random color adjustments\n",
    "\n",
    "tensor_shape = (90,60,3,128,128)\n",
    "flipped_images = torch.zeros(tensor_shape)\n",
    "rotated_images = torch.zeros(tensor_shape)\n",
    "color_jittered_images = torch.zeros(tensor_shape)\n",
    "\n",
    "for i in range(90):\n",
    "    for j in range(60):\n",
    "        flipped_images[i][j] = horizontal_flip(animal_images[i][j])\n",
    "        rotated_images[i][j] = rotation(animal_images[i][j])\n",
    "        color_jittered_images[i][j] = color_jitter(animal_images[i][j])\n",
    "\n",
    "animal_images_augmented = torch.stack([animal_images,flipped_images,rotated_images,color_jittered_images])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O5RfPEwptN30"
   },
   "outputs": [],
   "source": [
    "# DATA AUGMENTATION FOR BUTTERFLY\n",
    "horizontal_flip = transforms.RandomHorizontalFlip(p=1)    # Randomly flip the image horizontally\n",
    "rotation = transforms.RandomRotation(degrees=20)    # Random rotation with expanding size\n",
    "color_jitter = transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)  # Random color adjustments\n",
    "\n",
    "tensor_shape = (6499,3,128,128)\n",
    "flipped_images = torch.zeros(tensor_shape)\n",
    "rotated_images = torch.zeros(tensor_shape)\n",
    "color_jittered_images = torch.zeros(tensor_shape)\n",
    "\n",
    "for i in range(6499):\n",
    "    flipped_images[i] = horizontal_flip(butterfly_images[i])\n",
    "    rotated_images[i] = rotation(butterfly_images[i])\n",
    "    color_jittered_images[i]= color_jitter(butterfly_images[i])\n",
    "\n",
    "butterfly_images_augmented = torch.stack([butterfly_images,flipped_images,rotated_images,color_jittered_images])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hzThgrNItOFg"
   },
   "outputs": [],
   "source": [
    "# CHANGE THIS CODE BASED ON DATASET\n",
    "training_images = torch.reshape(animal_images,(5400,3,128,128))\n",
    "# Convert values from [0, 1] to [-1, 1]\n",
    "training_images = ((2 * training_images) - 1)\n",
    "print(training_images.size())\n",
    "first_image = (training_images[0] + 1)/2\n",
    "first_image = first_image.permute(1, 2, 0)\n",
    "plt.imshow(first_image)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "print(torch.min(training_images))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TaWT6MdaPuZh"
   },
   "source": [
    "**Functions to evaluate conjugate of various F-divergences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ko8nNpYEPt1D"
   },
   "outputs": [],
   "source": [
    "# u is real-valued!\n",
    "# While training GAN using Variational Divergence Minimisation and f-divergences\n",
    "# The loss term only uses the conjugate of the generator of the f-divergence\n",
    "# This function is for computing f*(u) where u would be output of critic and f* is the conjugate of f\n",
    "\n",
    "epsilon = 1e-15  # Small constant for numerical stability\n",
    "def JSD(u):\n",
    "    return -torch.log(2 - torch.exp(u))\n",
    "def Usual_GAN(u):\n",
    "    # Generally if we dont use framework of f* then (BCE + sigmoid)\n",
    "    # If we want to use f* framework we need to use (sigmoid + another_activation {g_f mentioned in paper})\n",
    "    # For Usual_GAN loss without f* is simpler but with f* is the general framework\n",
    "    # Below we do without f* using (BCE + sigmoid). For that we only need log\n",
    "    return (torch.log(u))\n",
    "def total_variation(u):\n",
    "    return (u)\n",
    "def KL(u):\n",
    "    return (torch.exp(u - 1))\n",
    "def Reverse_KL(u):\n",
    "    return (-torch.log(-u) - 1)\n",
    "def Pearson(u):\n",
    "    return ((u ** 2) / 4 + u)\n",
    "def Neyman(u):\n",
    "    return (2 * (1 - torch.sqrt(1 - u)))\n",
    "def Squared_Hellinger(u):\n",
    "    return (u / (1 - u))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygz9AYOs6Ivm"
   },
   "source": [
    "**[Q1, Q2 AND Q3] Vanilla DC-GAN Implementation for 2 datasets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aEXjjMjgJcXI"
   },
   "source": [
    "**CRITIC NETWORK (Denote by D/T)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9WD9gdNp_MnU"
   },
   "source": [
    "* Below is a DC-GAN where the Critic does not have any FFNN and relies only on Strided Convolutions(instead of Pooling) to continuously downsample the image input to a single real value (which is converted to Probability using Sigmoid)\n",
    "* BatchNormalisation and Activation functions are chosen as in the DC-GAN paper and there are no skip connections\n",
    "* The class instantiation is made flexible by making the architectural parameters as inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vzy_DdzZJerP"
   },
   "outputs": [],
   "source": [
    "class Critic_DCGAN(nn.Module):\n",
    "    def __init__(self,architecture):\n",
    "        # architecture = [conv_params , use_batchnorm, activation_fn]\n",
    "        # conv_params = [[in_channels,output_channels,kernel_size,stride,padding],....[*,1,*,*,*]]\n",
    "        # use_batchnorm = [False,True,....False]\n",
    "        # activation_fn = [nn.Relu,nn.LeakyRelu,.....nn.sigmoid()]\n",
    "        super(Critic_DCGAN, self).__init__()\n",
    "        assert len(architecture[0]) == len(architecture[1]) == len(architecture[2])\n",
    "        self.conv_params = architecture[0]\n",
    "        self.use_batchnorm = architecture[1]\n",
    "        self.activation_fn = architecture[2]\n",
    "        layers = []\n",
    "        for j,i in enumerate(self.conv_params):\n",
    "            layers.append(nn.Conv2d(in_channels=i[0],out_channels=i[1],kernel_size=i[2],stride=i[3],padding=i[4]))\n",
    "            if (self.use_batchnorm)[j]:\n",
    "                layers.append(nn.BatchNorm2d(i[1]))\n",
    "            layers.append(self.activation_fn[j])\n",
    "        # The last conv2d layer outputs [1,1,1]\n",
    "        # Sigmoid is the last activation function\n",
    "        # the * operator is used to unpack a list or tuple into separate positional arguments\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    # Below function just asks the opinion of T\n",
    "    # Return value must be in dom(f*)\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    # We will use this to prevent calculating gradients wrt parameters of T\n",
    "    # During training the generator G\n",
    "    def set_requires_grad(self, requires_grad):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = requires_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sfMSj2h0Do4F"
   },
   "outputs": [],
   "source": [
    "# Example instantiation used in the DC-GAN paper where no BatchNormalisation at the beginning and end\n",
    "# Assuming input is [3,128,128] final output is [1024,4,4] then a final Conv2d to give [1,1,1]\n",
    "# output_size = ({input_size - kernel_size + 2*padding}/stride) + 1\n",
    "# TRIAL 1\n",
    "# conv_params_1 = [[3,64,4,2,1],[64,128,4,2,1],[128,256,4,2,1],[256,512,4,2,1],[512,1024,4,2,1],[1024,1,4,4,1]]\n",
    "# use_batchnorm_1 = [True for _ in range(5)] + [False]\n",
    "# activation_fn_1 = [nn.LeakyReLU(0.2, inplace=False) for _ in range(5)] +[nn.Sigmoid()]\n",
    "# architecture_1 = [conv_params_1,use_batchnorm_1,activation_fn_1]\n",
    "\n",
    "# TRIAL 2\n",
    "# conv_params_1= [[3,32,4,2,1],[32,64,1,1,0],[64,64,4,2,1],[64,128,4,4,0],[128,1,8,1,0]]\n",
    "# use_batchnorm_1 = [True for _ in range(4)] + [False]\n",
    "# activation_fn_1= [nn.LeakyReLU(0.2, inplace=False) for _ in range(4)] +[nn.Sigmoid()]\n",
    "# architecture_1= [conv_params_1,use_batchnorm_1,activation_fn_1]\n",
    "\n",
    "# TRIAL 3\n",
    "# conv_params_1 = [[3,32,4,2,1],[32,64,4,2,1],[64,64,4,2,1],[64,128,4,2,1],[128,256,4,2,1],[256,1,4,4,1]]\n",
    "# use_batchnorm_1 = [True for _ in range(5)] + [False]\n",
    "# activation_fn_1 = [nn.LeakyReLU(0.2, inplace=False) for _ in range(5)] +[nn.Sigmoid()]\n",
    "# architecture_1 = [conv_params_1,use_batchnorm_1,activation_fn_1]\n",
    "\n",
    "# TRIAL 4\n",
    "conv_params_1 = [[3,32,4,2,1],[32,64,4,2,1],[64,128,4,2,1],[128,128,4,2,1],[128,256,4,2,1],[256,1,4,4,1]]\n",
    "use_batchnorm_1 = [True for _ in range(5)] + [False]\n",
    "activation_fn_1 = [nn.LeakyReLU(0.2, inplace=False) for _ in range(5)] +[nn.Sigmoid()]\n",
    "architecture_1 = [conv_params_1,use_batchnorm_1,activation_fn_1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_GdMUvKtvnyE"
   },
   "outputs": [],
   "source": [
    "# Just checking\n",
    "T_1 = Critic_DCGAN(architecture_1)\n",
    "random_tensor = torch.randn((5,3,128,128))\n",
    "random_output = T_1(random_tensor)\n",
    "print(random_output.shape)\n",
    "print(random_output[0])\n",
    "TP1 = sum(p.numel() for p in T_1.parameters())\n",
    "print(TP1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MenANDjfJfBv"
   },
   "source": [
    "**GENERATOR NETWORK (Denote by G)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4enMqZxK8_3"
   },
   "source": [
    "* Strided 2D-Convolutional transpose layers are being used to up-sample latent space vectors to images\n",
    "*  Existence of the BatchNorm after the Conv-transpose layers is the critical contribution of the DCGAN paper which helps in the flow of the gradients during training\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "59LSooV6Jg9q"
   },
   "outputs": [],
   "source": [
    "class Generator_DCGAN(nn.Module):\n",
    "    def __init__(self, architecture):\n",
    "        super(Generator_DCGAN, self).__init__()\n",
    "        assert len(architecture[0]) == len(architecture[1]) == len(architecture[2])\n",
    "        self.transpose_conv_params = architecture[0]\n",
    "        self.use_batchnorm = architecture[1]\n",
    "        self.activation_fn = architecture[2]\n",
    "        layers = []\n",
    "        # Starting with input latent vector (e.g., size 100)\n",
    "        for j, i in enumerate(self.transpose_conv_params):\n",
    "            layers.append(nn.ConvTranspose2d(in_channels=i[0], out_channels=i[1], kernel_size=i[2], stride=i[3], padding=i[4]))\n",
    "            if self.use_batchnorm[j]:\n",
    "                layers.append(nn.BatchNorm2d(i[1]))\n",
    "            layers.append(self.activation_fn[j])\n",
    "        # tanh() us used as the final activation function\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    # Sampling images from the generator G\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    # We will use this to prevent calculating gradients wrt parameters of G\n",
    "    # During training the discriminator T\n",
    "    def set_requires_grad(self, requires_grad):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = requires_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K_QME9vdJmux"
   },
   "outputs": [],
   "source": [
    "# Example instantiation used in the DC-GAN paper where no BatchNormalisation at the beginning\n",
    "# Assuming input is [100,1,1] final output is [3,64,64]\n",
    "# output_size = {(input_size - 1) * stride} - (2*padding) + kernel_size}\n",
    "# TRIAL 1\n",
    "# transpose_conv_params_2 = [[100,512,4,1,0],[512,256,4,2,1],[256,128,4,2,1],[128,64,4,2,1],[64,32,4,2,1],[32,3,4,2,1]]\n",
    "# use_batchnorm_2 = [True for _ in range(5)] + [False]\n",
    "# activation_fn_2 = [nn.ReLU(inplace = False) for _ in range(5)] +[nn.Tanh()]\n",
    "# architecture_2 = [transpose_conv_params_2, use_batchnorm_2, activation_fn_2]\n",
    "\n",
    "# TRIAL 2\n",
    "# transpose_conv_params_2 = [[100,256,4,1,0],[256,128,4,2,1],[128,64,4,2,1],[64,32,4,2,1],[32,3,4,4,0]]\n",
    "# use_batchnorm_2 = [True for _ in range(4)] + [False]\n",
    "# activation_fn_2 = [nn.ReLU(inplace = False) for _ in range(4)] +[nn.Tanh()]\n",
    "# architecture_2 = [transpose_conv_params_2, use_batchnorm_2, activation_fn_2]\n",
    "\n",
    "# TRIAL 3\n",
    "transpose_conv_params_2 = [[200,512,4,1,0],[512,256,4,2,1],[256,128,4,2,1],[128,64,4,2,1],[64,32,4,2,1],[32,3,4,2,1]]\n",
    "use_batchnorm_2 = [True for _ in range(5)] + [False]\n",
    "activation_fn_2 = [nn.ReLU(inplace = False) for _ in range(5)] +[nn.Tanh()]\n",
    "architecture_2 = [transpose_conv_params_2,use_batchnorm_2,activation_fn_2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Zjh_E2pw-Wd"
   },
   "outputs": [],
   "source": [
    "# Just checking\n",
    "G_1 = Generator_DCGAN(architecture_2)\n",
    "random_tensor = torch.randn((5,100,1,1))\n",
    "random_output = G_1(random_tensor)\n",
    "print(random_output.shape)\n",
    "print(torch.max(random_output),torch.min(random_output))\n",
    "TP2 = sum(p.numel() for p in G_1.parameters())\n",
    "print(TP2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9RgWLF7QvD5B"
   },
   "source": [
    "**Generator using BilinearInterpolation and Conv2d**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "123zQPD3vLGA"
   },
   "outputs": [],
   "source": [
    "class Generator_BilinearUpsample(nn.Module):\n",
    "    def __init__(self, architecture):\n",
    "        super(Generator_BilinearUpsample, self).__init__()\n",
    "        assert len(architecture[0]) == len(architecture[1]) == len(architecture[2])\n",
    "        self.conv_params = architecture[0]\n",
    "        self.use_batchnorm = architecture[1]\n",
    "        self.activation_fn = architecture[2]\n",
    "        self.Bilinear_size = architecture[3]\n",
    "        layers = []\n",
    "        # Starting with input latent vector (e.g., size 100)\n",
    "        # Upsampling done on h,w input is (bs,c,h,w)\n",
    "        for j, i in enumerate(self.conv_params):\n",
    "            layers.append(nn.Upsample(scale_factor = self.Bilinear_size[j], mode='bilinear', align_corners=False))\n",
    "            layers.append(nn.Conv2d(in_channels=i[0],out_channels=i[1],kernel_size=i[2],stride=i[3],padding=i[4]))\n",
    "            if self.use_batchnorm[j]:\n",
    "                layers.append(nn.BatchNorm2d(i[1]))\n",
    "            layers.append(self.activation_fn[j])\n",
    "        # tanh() us used as the final activation function\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    # We will use this to prevent calculating gradients wrt parameters of G\n",
    "    # During training the discriminator T\n",
    "    def set_requires_grad(self, requires_grad):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = requires_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9p8InJh0yM2q"
   },
   "outputs": [],
   "source": [
    "# Example instantiation used in the DC-GAN paper where no BatchNormalisation at the beginning\n",
    "# Assuming input is [100,1,1] final output is [3,64,64]\n",
    "# output_size = ({input_size - kernel_size + 2*padding}/stride) + 1\n",
    "# We take (s=1 and k=3 to match 2p = 2)\n",
    "Bilinear_size_3 = [1,2,2,2,4,4]\n",
    "conv_params_3 = [[100,512,1,1,0],[512,256,3,1,1],[256,128,3,1,1],[128,64,3,1,1],[64,32,3,1,1],[32,3,3,1,1]]\n",
    "use_batchnorm_3 = [True for _ in range(5)] + [False]\n",
    "activation_fn_3 = [nn.ReLU(inplace = False) for _ in range(5)] + [nn.Tanh()]\n",
    "architecture_3 = [conv_params_3, use_batchnorm_3, activation_fn_3 ,Bilinear_size_3]\n",
    "\n",
    "# TRIAL 2\n",
    "# TO BE ADDED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jPyLN71DyKUj"
   },
   "outputs": [],
   "source": [
    "G_2 = Generator_BilinearUpsample(architecture_3)\n",
    "random_tensor = torch.randn((5,100,1,1))\n",
    "random_output = G_2(random_tensor)\n",
    "print(random_output.shape)\n",
    "TP2 = sum(p.numel() for p in G_2.parameters())\n",
    "print(TP2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wa9Yc2AahCMV"
   },
   "source": [
    "**Sampling from various distributions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rE-cKGpmhFJu"
   },
   "outputs": [],
   "source": [
    "# Sample from a Gaussian for inference\n",
    "def sample_gaussian(mean, covariance_matrix , n_samples):\n",
    "    # mean should be of size ([d])\n",
    "    # variance is the covariance matrix of size ([d,d]) must be +ve semidefinite\n",
    "    z = torch.distributions.MultivariateNormal(mean,covariance_matrix)\n",
    "    return z.sample([n_samples])\n",
    "    # returns of size ([n_samples,d])\n",
    "\n",
    "# Sample from \"CURRENT TRAINING IMAGES\"\n",
    "def sample_train(n_samples):\n",
    "    i_indices = torch.randint(0, training_images.size(0), (n_samples,)).tolist()\n",
    "    sampled_images = []\n",
    "    for i in i_indices:\n",
    "        sampled_images.append(training_images[i])\n",
    "    sampled_images_tensor = torch.stack(sampled_images)\n",
    "    return sampled_images_tensor\n",
    "\n",
    "# Sample from Real_images [BUTTERFLY]\n",
    "def sample_real_butterfly(n_samples):\n",
    "    i_indices = torch.randint(0, 6499, (n_samples,)).tolist()\n",
    "    sampled_images = []\n",
    "    for i in i_indices:\n",
    "        sampled_images.append(butterfly_images[i])\n",
    "    sampled_images_tensor = torch.stack(sampled_images)\n",
    "    return sampled_images_tensor\n",
    "\n",
    "# Sample from Real_images [ANIMAL]\n",
    "def sample_real_animal(n_samples):\n",
    "    i_indices = torch.randint(0, 90, (n_samples,)).tolist() # chooses class\n",
    "    j_indices = torch.randint(0, 60, (n_samples,)).tolist() # chooses image in class\n",
    "    sampled_images = []\n",
    "    for i, j in zip(i_indices, j_indices):\n",
    "        sampled_images.append(animal_images[i, j])\n",
    "    sampled_images_tensor = torch.stack(sampled_images)\n",
    "    return sampled_images_tensor\n",
    "\n",
    "# Sample from Real_images [ANIMAL] including augmented\n",
    "def sample_real_augmented_animal(n_samples):\n",
    "    i_indices = torch.randint(0, 90, (n_samples,)).tolist() # chooses class\n",
    "    j_indices = torch.randint(0, 60, (n_samples,)).tolist() # chooses image in class\n",
    "    k_indices = torch.randint(0, 4, (n_samples,)).tolist() # chooses augmentation type\n",
    "    sampled_images = []\n",
    "    for i, j , k in zip(i_indices, j_indices , k_indices):\n",
    "        sampled_images.append(animal_images_augmented[k, i, j])\n",
    "    sampled_images_tensor = torch.stack(sampled_images)\n",
    "    return sampled_images_tensor\n",
    "\n",
    "# Sample from Real_images [BUTTERFLY] including augmented\n",
    "def sample_real_augmented_butterfly(n_samples):\n",
    "    i_indices = torch.randint(0, 6499, (n_samples,)).tolist() # chooses image\n",
    "    k_indices = torch.randint(0, 4, (n_samples,)).tolist() # chooses augmentation type\n",
    "    sampled_images = []\n",
    "    for i, k in zip(i_indices, k_indices):\n",
    "        sampled_images.append(butterfly_images_augmented[k, i])\n",
    "    sampled_images_tensor = torch.stack(sampled_images)\n",
    "    return sampled_images_tensor\n",
    "\n",
    "# Sample from Real_images [ANIME]\n",
    "def sample_real_anime(n_samples):\n",
    "    i_indices = torch.randint(0, 21551, (n_samples,)).tolist()\n",
    "    sampled_images = []\n",
    "    for i in i_indices:\n",
    "        sampled_images.append(anime_images[i])\n",
    "    sampled_images_tensor = torch.stack(sampled_images)\n",
    "    return sampled_images_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tX5Z-YfpvR1x"
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "t = sample_gaussian(torch.randn((100,)),torch.eye(100),10)\n",
    "print(t.size())\n",
    "t1 = sample_train(10)\n",
    "print(t1.size())\n",
    "image_1 = (t1[0]+1)/2\n",
    "image_1 = image_1.permute(1,2,0)\n",
    "plt.imshow(image_1)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5hHL2ENScigM"
   },
   "source": [
    "**Different types of Optimizer classes to be experimented with**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D13G8xAXcnvc"
   },
   "source": [
    "* In the W-GAN paper they have used RMSProp for optimisation\n",
    "* RMSProp (Root Mean Square Propagation) is an adaptive learning rate optimization algorithm designed to address some of the challenges faced by traditional SGD especially in dealing with non-stationary and noisy gradients\n",
    "* In the DC-GAN paper they use Adam Optimizer with beta = 0.5 for both G,D\n",
    "* In some other papers SGD is used for Discriminator and Adam for Generator\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Xzh8mbUayj-"
   },
   "source": [
    "**DC-GAN Training Algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k9vSh8Lc3I_3"
   },
   "source": [
    "**Experiments and Observations on using Vanilla DC-GAN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-JzC88jUixUL"
   },
   "source": [
    "* Using DC-GAN architecture with the loss function mentioned in the original GAN paper the gradients quickly became zero for both the generator and discriminator. So the modified loss function of max (log(D(G(z))) instead of minimising (log(1-D(G(z)) improved the training stability, but still G_loss kept fluctuating a bit and D_loss seems to be hovering around zero when n_critic = 5.\n",
    "* Even after implementing the (-logD) trick still at times the loss function explodes to \"nan\" in between of the training or else the D_loss stays zero and G_loss doesnt decrease and stays quite high.So we need to move to W-GAN or try various hyperparameters and architectures.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "kCZ1-gbnOdyE"
   },
   "outputs": [],
   "source": [
    "\"\"\"Assuming \"Critic/Discriminator/D\" and \"Generator/G\" are defined as classes derived from nn.Module\n",
    "Hyperparameters := {lr,mini_batch_size,# D updates per G update,latent_space dimesnion} to be chosen for optimisation\"\"\"\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "lr_D, lr_G, m, n_critic, d = 2e-4, 2e-4, 128, 5, 100\n",
    "D_architecture = architecture_1\n",
    "G_architecture = architecture_2\n",
    "D = Critic_DCGAN(D_architecture).to(device)\n",
    "G = Generator_DCGAN(G_architecture).to(device)\n",
    "D_loss = []\n",
    "G_loss = []\n",
    "\n",
    "# We will use 2 optimizers for convenience\n",
    "# Adam optimizer\n",
    "D_optimizer = optim.Adam(D.parameters(), lr=lr_D, betas=(0.5, 0.99), eps=1e-8, weight_decay=0)\n",
    "G_optimizer = optim.Adam(G.parameters(), lr=lr_G, betas=(0.5, 0.99), eps=1e-8, weight_decay=0)\n",
    "# RMSPROP\n",
    "# D_optimizer = optim.RMSprop(D.parameters(), lr = lr_D, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)\n",
    "# G_optimizer = optim.RMSprop(G.parameters(), lr = lr_G, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AEN1MJmZ-J_e"
   },
   "outputs": [],
   "source": [
    "# This part is used for manual changing if needed inbetween of the training\n",
    "lr_D, lr_G, m, d = 1e-4 , 1e-4 , 128, 100\n",
    "D_optimizer = optim.Adam(D.parameters(), lr=lr_D, betas=(0.5, 0.99), eps=1e-8, weight_decay=0)\n",
    "G_optimizer = optim.Adam(G.parameters(), lr=lr_G, betas=(0.5, 0.99), eps=1e-8, weight_decay=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mSr9yzFSCDE6"
   },
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "n_critic = 5\n",
    "n_generator = 1\n",
    "mini_batch_epochs = 50\n",
    "epsilon = 1e-15\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for _ in range(mini_batch_epochs):\n",
    "    # start = time.time()\n",
    "    # We will track image quality and gradient norm and decide when to halt\n",
    "        for t in range(n_critic):\n",
    "            \"\"\"How freezing works in Pytorch:\n",
    "            In the forward pass, torch.no_grad()/reqd_grad = False will prevent the computation of gradients for the frozen layers.\n",
    "            However, in the backward pass, gradients will still flow through them\n",
    "            but the optimizer won't update its parameters because they are marked as not requiring gradient\n",
    "            The parameter gradients, which are not needed, won’t be computed (their .grad attribute won’t be updated)\n",
    "            But the gradient calculation will continue, if it’s needed for earlier layers.\"\"\"\n",
    "            G.set_requires_grad(False)\n",
    "            D.set_requires_grad(True)\n",
    "            D_optimizer.zero_grad()\n",
    "            G_optimizer.zero_grad()\n",
    "            x = sample_train(m).to(device)\n",
    "            z = (sample_gaussian(torch.zeros((d,)),torch.eye(d),m)).unsqueeze(-1).unsqueeze(-1).to(device)\n",
    "            y = G(z)\n",
    "            # f-GAN objective F(θ,w) maximizes wrt w and minimises wrt θ\n",
    "            # Compute gradients for D where loss_D is a 0-dimensional tensor\n",
    "            loss_D = -((torch.log(D(x) + epsilon)).mean() + (torch.log(1 - D(y) + epsilon)).mean())\n",
    "            loss_D.backward()\n",
    "            D_optimizer.step()\n",
    "            D_optimizer.zero_grad()\n",
    "\n",
    "        for t in range(n_generator):\n",
    "            # Sample another batch of noise z from prior p(z)\n",
    "            G.set_requires_grad(True)\n",
    "            D.set_requires_grad(False)\n",
    "            D_optimizer.zero_grad()\n",
    "            G_optimizer.zero_grad()\n",
    "            z1 = (sample_gaussian(torch.zeros((d,)),torch.eye(d),m)).unsqueeze(-1).unsqueeze(-1).to(device)\n",
    "            y1 = G(z1)\n",
    "            # Compute gradients for G\n",
    "            loss_G = -(torch.log(D(y1) + epsilon)).mean()\n",
    "            loss_G.backward()\n",
    "            G_optimizer.step()\n",
    "            G_optimizer.zero_grad()\n",
    "\n",
    "        D_loss.append(loss_D.item())\n",
    "        G_loss.append(loss_G.item())\n",
    "        # Optional: Logging after each epoch\n",
    "        # print(f\"Epoch [{epoch+1}/{num_epochs}] | D Loss: {loss_D.item():.4f} | G Loss: {loss_G.item():.4f}\")\n",
    "        # stop = time.time()\n",
    "        # print(stop - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GEsCTzeV3qfb"
   },
   "outputs": [],
   "source": [
    "# Plotting the Loss curves so far\n",
    "# Create a figure with two subplots (1 row, 2 columns)\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "# Plot D_loss in the first subplot\n",
    "D_steps = range(len(D_loss))\n",
    "ax1.plot(D_steps, D_loss)\n",
    "ax1.set_xlabel('Mini-Batch')\n",
    "ax1.set_ylabel('loss')\n",
    "ax1.set_title('D_loss')\n",
    "# Plot G_loss in the second subplot\n",
    "G_steps = range(len(G_loss))\n",
    "ax2.plot(G_steps, G_loss)\n",
    "ax2.set_xlabel('Mini-Batch')\n",
    "ax2.set_ylabel('loss')\n",
    "ax2.set_title('G_loss')\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kGNRWWey5XXb"
   },
   "outputs": [],
   "source": [
    "# Inference\n",
    "with torch.no_grad():\n",
    "    z = (sample_gaussian(torch.zeros((d,)),torch.eye(d),100)).unsqueeze(-1).unsqueeze(-1).to(device)\n",
    "    y = (G(z) + 1) / 2\n",
    "    images = y.detach().cpu().numpy()\n",
    "    # Create a figure for the grid of images\n",
    "    fig, axes = plt.subplots(nrows=10, ncols=10, figsize=(15, 15))\n",
    "    # Loop through the 100 images and display them in the grid\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        img = images[i].transpose(1, 2, 0)\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vJa20UpgCIYq"
   },
   "source": [
    "**FOLLOWING WAS THE BEST DCGAN IMAGES OBTAINED FOR BUTTERFLY DATASET**\n",
    "* D has 760,929 params and G has 3,608,995 params . Latent_dim = d = 100 ; lr = 2e-4 ; m = 128 ; Adam Optimizer with (0.5,0.99) was used.\n",
    "* Less parameters in G caused very blurry images and making D,G somewhat equally powerful seems to give best results.\n",
    "* Augmentation was not used . {n_critic and n_generator} were varied manually every 20/30 epochs taking {5,1},{5,2},{5,3} values progressively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6uWiKuHuL7iC"
   },
   "source": [
    "**FOLLOWING WAS THE BEST DCGAN IMAGES OBTAINED FOR ANIMAL DATASET**\n",
    "* Same hyperparameters as the [butterfly-best] was used along with augmentations{flip,colour-jitter,rotations} for the first 130 epochs and then only flip was used.\n",
    "* Animal images seemed to require more iterations when using augmentations and also seem to require More parameters to model the distribution due to much higher variety than butterflies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_H1B-3HuMCQ5"
   },
   "source": [
    "**Increasing Dimension of LatentSpace**\n",
    "\n",
    "* Increasing the dimension of input latent space to d = 200 and using a different architecture for D,G some slight imporvements seem to happen visually but image quality is still not recognisable and loss function saturates\n",
    "* Below are few images of d = 200\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRMHPT2NaTkd"
   },
   "source": [
    "**[Q5] Latent Space Traversal**\n",
    "* We randomly sample 2 Gaussians z1,z2\n",
    "* We then try Linear and Non-linear interpolations on some of the saved weights of various GANs below\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0q31FWnYam2H"
   },
   "outputs": [],
   "source": [
    "# Linear Interpolation\n",
    "# Load the pretrained generator\n",
    "device = torch.device(\"cpu\")\n",
    "d_t = 200\n",
    "G_t = torch.load(\"A2_weights/1c.New_530_G.pth\", map_location=torch.device('cpu')).to(device)\n",
    "\n",
    "n_pairs = 10  # Number of random pairs\n",
    "n_steps = 10  # Number of interpolations between each pair\n",
    "\n",
    "# Store all interpolations\n",
    "all_interpolations = []\n",
    "for _ in range(n_pairs):\n",
    "    # Sample two random latent vectors (z1, z2)\n",
    "    z = sample_gaussian(torch.zeros((d_t,)), torch.eye(d_t), 2).unsqueeze(-1).unsqueeze(-1)\n",
    "    z1 = z[0]\n",
    "    z2 = z[1]\n",
    "    # Interpolate between z1 and z2\n",
    "    interpolations = []\n",
    "    for alpha in np.linspace(0, 1, n_steps):\n",
    "        z_interpolated = (1 - alpha) * z1 + alpha * z2  # Linear interpolation\n",
    "        interpolations.append(z_interpolated)\n",
    "    all_interpolations.append(torch.stack(interpolations))\n",
    "\n",
    "# Stack all interpolations into a single tensor of shape (n_pairs * n_steps, 3, H, W)\n",
    "all_interpolations = torch.cat(all_interpolations, dim=0).to(device)\n",
    "with torch.no_grad():\n",
    "    generated_images = (G_t(all_interpolations) + 1) / 2\n",
    "\n",
    "# Plot the images row-wise\n",
    "fig, axs = plt.subplots(n_pairs, n_steps, figsize=(n_steps * 2, n_pairs * 2))\n",
    "\n",
    "# Display the images in rows, each row corresponding to one interpolation sequence\n",
    "for i in range(n_pairs):\n",
    "    for j in range(n_steps):\n",
    "        img_idx = i * n_steps + j\n",
    "        img = generated_images[img_idx].permute(1, 2, 0).cpu().numpy()  # Convert from (C, H, W) to (H, W, C)\n",
    "        axs[i, j].imshow(img)\n",
    "        axs[i, j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Obym7bgWegEI"
   },
   "outputs": [],
   "source": [
    "#Spherical interpolation\n",
    "#Linear interpolation moves along a straight line between two points\n",
    "#Spherical interpolation moves along the shortest arc on a sphere between the two points.\n",
    "#It can produce more natural transitions in the latent space of a GAN\n",
    "def s_i(val, low, high):\n",
    "    \"\"\"Spherical linear interpolation between two vectors.\"\"\"\n",
    "    omega = torch.acos(torch.clamp(torch.dot(low / low.norm(), high / high.norm()), -1, 1))\n",
    "    so = torch.sin(omega)\n",
    "    if so == 0:\n",
    "        return (1.0 - val) * low + val * high  # Linear interpolation as a fallback\n",
    "    return torch.sin((1.0 - val) * omega) / so * low + torch.sin(val * omega) / so * high\n",
    "\n",
    "# Load the pretrained generator\n",
    "G_t = torch.load(\"A2_weights/1c.New_530_G.pth\",map_location=torch.device('cpu')).to(device)\n",
    "d_t = 200\n",
    "\n",
    "n_pairs = 10  # Number of random pairs\n",
    "n_steps = 9  # Number of interpolations between each pair\n",
    "\n",
    "# Store all interpolations\n",
    "all_interpolations = []\n",
    "for _ in range(n_pairs):\n",
    "    # Sample two random latent vectors (z1, z2)\n",
    "    z = sample_gaussian(torch.zeros((d_t,)), torch.eye(d_t), 2).unsqueeze(-1).unsqueeze(-1) # shape (2, 100, 1, 1)\n",
    "    z1 = z[0]\n",
    "    z2 = z[1]\n",
    "    # Generate interpolations using s_i\n",
    "    interpolations = []\n",
    "    for step in range(n_steps + 1):\n",
    "        alpha = step / n_steps  # Interpolation parameter (0 to 1)\n",
    "        z_interpolated = s_i(alpha, z1.squeeze().squeeze(), z2.squeeze().squeeze())\n",
    "        interpolations.append(z_interpolated.unsqueeze(-1).unsqueeze(-1))\n",
    "    all_interpolations.append(torch.stack(interpolations))\n",
    "\n",
    "# Stack all interpolations into a single tensor of shape (n_pairs * (n_steps + 1), 100, 1, 1)\n",
    "all_interpolations = torch.cat(all_interpolations, dim=0).to(device)\n",
    "with torch.no_grad():\n",
    "    generated_images = (G_t(all_interpolations) + 1) / 2\n",
    "\n",
    "# Plot the images row-wise\n",
    "fig, axs = plt.subplots(n_pairs, n_steps + 1, figsize=((n_steps + 1) * 2, n_pairs * 2))\n",
    "# Display the images in rows, each row corresponding to one interpolation sequence\n",
    "for i in range(n_pairs):\n",
    "    for j in range(n_steps + 1):\n",
    "        img_idx = i * (n_steps + 1) + j\n",
    "        img = generated_images[img_idx].permute(1, 2, 0).cpu().numpy()  # Convert from (C, H, W) to (H, W, C)\n",
    "        axs[i, j].imshow(img)\n",
    "        axs[i, j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hPh6RIqDbOf3"
   },
   "source": [
    "**[Q7] W-GAN Implementation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mh8RAxITDcn7"
   },
   "source": [
    "**Observations**\n",
    "* Last activation of discriminator is tried with both sigmoid and a linear layer . The principled way is to make it Linear since the optimisation is over space of all K-Lipshitz functions\n",
    "* The G_loss increases for most choices of the hyperparameters of W-GAN when using weight/gradient clipping\n",
    "* Increasing the lr to order of -4 causes excessive instability in training and {lr = 5e-5} as outlined in the paper was used\n",
    "* W-GAN was significantly \"slower\" than DC-GAN training due to learning rate and convergence properties . In the paper for GNP the authors suggest to use Adam optimizer over RMSProp.\n",
    "* Weight-clipping, Gradient Clipping and Gradient Norm Penalty were tried .\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SlZi6JKacmTc"
   },
   "outputs": [],
   "source": [
    "# Function to clip the gradients of the critic\n",
    "def clip_gradients(critic, c):\n",
    "    for name, param in critic.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            # print(f\"Param: {name}, Grad: {param.grad}\")\n",
    "            param.grad.data = torch.clamp(param.grad.data, -c, c)\n",
    "\n",
    "# Function to clip the weights of the critic\n",
    "def clip_weights(critic, c):\n",
    "    for name, param in critic.named_parameters():\n",
    "        param.data = torch.clamp(param.data, -c, c)\n",
    "        # if param.grad is not None:\n",
    "            # print(f\"Param: {name}, Grad: {param.grad}\")\n",
    "\n",
    "# Gradient Norm Penalty\n",
    "def compute_gp(netD,x,y):\n",
    "        # x is real_image and y is generated_image\n",
    "        m = x.size(0)\n",
    "        # Sample Epsilon from uniform distribution\n",
    "        eps = torch.rand(m, 1, 1, 1).to(x.device)\n",
    "        eps = eps.expand_as(x)\n",
    "        # Interpolation between real data and fake data.\n",
    "        interpolation = eps * x + (1 - eps) * y\n",
    "        interpolation.requires_grad_(True)\n",
    "        # get logits for interpolated images\n",
    "        interp_logits = netD(interpolation)\n",
    "        grad_outputs = torch.ones_like(interp_logits)\n",
    "\n",
    "        # Compute Gradients wrt interpolated images\n",
    "        gradients = torch.autograd.grad(\n",
    "            outputs=interp_logits,\n",
    "            inputs=interpolation,\n",
    "            create_graph = True,\n",
    "            grad_outputs=grad_outputs,\n",
    "            retain_graph = True\n",
    "        )[0]\n",
    "\n",
    "        # Compute and return Gradient Norm\n",
    "        gradients = gradients.view(m, -1)\n",
    "        grad_norm = gradients.norm(2, 1)\n",
    "        return torch.mean((grad_norm - 1) ** 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TiS8X1ALOvmO"
   },
   "outputs": [],
   "source": [
    "# (D) (TRIAL_3)\n",
    "conv_params_1 = [[3,32,4,2,1],[32,64,4,2,1],[64,64,4,2,1],[64,128,4,2,1],[128,256,4,2,1],[256,1,4,4,1]]\n",
    "use_batchnorm_1 = [False for _ in range(6)]\n",
    "activation_fn_1 = [nn.LeakyReLU(0.2, inplace=False) for _ in range(5)] +[nn.Identity()]\n",
    "architecture_1 = [conv_params_1,use_batchnorm_1,activation_fn_1]\n",
    "# 760,929 [760k]\n",
    "# (G) (DCGAN/TRIAL_1)\n",
    "transpose_conv_params_2 = [[100,512,4,1,0],[512,256,4,2,1],[256,128,4,2,1],[128,64,4,2,1],[64,32,4,2,1],[32,3,4,2,1]]\n",
    "use_batchnorm_2 = [True for _ in range(5)] + [False]\n",
    "activation_fn_2 = [nn.ReLU(inplace = False) for _ in range(5)] +[nn.Tanh()]\n",
    "architecture_2 = [transpose_conv_params_2,use_batchnorm_2,activation_fn_2]\n",
    "# 3,608,995 [3M]\n",
    "\n",
    "# Most prior GAN implementations [22, 23, 2] use batch normalization\n",
    "# in both the generator and the discriminator to help stabilize training, but batch normalization\n",
    "# changes the form of the discriminator’s problem from mapping a single input to a single output to\n",
    "# mapping from an entire batch of inputs to a batch of outputs [23]. Our penalized training objective\n",
    "# is no longer valid in this setting, since we penalize the norm of the critic’s gradient with respect\n",
    "# to each input independently, and not the entire batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z-Vjn4a3bQkT"
   },
   "outputs": [],
   "source": [
    "\"\"\"Assuming \"Critic/Discriminator/D\" and \"Generator/G\" are defined as classes derived from nn.Module\n",
    "Hyperparameters := {lr,clipping_parameter,mini_batch_size,# D updates per G update,latent_space dimesnion} to be chosen for optimisation\"\"\"\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "lr_D, lr_G, c, m, d = 5e-5, 5e-5, 0.01, 64, 100\n",
    "D_architecture = architecture_1\n",
    "G_architecture = architecture_2\n",
    "D = Critic_DCGAN(D_architecture).to(device)\n",
    "G = Generator_DCGAN(G_architecture).to(device)\n",
    "D_loss = []\n",
    "G_loss = []\n",
    "\n",
    "# RMSPROP\n",
    "# D_optimizer = optim.RMSprop(D.parameters(), lr = lr_D, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)\n",
    "# G_optimizer = optim.RMSprop(G.parameters(), lr = lr_G, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)\n",
    "# Adam optimizer\n",
    "D_optimizer = optim.Adam(D.parameters(), lr=lr_D, betas=(0.5, 0.99), eps=1e-8, weight_decay=0)\n",
    "G_optimizer = optim.Adam(G.parameters(), lr=lr_G, betas=(0.5, 0.99), eps=1e-8, weight_decay=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qY9wIvOelufP"
   },
   "outputs": [],
   "source": [
    "# This part is used for manual changing if needed inbetween of the training\n",
    "lr_D, lr_G, m, d , c = 5e-5, 5e-5, 64, 100 , 0.01\n",
    "\n",
    "# RMSPROP [used in paper]\n",
    "# D_optimizer = optim.RMSprop(D.parameters(), lr = lr_D, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)\n",
    "# G_optimizer = optim.RMSprop(G.parameters(), lr = lr_G, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)\n",
    "# Adam Optimizer\n",
    "D_optimizer = optim.Adam(D.parameters(), lr=lr_D, betas=(0.5, 0.999), eps=1e-8, weight_decay=0)\n",
    "G_optimizer = optim.Adam(G.parameters(), lr=lr_G, betas=(0.5, 0.999), eps=1e-8, weight_decay=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MqaNR08AoHt_"
   },
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "n_critic = 5\n",
    "n_generator = 2\n",
    "gp_factor = 10\n",
    "mini_batch_epochs = 84\n",
    "epsilon = 1e-15\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for _ in range(mini_batch_epochs):\n",
    "      # start = time.time()\n",
    "      # We will track image quality and gradient norm and decide when to halt\n",
    "      for t in range(n_critic):\n",
    "          G.set_requires_grad(False)\n",
    "          D.set_requires_grad(True)\n",
    "          D_optimizer.zero_grad()\n",
    "          G_optimizer.zero_grad()\n",
    "          x = sample_train(m).to(device)\n",
    "          z = (sample_gaussian(torch.zeros((d,)),torch.eye(d),m)).unsqueeze(-1).unsqueeze(-1).to(device)\n",
    "          y = G(z)\n",
    "          # The W-GAN loss terms is just difference of D(x) - D(y)\n",
    "          # We need to maximise the loss wrt the critic to correctly approximate W(P_r,P_theta)\n",
    "          # And then we minimise the Wasserstein metric wrt the G\n",
    "          gradient_norm_penalty = (gp_factor) * compute_gp(D,x,y)\n",
    "          loss_D = (D(y).mean() - D(x).mean()) + gradient_norm_penalty\n",
    "          loss_D.backward()\n",
    "          D_optimizer.step()\n",
    "          D_optimizer.zero_grad()\n",
    "          # clip_weights(D,c)\n",
    "\n",
    "      for t in range(n_generator):\n",
    "          # Sample another batch of noise z from prior p(z)\n",
    "          G.set_requires_grad(True)\n",
    "          D.set_requires_grad(False)\n",
    "          D_optimizer.zero_grad()\n",
    "          G_optimizer.zero_grad()\n",
    "          z1 = (sample_gaussian(torch.zeros((d,)),torch.eye(d),m)).unsqueeze(-1).unsqueeze(-1).to(device)\n",
    "          y1 = G(z1)\n",
    "          # Compute gradients for G\n",
    "          loss_G = -(D(y1).mean())\n",
    "          loss_G.backward()\n",
    "          G_optimizer.step()\n",
    "          G_optimizer.zero_grad()\n",
    "\n",
    "      D_loss.append(loss_D.item())\n",
    "      G_loss.append(loss_G.item())\n",
    "      # Optional: Logging after each epoch\n",
    "      # print(f\"Epoch [{epoch+1}/{num_epochs}] | D Loss: {loss_D:.4f} | G Loss: {loss_G:.4f}\")\n",
    "      # stop = time.time()\n",
    "      # print(stop - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zuKuMvcqfyAI"
   },
   "outputs": [],
   "source": [
    "# Plotting the Loss curves so far\n",
    "# Create a figure with two subplots (1 row, 2 columns)\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "# Plot D_loss in the first subplot\n",
    "D_steps = range(len(D_loss))\n",
    "ax1.plot(D_steps, D_loss)\n",
    "ax1.set_xlabel('Mini-batch iteration')\n",
    "ax1.set_ylabel('loss')\n",
    "ax1.set_title('D_loss')\n",
    "# Plot G_loss in the second subplot\n",
    "G_steps = range(len(G_loss))\n",
    "ax2.plot(G_steps, G_loss)\n",
    "ax2.set_xlabel('Mini-batch iterations')\n",
    "ax2.set_ylabel('loss')\n",
    "ax2.set_title('G_loss')\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T3nD3jr_iabz"
   },
   "outputs": [],
   "source": [
    "# Inference\n",
    "with torch.no_grad():\n",
    "    z = (sample_gaussian(torch.zeros((d,)),torch.eye(d),100)).unsqueeze(-1).unsqueeze(-1).to(device)\n",
    "    y = (G(z) + 1) / 2\n",
    "    images = y.detach().cpu().numpy()\n",
    "    # Create a figure for the grid of images\n",
    "    fig, axes = plt.subplots(nrows=10, ncols=10, figsize=(15, 15))\n",
    "    # Loop through the 100 images and display them in the grid\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        img = images[i].transpose(1, 2, 0)\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l-g-Y0nsY09P"
   },
   "outputs": [],
   "source": [
    "torch.save(D, 'Animal_1d.New_85_D.pth')\n",
    "torch.save(G, 'Animal_1d.New_85_G.pth')\n",
    "\n",
    "# # Load entire model\n",
    "# D = torch.load('1d_650_D.pth')\n",
    "# G = torch.load('1d_650_G.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xl0DmP4zurFv"
   },
   "source": [
    "**Below is the best Images obtained using Gradient Norm Penalty**\n",
    "* gp_factor = 10 and Adam(0.5,0.99)\n",
    "* lr = 5e-5 for stability and No BN in critic with last layer being nn.Identity()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2a0cH07HOZGB"
   },
   "source": [
    "**Below is the results of WeightClipping/Gradient Clipping**\n",
    "* Same architecture as before ; d=100 ; Adam\n",
    "* Clipping factor = c = 0.01\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2tqOxzRBppo"
   },
   "source": [
    "**[Q8 AND Q9 ] AUTO ENCODING GAN / LATENT-VARIABLE INFERENCE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9yYlICi9s0Bi"
   },
   "source": [
    "* There exists various ways to implement a decoder and encoder along with the GAN architectures in the literature which are Cycle-GAN,DE-GAN,AAA VAE,VAE/GAN and AutoEncoding GAN.\n",
    "* But most of these have a lower dimensional latent space as the intermediate. But for this assignment we simply implement a single decoder that solely converts generated images into the input random noise\n",
    "* We train (GAN + Decoder) together with (GAN_loss + Reconstruction_loss btw the predicted noise and actual noise).\n",
    "Note that the decoder has no direct relation with real images\n",
    "* The architecture used for the decoder is going to be very similar to that of the critic itself using convolutions but this time the final o/p is not [1,1,1] but [100,1,1]\n",
    "* Hopefully the \"predicted noise\" would also be distributed as the gaussian which was the true input. This is enforced using the reconstruction loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCCwMCM3utRI"
   },
   "source": [
    "**Decoder that tries to predict the noise that would generate the image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oVSEx2Z-Bxva"
   },
   "outputs": [],
   "source": [
    "class Decoder_GAN(nn.Module):\n",
    "    def __init__(self,architecture):\n",
    "        # architecture = [conv_params , use_batchnorm, activation_fn]\n",
    "        # conv_params = [[in_channels,output_channels,kernel_size,stride,padding],....[*,100,*,*,*]]\n",
    "        # use_batchnorm = [False,True,....False]\n",
    "        # activation_fn = [nn.Relu,nn.LeakyRelu,.....nn.Identity()]\n",
    "        super(Decoder_GAN, self).__init__()\n",
    "        assert len(architecture[0]) == len(architecture[1]) == len(architecture[2])\n",
    "        self.conv_params = architecture[0]\n",
    "        self.use_batchnorm = architecture[1]\n",
    "        self.activation_fn = architecture[2]\n",
    "        layers = []\n",
    "        for j,i in enumerate(self.conv_params):\n",
    "            layers.append(nn.Conv2d(in_channels=i[0],out_channels=i[1],kernel_size=i[2],stride=i[3],padding=i[4]))\n",
    "            if (self.use_batchnorm)[j]:\n",
    "                layers.append(nn.BatchNorm2d(i[1]))\n",
    "            layers.append(self.activation_fn[j])\n",
    "        # The last conv2d layer outputs [100,1,1]\n",
    "        # Since ouptut is gaussian and we expect any real number better to have no activations in the final layer\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    # return value must be (bs,100,1,1) in our general examples\n",
    "    # input is (bs,3,128,128) generated image\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    # We will use this for flexibility\n",
    "    def set_requires_grad(self, requires_grad):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = requires_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FZ_iWX5cBxz9"
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# output_size = ({input_size - kernel_size + 2*padding}/stride) + 1\n",
    "# (E) (TRIAL 1)\n",
    "conv_params_4 = [[3,8,4,2,1],[8,16,4,2,1],[16,32,4,2,1],[32,64,4,2,1],[64,100,8,1,0]]\n",
    "use_batchnorm_4 = [True for _ in range(4)] + [False]\n",
    "activation_fn_4 = [nn.LeakyReLU(0.4, inplace=False) for _ in range(4)] + [nn.Identity()]\n",
    "architecture_4 = [conv_params_4,use_batchnorm_4,activation_fn_4]\n",
    "# 453,452\n",
    "# (D) (TRIAL_3)\n",
    "conv_params_1 = [[3,32,4,2,1],[32,64,4,2,1],[64,64,4,2,1],[64,128,4,2,1],[128,256,4,2,1],[256,1,4,4,1]]\n",
    "use_batchnorm_1 = [True for _ in range(5)] + [False]\n",
    "activation_fn_1 = [nn.LeakyReLU(0.2, inplace=False) for _ in range(5)] +[nn.Sigmoid()]\n",
    "architecture_1 = [conv_params_1,use_batchnorm_1,activation_fn_1]\n",
    "# 760,929 [760k]\n",
    "# (G) (DCGAN/TRIAL_1)\n",
    "transpose_conv_params_2 = [[100,512,4,1,0],[512,256,4,2,1],[256,128,4,2,1],[128,64,4,2,1],[64,32,4,2,1],[32,3,4,2,1]]\n",
    "use_batchnorm_2 = [True for _ in range(5)] + [False]\n",
    "activation_fn_2 = [nn.ReLU(inplace = False) for _ in range(5)] +[nn.Tanh()]\n",
    "architecture_2 = [transpose_conv_params_2,use_batchnorm_2,activation_fn_2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aFmD_O1lAhB_"
   },
   "outputs": [],
   "source": [
    "# Just Check\n",
    "Decoder_1 = Decoder_GAN(architecture_4)\n",
    "random_tensor = torch.randn((5,3,128,128))\n",
    "random_output = Decoder_1(random_tensor)\n",
    "print(random_output.shape)\n",
    "TP4 = sum(p.numel() for p in Decoder_1.parameters())\n",
    "print(TP4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ACst98sO5rtQ"
   },
   "source": [
    "**Training Decoder along with the GAN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G5HD-yqm55TN"
   },
   "outputs": [],
   "source": [
    "\"\"\"Assuming \"Critic/Discriminator/D\" and \"Generator/G\" are defined as classes derived from nn.Module\n",
    "Hyperparameters := {lr,mini_batch_size,# D updates per G update,latent_space dimesnion} to be chosen for optimisation\n",
    "This time we also have the Decoder name it E becos it actually encodes !!\"\"\"\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "lr_D, lr_G, lr_E , m, d = 2e-4, 2e-4, 2e-4, 128, 100\n",
    "D_architecture = architecture_1\n",
    "G_architecture = architecture_2\n",
    "E_architecture = architecture_4\n",
    "D = Critic_DCGAN(D_architecture).to(device)\n",
    "G = Generator_DCGAN(G_architecture).to(device)\n",
    "E = Decoder_GAN(E_architecture).to(device)\n",
    "# We will use 2 optimizers for convenience\n",
    "# Adam optimizer\n",
    "D_optimizer = optim.Adam(D.parameters(), lr=lr_D, betas=(0.5, 0.99), eps=1e-8, weight_decay=0)\n",
    "G_optimizer = optim.Adam(G.parameters(), lr=lr_G, betas=(0.5, 0.99), eps=1e-8, weight_decay=0)\n",
    "E_optimizer = optim.Adam(E.parameters(), lr=lr_E, betas=(0.5, 0.99), eps=1e-8, weight_decay=0)\n",
    "D_loss = []\n",
    "G_loss = []\n",
    "E_loss = []\n",
    "# RMSPROP\n",
    "# D_optimizer = optim.RMSprop(D.parameters(), lr = lr_D, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)\n",
    "# G_optimizer = optim.RMSprop(G.parameters(), lr = lr_G, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6_onoR2j57md"
   },
   "outputs": [],
   "source": [
    "# This part is used for manual changing if needed inbetween of the training\n",
    "lr_D, lr_G, lr_E, m, d = 1e-4 ,1e-4, 1e-4, 64, 100\n",
    "D_optimizer = optim.Adam(D.parameters(), lr=lr_D, betas=(0.5, 0.99), eps=1e-8, weight_decay=0)\n",
    "G_optimizer = optim.Adam(G.parameters(), lr=lr_G, betas=(0.5, 0.99), eps=1e-8, weight_decay=0)\n",
    "E_optimizer = optim.Adam(E.parameters(), lr=lr_E, betas=(0.5, 0.99), eps=1e-8, weight_decay=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yknCr7Zf580t"
   },
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "n_critic = 4\n",
    "n_generator = 1\n",
    "n_decoder = 1\n",
    "mini_batch_epochs = 42\n",
    "epsilon = 1e-15\n",
    "mse_loss = nn.MSELoss()\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for _ in range(mini_batch_epochs):\n",
    "        # We will track image quality and gradient norm and decide when to halt\n",
    "        for t in range(n_critic):\n",
    "            G.set_requires_grad(False)\n",
    "            D.set_requires_grad(True)\n",
    "            E.set_requires_grad(False)\n",
    "            D_optimizer.zero_grad()\n",
    "            G_optimizer.zero_grad()\n",
    "            E_optimizer.zero_grad()\n",
    "            x = sample_train(m).to(device)\n",
    "            z = (sample_gaussian(torch.zeros((d,)),torch.eye(d),m)).unsqueeze(-1).unsqueeze(-1).to(device)\n",
    "            y = G(z)\n",
    "            # f-GAN objective F(θ,w) maximizes wrt w and minimises wrt θ\n",
    "            # Compute gradients for D where loss_D is a 0-dimensional tensor\n",
    "            loss_D = -((torch.log(D(x) + epsilon)).mean() + (torch.log(1 - D(y) + epsilon)).mean())\n",
    "            loss_D.backward()\n",
    "            D_optimizer.step()\n",
    "            D_optimizer.zero_grad()\n",
    "\n",
    "        for t in range(n_generator):\n",
    "            # Sample another batch of noise z from prior p(z)\n",
    "            G.set_requires_grad(True)\n",
    "            D.set_requires_grad(False)\n",
    "            E.set_requires_grad(False)\n",
    "            E_optimizer.zero_grad()\n",
    "            D_optimizer.zero_grad()\n",
    "            G_optimizer.zero_grad()\n",
    "            z1 = (sample_gaussian(torch.zeros((d,)),torch.eye(d),m)).unsqueeze(-1).unsqueeze(-1).to(device)\n",
    "            y1 = G(z1)\n",
    "            z2 = E(y1)\n",
    "            # Compute gradients for G\n",
    "            loss_G = -(torch.log(D(y1) + epsilon).mean()) + (mse_loss(z1,z2))\n",
    "            loss_G.backward()\n",
    "            G_optimizer.step()\n",
    "            G_optimizer.zero_grad()\n",
    "\n",
    "        for t in range(n_decoder):\n",
    "            # Sample yet another batch of noise z from prior p(z)\n",
    "            D.set_requires_grad(False)\n",
    "            G.set_requires_grad(False)\n",
    "            E.set_requires_grad(True)\n",
    "            D_optimizer.zero_grad()\n",
    "            G_optimizer.zero_grad()\n",
    "            E_optimizer.zero_grad()\n",
    "            z3 = (sample_gaussian(torch.zeros((d,)),torch.eye(d),m)).unsqueeze(-1).unsqueeze(-1).to(device)\n",
    "            y2 = G(z3)\n",
    "            z4 = E(y2)\n",
    "            # Compute gradients for E\n",
    "            loss_E = (mse_loss(z4,z3))\n",
    "            loss_E.backward()\n",
    "            E_optimizer.step()\n",
    "            E_optimizer.zero_grad()\n",
    "\n",
    "        D_loss.append(loss_D.item())\n",
    "        G_loss.append(loss_G.item())\n",
    "        E_loss.append(loss_E.item())\n",
    "\n",
    "    # Optional: Logging after each epoch\n",
    "    # print(f\"Epoch [{epoch+1}/{num_epochs}] | D Loss: {loss_D.item():.4f} | G Loss: {loss_G.item():.4f} | E_loss: {loss_E.item():.4f}\")\n",
    "    # stop = time.time()\n",
    "    # print(stop - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XvhmG1erLypH"
   },
   "outputs": [],
   "source": [
    "# Plotting the Loss curves with different colors\n",
    "# Create a figure with three subplots (1 row, 3 columns)\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(12, 6))\n",
    "\n",
    "# Plot D_loss in the first subplot (red)\n",
    "D_steps = range(len(D_loss))\n",
    "ax1.plot(D_steps, D_loss, color='red')\n",
    "ax1.set_xlabel('Mini-batch iteration')\n",
    "ax1.set_ylabel('loss')\n",
    "ax1.set_title('D_loss')\n",
    "# Plot G_loss in the second subplot (blue)\n",
    "G_steps = range(len(G_loss))\n",
    "ax2.plot(G_steps, G_loss, color='blue')\n",
    "ax2.set_xlabel('Mini-batch iterations')\n",
    "ax2.set_ylabel('loss')\n",
    "ax2.set_title('G_loss')\n",
    "# Plot E_loss in the third subplot (green)\n",
    "E_steps = range(len(E_loss))\n",
    "ax3.plot(E_steps, E_loss, color='green')\n",
    "ax3.set_xlabel('Mini-batch iterations')\n",
    "ax3.set_ylabel('loss')\n",
    "ax3.set_title('E_loss')\n",
    "\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZYAHGrE35945"
   },
   "outputs": [],
   "source": [
    "# Inference\n",
    "with torch.no_grad():\n",
    "    z = (sample_gaussian(torch.zeros((d,)),torch.eye(d),100)).unsqueeze(-1).unsqueeze(-1).to(device)\n",
    "    y = (G(z) + 1) / 2\n",
    "    images = y.detach().cpu().numpy()\n",
    "    # Create a figure for the grid of images\n",
    "    fig, axes = plt.subplots(nrows=10, ncols=10, figsize=(15, 15))\n",
    "    # Loop through the 100 images and display them in the grid\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        img = images[i].transpose(1, 2, 0)\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jaoM_CoaIsK"
   },
   "source": [
    "**Following were the best images obtained**\n",
    "*  lr = 1e-4 then reduced to 5e-5 using Adam(0.5,0.99)\n",
    "*  We update {5,2,1} for D,G,E respectively then slowly change it to {5,3,1} and finally {4,2,1} with updates to G,E happening separately\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DRHDg1dT94Su"
   },
   "source": [
    "**A simple MLP for classifying using Decoder outputs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OayZCGAi-BUB"
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, architecture):\n",
    "        # architecture = [linear_params, use_batchnorm, activation_fn]\n",
    "        # linear_params = [[in_dim, out_dim], .... [*, *]]\n",
    "        # use_batchnorm = [False, True, .... False]\n",
    "        # activation_fn = [nn.ReLU(), nn.LeakyReLU(), .... nn.Identity()]\n",
    "        super(MLP, self).__init__()\n",
    "        assert len(architecture[0]) == len(architecture[1]) == len(architecture[2])\n",
    "        self.linear_params = architecture[0]\n",
    "        self.use_batchnorm = architecture[1]\n",
    "        self.activation_fn = architecture[2]\n",
    "        layers = []\n",
    "        for i in range(len(self.linear_params)):\n",
    "            in_dim, out_dim = self.linear_params[i]\n",
    "            layers.append(nn.Linear(in_dim, out_dim))\n",
    "            if self.use_batchnorm[i]:\n",
    "                layers.append(nn.BatchNorm1d(out_dim))\n",
    "            layers.append(self.activation_fn[i])\n",
    "        # Final layer would be softmax and return shape would be (bs,90)\n",
    "        # But this gets done automatically in nn.CELoss\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    # We will use this for flexibility\n",
    "    def set_requires_grad(self, requires_grad):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = requires_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ElKzNlB0rfDo"
   },
   "outputs": [],
   "source": [
    "linear_params_5 = [[100,512],[512,256],[256,90]]\n",
    "use_batchnorm_5 = [True for _ in range(2)] + [False]\n",
    "activation_fn_5 = [nn.ReLU(inplace = False) for _ in range(2)] + [nn.Identity()]\n",
    "architecture_5 = [linear_params_5, use_batchnorm_5, activation_fn_5]\n",
    "MLP_1 = MLP(architecture_5)\n",
    "random_input = torch.randn((7,100))\n",
    "random_output = MLP_1(random_input)\n",
    "print(random_output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7lR3NGXl9BoQ"
   },
   "source": [
    "**Training the MLP to classify**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sns8KgFa9IHe"
   },
   "outputs": [],
   "source": [
    "learning_rate = 2e-3\n",
    "N = MLP(architecture_5).to(device)\n",
    "N_optimizer = optim.SGD(N.parameters(), lr = learning_rate, momentum=0.2)\n",
    "N_loss = []\n",
    "E = torch.load(\"E_2a_New_330.pth\").to(device)\n",
    "E.set_requires_grad(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_1-7PkF_1f9A"
   },
   "outputs": [],
   "source": [
    "# Changing lr manually during training if needed\n",
    "learning_rate = 1e-5\n",
    "N_optimizer = optim.SGD(N.parameters(), lr = learning_rate, momentum=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OGc4h_Wfu4Iu"
   },
   "outputs": [],
   "source": [
    "# Training the MLP\n",
    "num_epochs = 100\n",
    "m = 32\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# We keep images in [0,1]\n",
    "for epoch in range(num_epochs):\n",
    "    # start = time.time()\n",
    "    N.set_requires_grad(True)\n",
    "    N_optimizer.zero_grad()\n",
    "    i_indices_0 = torch.randint(0, 90, (m,)).to(device)\n",
    "    i_indices = i_indices_0.tolist() # chooses class\n",
    "    j_indices = torch.randint(0, 60, (m,)).tolist() # chooses image in class\n",
    "    sampled_images = []\n",
    "    for i, j in zip(i_indices, j_indices):\n",
    "        sampled_images.append(animal_images[i, j])\n",
    "\n",
    "    x = torch.stack(sampled_images).to(device)\n",
    "    x_1 = (2*x - 1)\n",
    "    # Get the encodings using E of shape (bs,100,1,1)\n",
    "    z = (E(x_1).squeeze(-1).squeeze(-1))\n",
    "    y = N(z) # Output of shape (bs,90)\n",
    "    loss_N = criterion(y, i_indices_0)\n",
    "    loss_N.backward()\n",
    "    N_optimizer.step()\n",
    "    N_optimizer.zero_grad()\n",
    "    N_loss.append(loss_N.item())\n",
    "\n",
    "    # print(f\"Epoch [{epoch+1}/{num_epochs}] | N Loss: {N_loss[-1]:.4f}\")\n",
    "    # stop = time.time()\n",
    "    # print(stop - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IibdUDSYCnrc"
   },
   "outputs": [],
   "source": [
    "# Create a figure for the plot\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "# Plot N_loss\n",
    "N_steps = range(len(N_loss))  # X-axis values (iterations)\n",
    "ax.plot(N_steps, N_loss)      # Y-axis values (D_loss)\n",
    "# Set labels and title\n",
    "ax.set_xlabel('Mini-batch iteration')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('N_loss')\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-aNgC9h-6Nni"
   },
   "outputs": [],
   "source": [
    "# Determine the final classification accuracy on TRAINING SET\n",
    "Correct = 0\n",
    "for i in range(0,90):\n",
    "    with torch.no_grad():\n",
    "        # bs = 60\n",
    "        x = animal_images[i].to(device)\n",
    "        z = (E(x).squeeze(-1).squeeze(-1))\n",
    "        y = N(z)\n",
    "    prediction = torch.argmax(y, dim=1)\n",
    "    # (60,)\n",
    "    for j in range(60):\n",
    "        if (int(prediction[j]) == i):\n",
    "            Correct += 1\n",
    "print(\"Final Accuracy of MLP based on decoder outputs\", Correct/54)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BbyY68c0KK5X"
   },
   "source": [
    "\n",
    "*  An MLP trained on the latents after (2000 mini-batch epochs for the {GAN + Decoder}) initially gives 1% to around 4% accuracy\n",
    "*  On further training CE_Loss decreases to around 0.3 but accuracy drops to 1% again which shows that Latents learnt in previous Decoder isnt good and that CE_Loss doesnt directly correlate with Accuracy\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

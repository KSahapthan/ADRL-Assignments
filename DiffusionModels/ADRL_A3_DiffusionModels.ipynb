{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ye2aDXvUiGGa"
   },
   "source": [
    "**GROUP 1 : ASSIGNMENT 3 : DiffusionModels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vP0_dyvdyOS-"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import InterpolationMode\n",
    "import os\n",
    "import zipfile\n",
    "import random\n",
    "from PIL import Image\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from scipy.linalg import sqrtm\n",
    "from torchvision import transforms, models\n",
    "# import pandas as pd\n",
    "# import json\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader,Dataset,random_split,TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_T-3oYyMiXEt"
   },
   "outputs": [],
   "source": [
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# Dont forget to use the GPU using .to(device)!\n",
    "# Use anaconda3/bin/python 3.12.4 base environment while using FIST server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x-Lo-xY1icwj"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LbaOC0AqZTov"
   },
   "outputs": [],
   "source": [
    "# Loading the dataset in [0,1] range\n",
    "# animal_images = torch.load(\"/home/sahapthank/models/training_images.pt\")\n",
    "butterfly_images = torch.load(\"/home/sahapthank/models/butterfly_training_images.pth\")\n",
    "# butterfly_labels = torch.load(\"/home/sahapthank/models/butterfly_training_classindices.pth\")\n",
    "# anime_images = torch.load(\"/home/sahapthank/models/anime_images.pth\")\n",
    "# animal_images_augmented = torch.load(\"/home/sahapthank/models/animal_images_augmented.pth\")\n",
    "# butterfly_images_augmented = torch.load(\"/home/sahapthank/models/butterfly_images_augmented.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u6P5xpe-ZZ5M"
   },
   "outputs": [],
   "source": [
    "# DATA AUGMENTATION FOR BUTTERFLY\n",
    "horizontal_flip = transforms.RandomHorizontalFlip(p=1)    # Randomly flip the image horizontally\n",
    "rotation = transforms.RandomRotation(degrees=20)    # Random rotation with expanding size\n",
    "color_jitter = transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)  # Random color adjustments\n",
    "\n",
    "tensor_shape = (6499,3,128,128)\n",
    "flipped_images = torch.zeros(tensor_shape)\n",
    "rotated_images = torch.zeros(tensor_shape)\n",
    "color_jittered_images = torch.zeros(tensor_shape)\n",
    "\n",
    "for i in range(6499):\n",
    "    flipped_images[i] = horizontal_flip(butterfly_images[i])\n",
    "    rotated_images[i] = rotation(butterfly_images[i])\n",
    "    color_jittered_images[i]= color_jitter(butterfly_images[i])\n",
    "\n",
    "butterfly_images_augmented = torch.stack([butterfly_images,flipped_images,rotated_images,color_jittered_images])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ABDcLgA6ZaqP"
   },
   "outputs": [],
   "source": [
    "# DATA AUGMENTATION FOR ANIMAL\n",
    "horizontal_flip = transforms.RandomHorizontalFlip(p=1)    # Randomly flip the image horizontally\n",
    "rotation = transforms.RandomRotation(degrees=20)    # Random rotation with expanding size\n",
    "color_jitter = transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)  # Random color adjustments\n",
    "\n",
    "tensor_shape = (90,60,3,128,128)\n",
    "flipped_images = torch.zeros(tensor_shape)\n",
    "rotated_images = torch.zeros(tensor_shape)\n",
    "color_jittered_images = torch.zeros(tensor_shape)\n",
    "\n",
    "for i in range(90):\n",
    "    for j in range(60):\n",
    "        flipped_images[i][j] = horizontal_flip(animal_images[i][j])\n",
    "        rotated_images[i][j] = rotation(animal_images[i][j])\n",
    "        color_jittered_images[i][j] = color_jitter(animal_images[i][j])\n",
    "\n",
    "animal_images_augmented = torch.stack([animal_images,flipped_images,rotated_images,color_jittered_images])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpU5l_4vZxmQ"
   },
   "outputs": [],
   "source": [
    "# ANOTHER WAY OF AUGMENTATION FOR BUTTERFLY [preferred]\n",
    "horizontal_flip = transforms.RandomHorizontalFlip(p=1)    # Randomly flip the image horizontally\n",
    "flipped_images = torch.stack([horizontal_flip(img) for img in butterfly_images])\n",
    "# Define the elastic transform\n",
    "elastic_transform = transforms.ElasticTransform(\n",
    "    alpha=50.0,\n",
    "    sigma=5.0,\n",
    "    interpolation=InterpolationMode.BILINEAR,\n",
    "    fill=0\n",
    ")\n",
    "# Apply the elastic transform to each image in the batch\n",
    "deformed_images = torch.stack([elastic_transform(img) for img in butterfly_images])\n",
    "butterfly_images_augmented_2 = torch.cat((butterfly_images,flipped_images),dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AJ-OHF08aHSb"
   },
   "outputs": [],
   "source": [
    "# CHANGE THIS CODE BASED ON DATASET\n",
    "training_images = torch.reshape(butterfly_images_augmented_2,(-1,3,128,128))\n",
    "# Convert values from [0, 1] to [-1, 1]\n",
    "training_images = ((2 * training_images) - 1)\n",
    "print(training_images.size())\n",
    "first_image = (training_images[0] + 1)/2\n",
    "first_image = first_image.permute(1, 2, 0)\n",
    "plt.imshow(first_image)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "print(torch.min(training_images))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "esSZyF-BZlCr"
   },
   "outputs": [],
   "source": [
    "# Sample from a Gaussian for inference\n",
    "def sample_gaussian(mean, covariance_matrix , n_samples):\n",
    "    # mean should be of size ([d])\n",
    "    # variance is the covariance matrix of size ([d,d]) must be +ve semidefinite\n",
    "    z = torch.distributions.MultivariateNormal(mean,covariance_matrix)\n",
    "    return z.sample([n_samples])\n",
    "    # returns of size ([n_samples,d])\n",
    "\n",
    "# Sample from \"CURRENT TRAINING IMAGES\"\n",
    "def sample_train(n_samples):\n",
    "    i_indices = torch.randint(0, training_images.size(0), (n_samples,)).tolist()\n",
    "    sampled_images = []\n",
    "    for i in i_indices:\n",
    "        sampled_images.append(training_images[i])\n",
    "    sampled_images_tensor = torch.stack(sampled_images)\n",
    "    return sampled_images_tensor\n",
    "\n",
    "# Sample from Real_images [BUTTERFLY]\n",
    "def sample_real_butterfly(n_samples):\n",
    "    i_indices = torch.randint(0, 6499, (n_samples,)).tolist()\n",
    "    sampled_images = []\n",
    "    for i in i_indices:\n",
    "        sampled_images.append(butterfly_images[i])\n",
    "    sampled_images_tensor = torch.stack(sampled_images)\n",
    "    return sampled_images_tensor\n",
    "\n",
    "# Sample from Real_images [ANIMAL]\n",
    "def sample_real_animal(n_samples):\n",
    "    i_indices = torch.randint(0, 90, (n_samples,)).tolist() # chooses class\n",
    "    j_indices = torch.randint(0, 60, (n_samples,)).tolist() # chooses image in class\n",
    "    sampled_images = []\n",
    "    for i, j in zip(i_indices, j_indices):\n",
    "        sampled_images.append(animal_images[i, j])\n",
    "    sampled_images_tensor = torch.stack(sampled_images)\n",
    "    return sampled_images_tensor\n",
    "\n",
    "# Sample from Real_images [ANIMAL] including augmented\n",
    "def sample_real_augmented_animal(n_samples):\n",
    "    i_indices = torch.randint(0, 90, (n_samples,)).tolist() # chooses class\n",
    "    j_indices = torch.randint(0, 60, (n_samples,)).tolist() # chooses image in class\n",
    "    k_indices = torch.randint(0, 4, (n_samples,)).tolist() # chooses augmentation type\n",
    "    sampled_images = []\n",
    "    for i, j , k in zip(i_indices, j_indices , k_indices):\n",
    "        sampled_images.append(animal_images_augmented[k, i, j])\n",
    "    sampled_images_tensor = torch.stack(sampled_images)\n",
    "    return sampled_images_tensor\n",
    "\n",
    "# Sample from Real_images [BUTTERFLY] including augmented\n",
    "def sample_real_augmented_butterfly(n_samples):\n",
    "    i_indices = torch.randint(0, 6499, (n_samples,)).tolist() # chooses image\n",
    "    k_indices = torch.randint(0, 4, (n_samples,)).tolist() # chooses augmentation type\n",
    "    sampled_images = []\n",
    "    for i, k in zip(i_indices, k_indices):\n",
    "        sampled_images.append(butterfly_images_augmented[k, i])\n",
    "    sampled_images_tensor = torch.stack(sampled_images)\n",
    "    return sampled_images_tensor\n",
    "\n",
    "# Sample from Real_images [ANIME]\n",
    "def sample_real_anime(n_samples):\n",
    "    i_indices = torch.randint(0, 21551, (n_samples,)).tolist()\n",
    "    sampled_images = []\n",
    "    for i in i_indices:\n",
    "        sampled_images.append(anime_images[i])\n",
    "    sampled_images_tensor = torch.stack(sampled_images)\n",
    "    return sampled_images_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GAIKGJeUikDh"
   },
   "source": [
    "**[Q1,2] DDPM : Denoising Diffusion Probabilistic Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M2SCXJ5uh_5F"
   },
   "source": [
    "**U-Net Architecture [Without TimeEmbeddings]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pTSno-vPiDzI"
   },
   "outputs": [],
   "source": [
    "# UNet class\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, A):\n",
    "        super(UNet, self).__init__()\n",
    "        self.down_blocks = nn.ModuleList()\n",
    "        self.bottleneck = nn.ModuleList()\n",
    "        self.up_blocks = nn.ModuleList()\n",
    "        self.final_conv = nn.Sequential(nn.Conv2d(A[3][0], A[3][1], kernel_size=A[3][2], stride=A[3][3], padding = A[3][4]), nn.Tanh())\n",
    "\n",
    "        # A[0] contains all info about down_blocks\n",
    "        # A[0] = [down_block_1,down_block_2,...]\n",
    "        # down_block_1 = [2,[3,64,3,1,0],nn.ReLU(),2]\n",
    "        # A[1] contains information about BottleNeck layer\n",
    "        # A[1] = [2,[512,1024,3,1,0],nn.ReLU()]\n",
    "        # A[2] contains all information about up_blocks in same way as A[0]\n",
    "        # A[2] = [up_block_1,up_block_2,...]\n",
    "        # up_block_1 = [2,[1024,512,3,1,0],nn.ReLU(),[1024,512,2,2,0]]\n",
    "        # A[3] = [64,3,1,1,0] is the 1x1 final_conv\n",
    "\n",
    "        # Downsampling path\n",
    "        for i in range(len(A[0])):\n",
    "            block = nn.ModuleList()\n",
    "            num_conv_layers = A[0][i][0]\n",
    "            conv_specs = A[0][i][1]\n",
    "            activation = A[0][i][2]\n",
    "            i_feature = conv_specs[0]\n",
    "            o_feature = conv_specs[1]\n",
    "            k = conv_specs[2]\n",
    "            s = conv_specs[3]\n",
    "            p = conv_specs[4]\n",
    "            for _ in range(num_conv_layers):\n",
    "                conv = nn.Conv2d(i_feature, o_feature, kernel_size = k , stride = s, padding = p)\n",
    "                block.append(nn.Sequential(conv, nn.BatchNorm2d(o_feature), activation))\n",
    "                # Update #channels for next conv_layer\n",
    "                i_feature = o_feature\n",
    "            self.down_blocks.append(block)\n",
    "            self.down_blocks.append(nn.MaxPool2d(A[0][i][3]))\n",
    "\n",
    "        # Bottleneck layer\n",
    "        i_feature = A[1][1][0]\n",
    "        o_feature = A[1][1][1]\n",
    "        k = A[1][1][2]\n",
    "        s = A[1][1][3]\n",
    "        p = A[1][1][4]\n",
    "        for _ in range(A[1][0]):\n",
    "            bottleneck_conv = nn.Conv2d(i_feature, o_feature, k, stride=s, padding=p)\n",
    "            self.bottleneck.append(nn.Sequential(bottleneck_conv, nn.BatchNorm2d(o_feature), A[1][2]))\n",
    "            # Update it\n",
    "            i_feature = o_feature\n",
    "\n",
    "        # Upsampling path\n",
    "        for i in range(len(A[2])):\n",
    "            # Transposed Conv for upsampling instead of max-pooling\n",
    "            up_A = A[2][i][3]\n",
    "            up = nn.ConvTranspose2d(up_A[0], up_A[1], kernel_size=up_A[2], stride=up_A[3] , padding=up_A[4])\n",
    "            self.up_blocks.append(up)\n",
    "            block = nn.ModuleList()\n",
    "            num_conv_layers = A[2][i][0]\n",
    "            conv_specs = A[2][i][1]\n",
    "            activation = A[2][i][2]\n",
    "            i_feature = conv_specs[0]\n",
    "            o_feature = conv_specs[1]\n",
    "            k = conv_specs[2]\n",
    "            s = conv_specs[3]\n",
    "            p = conv_specs[4]\n",
    "            for _ in range(num_conv_layers):\n",
    "                conv = nn.Conv2d(i_feature, o_feature, kernel_size = k , stride = s, padding = p)\n",
    "                block.append(nn.Sequential(conv, nn.BatchNorm2d(o_feature), activation))\n",
    "                # Update #channels for next conv_layer\n",
    "                i_feature = o_feature\n",
    "            self.up_blocks.append(block)\n",
    "\n",
    "    # self.down_blocks is an instance of nn.ModuleList()\n",
    "    # It stores all the downsampling blocks as individual modules + MaxPooling\n",
    "    # Each downsampling block is also nn.ModuleList containing a sequence of {ConvLayers + BN + Activation}\n",
    "    def forward(self, x):\n",
    "        # To store the outputs for skip connections\n",
    "        encoder_outputs = []\n",
    "\n",
    "        # Downsampling path\n",
    "        for i in range(0, len(self.down_blocks), 2):\n",
    "            block = self.down_blocks[i]\n",
    "            # block is nn.ModuleList()\n",
    "            for layer in block:\n",
    "                # Each layer is nn.Sequential()\n",
    "                x = layer(x)\n",
    "            encoder_outputs.append(x)\n",
    "            # Perform MaxPooling\n",
    "            x = self.down_blocks[i+1](x)\n",
    "\n",
    "        # Bottleneck\n",
    "        for layer in self.bottleneck:\n",
    "            x = layer(x)\n",
    "\n",
    "        # Upsampling path\n",
    "        for i in range(0, len(self.up_blocks), 2):\n",
    "            # Upsample the feature map\n",
    "            up_conv = self.up_blocks[i]\n",
    "            x = up_conv(x)\n",
    "            # Retrieve the corresponding feature map from downsampling for skip connection\n",
    "            skip_connection = encoder_outputs[-(i//2 + 1)]\n",
    "            # Resize skip connection to match x dimensions (using crop or interpolation)\n",
    "            skip_connection = F.interpolate(skip_connection, size=x.shape[2:], mode='bilinear', align_corners=False)\n",
    "            # Concatenate skip connection with upsampled feature map\n",
    "            x = torch.cat([x, skip_connection], dim=1)\n",
    "            block = self.up_blocks[i+1]\n",
    "            for layer in block:\n",
    "                x = layer(x)\n",
    "\n",
    "        # Final output\n",
    "        x = self.final_conv(x)\n",
    "        return x\n",
    "\n",
    "    # We will use this to prevent calculating gradients wrt parameters of UNet\n",
    "    def set_requires_grad(self, requires_grad):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = requires_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tNcuIxEzeZKk"
   },
   "outputs": [],
   "source": [
    "# The original Unet architecture for reference\n",
    "# They used {2 times 3x3 Conv(p=0,s=1) + Relu + 2x2 MaxPooling(s=2)} in the downsampling\n",
    "# (2,572,572) becomes (512,32,32) on reaching the BottleNeck Layer\n",
    "# There they use {2 times 3x3 Conv(p=0,s=1) + Relu + No MaxPooling}\n",
    "# i/p to upsampling is (1024,28,28)\n",
    "# In the upsampling procedure max pooling is replaced by {up_con/trans_conc of 2x2 and feature maps are halved here}\n",
    "# The feature maps from the encoder are concatenated with the upsampled feature maps (skip connections)\n",
    "# Copy and crop is used to attach since feature maps of encoder have slightly larger spatial size due to bottleneck layer\n",
    "# Then they use  {2 times 3x3 Conv(p=0,s=1) + Relu} to reduce feature maps progressively\n",
    "# Final o/p before last operation is (64,388,388)\n",
    "# Last step is 1x1 Conv to (2,388,388) the o/p segmentation map\n",
    "# 31M parameters\n",
    "A_Original = [\n",
    "    # A[0]: Downsampling blocks\n",
    "    [\n",
    "        # Down block 1\n",
    "        [2, [3, 64, 3, 1, 0], nn.ReLU(), 2],\n",
    "        # Down block 2\n",
    "        [2, [64, 128, 3, 1, 0], nn.ReLU(), 2],\n",
    "        # Down block 3\n",
    "        [2, [128, 256, 3, 1, 0], nn.ReLU(), 2],\n",
    "        # Down block 4\n",
    "        [2, [256, 512, 3, 1, 0], nn.ReLU(), 2]\n",
    "    ],\n",
    "\n",
    "    # A[1]: Bottleneck layer\n",
    "    [2, [512, 1024, 3, 1, 0], nn.ReLU()],\n",
    "\n",
    "    # A[2]: Upsampling blocks\n",
    "    [\n",
    "        # Up block 1\n",
    "        [2, [1024, 512, 3, 1, 0], nn.ReLU(), [1024, 512, 2, 2, 0]],\n",
    "        # Up block 2\n",
    "        [2, [512, 256, 3, 1, 0], nn.ReLU(), [512, 256, 2, 2, 0]],\n",
    "        # Up block 3\n",
    "        [2, [256, 128, 3, 1, 0], nn.ReLU(), [256, 128, 2, 2, 0]],\n",
    "        # Up block 4\n",
    "        [2, [128, 64, 3, 1, 0], nn.ReLU(), [128, 64, 2, 2, 0]]\n",
    "    ],\n",
    "\n",
    "    # A[3]: Final 1x1 convolution\n",
    "    [64, 2, 1, 1, 0]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NLfuyxs6oykc"
   },
   "outputs": [],
   "source": [
    "# Just checking!\n",
    "U_check = UNet(A_Original)\n",
    "# print(model)\n",
    "a = torch.randn(7, 3, 572, 572)\n",
    "b = U_check(a)\n",
    "print(b.shape)\n",
    "TP1 = sum(p.numel() for p in U_check.parameters())\n",
    "print(TP1)\n",
    "# o/p is [7, 2, 388, 388] as expected!\n",
    "# 31,043,586"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zwcYE03YYr4d"
   },
   "source": [
    "**Unet Architecture [with Time Embeddings]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EjthWBgiax6p"
   },
   "source": [
    "* Main modification is Time embeddings are added after each Convolutional Block before applying the Pooling operation\n",
    "* Both MaxPooling and AvgPooling can be tried\n",
    "* Projection matrices are used to project time embeddings to the appropriate output channel size in each layer. Once projected the same is added to every pixel\n",
    "* Padding=1 is used instead of zero as in the original Unet paper. This helps to preserve the dimensions inside each block after the Convolutions. Reduction in spatial dimension is only achieved via pooling\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "18ixqfk7Yrf7"
   },
   "outputs": [],
   "source": [
    "# Sinusoidal embeddings are a technique used to encode position information, commonly used in Transformer architectures\n",
    "# Sin,Cos help the model to represent position of tokens in a continuous fashion\n",
    "# Suppose positional embedding is of dimension \"d\"\n",
    "# Then PE(t,2i) = sin(t/[10000]^{2i/d}) , PE(t,2i+1) = cos(t/[10000]^{2i/d})\n",
    "# These embedding gets added into the input of each residual block in the U-net\n",
    "\n",
    "# 1) The time embedding get added at end of each encoder block before max-pooling\n",
    "# 2) These additions affect the further encoder block outputs\n",
    "# 3) In upsampling the time embedding is added after the up_conv2d before convolutions\n",
    "# 4) Then this further affects the concatenation and further upsampling outputs\n",
    "\n",
    "# Typically the time embedding is initially is a 1D tensor of shape say (T,d)\n",
    "# Where T is for max_time_steps and d is embedding dimensions\n",
    "# Since both spatial and #channels vary in all the encoder outputs as well as upsampling\n",
    "# What we do is project \"d\" onto #channels and use expand to (H,W) that is the same embedding in all pixels!\n",
    "\n",
    "# A very important POINT TO Note\n",
    "# If we simply modify the contents of x in-place (for example, x += 1 or x.mul_(2)), then the original x_t will also reflect those changes\n",
    "# as both x and x_t reference the same data.\n",
    "# However, if we reassign x to a new tensor (e.g., x = layer(x)), we are not modifying the original tensor.\n",
    "# Instead, we are changing what x points to. The original tensor x_t remains unchanged.\n",
    "\n",
    "# UNet class for DDPM\n",
    "class UNet_DDPM(nn.Module):\n",
    "    def __init__(self, A, T , d , device):\n",
    "        super(UNet_DDPM, self).__init__()\n",
    "        self.down_blocks = nn.ModuleList()\n",
    "        self.bottleneck = nn.ModuleList()\n",
    "        self.up_blocks = nn.ModuleList()\n",
    "        self.final_conv = nn.Sequential(nn.Conv2d(A[3][0], A[3][1], kernel_size=A[3][2], stride=A[3][3], padding = A[3][4]), nn.Tanh())\n",
    "        # self.final_conv = nn.Sequential(nn.Conv2d(A[3][0], A[3][1], kernel_size=A[3][2], stride=A[3][3], padding = A[3][4]))\n",
    "\n",
    "        # Positional embeddings for all time T each of dim d\n",
    "        positions = torch.arange(T, dtype=torch.float).unsqueeze(1)  # (T, 1)\n",
    "        dimensions = torch.arange(d, dtype=torch.float).unsqueeze(0)  # (1, d)\n",
    "        angle_rates = 1 / (10000 ** (dimensions / d))  # (1, d)\n",
    "        angles = positions * angle_rates  # (T, d)\n",
    "        # Apply the sine and cosine functions\n",
    "        pos_embedding = torch.empty((T, d))\n",
    "        pos_embedding[:, 0::2] = torch.sin(angles[:, 0::2])  # Apply sine to even indices\n",
    "        pos_embedding[:, 1::2] = torch.cos(angles[:, 1::2])  # Apply cosine to odd indices\n",
    "        self.time_embedding = pos_embedding.to(device)\n",
    "\n",
    "        #Collecting feature map numbers and Setting up the projection maps\n",
    "        downsampling_features = []\n",
    "        upsampling_features = []\n",
    "        for block in A[0]:\n",
    "            downsampling_features.append(block[1][1])\n",
    "        for block in A[2]:\n",
    "            upsampling_features.append(block[1][0]//2)\n",
    "        # Creating projection layers as nn.ModuleList\n",
    "        self.downsampling_projections = nn.ModuleList([nn.Linear(d, out_features) for out_features in downsampling_features])\n",
    "        self.upsampling_projections = nn.ModuleList([nn.Linear(d, out_features) for out_features in upsampling_features])\n",
    "\n",
    "        # A[0] contains all info about down_blocks\n",
    "        # A[0] = [down_block_1,down_block_2,...]\n",
    "        # down_block_1 = [2,[3,64,3,1,0],nn.ReLU(),2]\n",
    "        # A[1] contains information about BottleNeck layer\n",
    "        # A[1] = [2,[512,1024,3,1,0],nn.ReLU()]\n",
    "        # A[2] contains all information about up_blocks in same way as A[0]\n",
    "        # A[2] = [up_block_1,up_block_2,...]\n",
    "        # up_block_1 = [2,[1024,512,3,1,0],nn.ReLU(),[1024,512,2,2,0]]\n",
    "        # A[3] = [64,3,1,1,0] is the 1x1 final_conv\n",
    "\n",
    "        # Downsampling path\n",
    "        for i in range(len(A[0])):\n",
    "            block = nn.ModuleList()\n",
    "            num_conv_layers = A[0][i][0]\n",
    "            conv_specs = A[0][i][1]\n",
    "            activation = A[0][i][2]\n",
    "            i_feature = conv_specs[0]\n",
    "            o_feature = conv_specs[1]\n",
    "            k = conv_specs[2]\n",
    "            s = conv_specs[3]\n",
    "            p = conv_specs[4]\n",
    "            for _ in range(num_conv_layers):\n",
    "                conv = nn.Conv2d(i_feature, o_feature, kernel_size = k , stride = s, padding = p)\n",
    "                block.append(nn.Sequential(conv, nn.BatchNorm2d(o_feature), activation))\n",
    "                # Update #channels for next conv_layer\n",
    "                i_feature = o_feature\n",
    "            self.down_blocks.append(block)\n",
    "            self.down_blocks.append(nn.MaxPool2d(A[0][i][3]))\n",
    "\n",
    "        # Bottleneck layer\n",
    "        i_feature = A[1][1][0]\n",
    "        o_feature = A[1][1][1]\n",
    "        k = A[1][1][2]\n",
    "        s = A[1][1][3]\n",
    "        p = A[1][1][4]\n",
    "        for _ in range(A[1][0]):\n",
    "            bottleneck_conv = nn.Conv2d(i_feature, o_feature, k, stride=s, padding=p)\n",
    "            self.bottleneck.append(nn.Sequential(bottleneck_conv, nn.BatchNorm2d(o_feature), A[1][2]))\n",
    "            # Update it\n",
    "            i_feature = o_feature\n",
    "\n",
    "        # Upsampling path\n",
    "        for i in range(len(A[2])):\n",
    "            # Transposed Conv for upsampling instead of max-pooling\n",
    "            up_A = A[2][i][3]\n",
    "            up = nn.ConvTranspose2d(up_A[0], up_A[1], kernel_size=up_A[2], stride=up_A[3] , padding=up_A[4])\n",
    "            self.up_blocks.append(up)\n",
    "            block = nn.ModuleList()\n",
    "            num_conv_layers = A[2][i][0]\n",
    "            conv_specs = A[2][i][1]\n",
    "            activation = A[2][i][2]\n",
    "            i_feature = conv_specs[0]\n",
    "            o_feature = conv_specs[1]\n",
    "            k = conv_specs[2]\n",
    "            s = conv_specs[3]\n",
    "            p = conv_specs[4]\n",
    "            for _ in range(num_conv_layers):\n",
    "                conv = nn.Conv2d(i_feature, o_feature, kernel_size = k , stride = s, padding = p)\n",
    "                block.append(nn.Sequential(conv, nn.BatchNorm2d(o_feature), activation))\n",
    "                # Update #channels for next conv_layer\n",
    "                i_feature = o_feature\n",
    "            self.up_blocks.append(block)\n",
    "\n",
    "    # self.down_blocks is an instance of nn.ModuleList()\n",
    "    # It stores all the downsampling blocks as individual modules + MaxPooling\n",
    "    # Each downsampling block is also nn.ModuleList containing a sequence of {ConvLayers + BN + Activation}\n",
    "    def forward(self, x, t):\n",
    "        # x is of shape (bs,C,H,W)\n",
    "        # t of shape (bs,) and take care of indices since t in [1,2,..T]\n",
    "        time_embedding = self.time_embedding[t-1]\n",
    "        # To store the outputs for skip connections\n",
    "        encoder_outputs = []\n",
    "\n",
    "        # Downsampling path\n",
    "        for i in range(0, len(self.down_blocks), 2):\n",
    "            block = self.down_blocks[i]\n",
    "            # block is nn.ModuleList()\n",
    "            for layer in block:\n",
    "                # Each layer is nn.Sequential()\n",
    "                x = layer(x)\n",
    "            projected_embedding = self.downsampling_projections[i//2](time_embedding).unsqueeze(-1).unsqueeze(-1)\n",
    "            projected_embedding = projected_embedding.expand_as(x)\n",
    "            x = x + projected_embedding\n",
    "            encoder_outputs.append(x)\n",
    "            # Perform MaxPooling\n",
    "            x = self.down_blocks[i+1](x)\n",
    "\n",
    "        # Bottleneck\n",
    "        for layer in self.bottleneck:\n",
    "            x = layer(x)\n",
    "\n",
    "        # Upsampling path\n",
    "        for i in range(0, len(self.up_blocks), 2):\n",
    "            # Upsample the feature map\n",
    "            up_conv = self.up_blocks[i]\n",
    "            x = up_conv(x)\n",
    "            # Retrieve the corresponding feature map from downsampling for skip connection\n",
    "            skip_connection = encoder_outputs[-(i//2 + 1)]\n",
    "            # Resize skip connection to match x dimensions (using crop or interpolation)\n",
    "            skip_connection = F.interpolate(skip_connection, size=x.shape[2:], mode='bilinear', align_corners=False)\n",
    "            # Concatenate skip connection with upsampled feature map after incorporating time_embedding\n",
    "            projected_embedding = self.upsampling_projections[i//2](time_embedding).unsqueeze(-1).unsqueeze(-1)\n",
    "            projected_embedding = projected_embedding.expand_as(x)\n",
    "            x = x + projected_embedding\n",
    "            x = torch.cat([x, skip_connection], dim=1)\n",
    "            block = self.up_blocks[i+1]\n",
    "            for layer in block:\n",
    "                x = layer(x)\n",
    "\n",
    "        # Final output\n",
    "        x = self.final_conv(x)\n",
    "        return x\n",
    "\n",
    "    # We will use this to prevent calculating gradients wrt parameters of UNet_DDPM\n",
    "    def set_requires_grad(self, requires_grad):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = requires_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Va1Vt07OxEY9"
   },
   "outputs": [],
   "source": [
    "# Just checking!\n",
    "U_check = UNet_DDPM(A_Original , 1000, 64 , \"cpu\")\n",
    "a = torch.randn(7, 3, 572, 572)\n",
    "t0 = torch.randint(low=0, high=1001, size=(7,))\n",
    "b = U_check(a , t0)\n",
    "print(b.shape)\n",
    "TP1 = sum(p.numel() for p in U_check.parameters())\n",
    "print(TP1)\n",
    "# o/p is [7, 2, 388, 388] as expected!\n",
    "# 311,68,386\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fH_4HBUJfmB-"
   },
   "source": [
    "**UNet with More Modifications specific for DDPM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Opu1IatdaOyl"
   },
   "source": [
    "* Incorporate time embeddings in BottleNeck layer as well\n",
    "* Add Self-attention maps at 16x16 spatial dimension layers\n",
    "* Can use ResidualBlocks instead of simple sequential {2,3x3 ConvLayers} in a single block\n",
    "* Can also use GroupNorm over BatchNorm2d\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N2IDIIrqaJcF"
   },
   "outputs": [],
   "source": [
    "class SelfAttention_Image(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(SelfAttention_Image, self).__init__()\n",
    "        self.W_Q = nn.Conv2d(in_channels, in_channels, 1)\n",
    "        self.W_K = nn.Conv2d(in_channels, in_channels, 1)\n",
    "        self.W_V = nn.Conv2d(in_channels, in_channels, 1)\n",
    "        self.scale = 1.0 / (in_channels ** 0.5)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, channels, height, width = x.size()\n",
    "        q = self.W_Q(x).view(batch, channels, -1)  # Shape: (batch, channels, H*W)\n",
    "        k = self.W_K(x).view(batch, channels, -1)\n",
    "        v = self.W_V(x).view(batch, channels, -1)\n",
    "        attn = torch.bmm(q.transpose(1, 2), k) * self.scale  # (batch, H*W, H*W)\n",
    "        attn = self.softmax(attn)\n",
    "        out = torch.bmm(v , attn.transpose(1, 2)).view(batch, channels, height, width)\n",
    "        return out + x  # Residual connection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vaWu5lE3fq6v"
   },
   "outputs": [],
   "source": [
    "\"\"\"U-Net, originally developed for semantic segmentation, is used in DDPMs with some slight modifications\n",
    "The network accepts two inputs: Image and time step\n",
    "Time Embeddings: Integrated at every skip connection and the bottleneck layer.\n",
    "Activation Functions: Swish or Leaky ReLU used instead of standard ReLU.\n",
    "Normalization: Group Normalization replaces Batch Normalization.\n",
    "Attention Mechanism: Often included, but not needed if not performing text conditional generation\"\"\"\n",
    "\n",
    "class UNet_DDPM_1(nn.Module):\n",
    "    def __init__(self, A, T , d , device):\n",
    "        super(UNet_DDPM_1, self).__init__()\n",
    "        self.down_blocks = nn.ModuleList()\n",
    "        self.bottleneck = nn.ModuleList()\n",
    "        self.up_blocks = nn.ModuleList()\n",
    "        self.final_conv = nn.Sequential(nn.Conv2d(A[3][0], A[3][1], kernel_size=A[3][2], stride=A[3][3], padding = A[3][4]), nn.Tanh())\n",
    "        # self.final_conv = nn.Sequential(nn.Conv2d(A[3][0], A[3][1], kernel_size=A[3][2], stride=A[3][3], padding = A[3][4]))\n",
    "\n",
    "        # Positional embeddings for all time T each of dim d\n",
    "        positions = torch.arange(T, dtype=torch.float).unsqueeze(1)  # (T, 1)\n",
    "        dimensions = torch.arange(d, dtype=torch.float).unsqueeze(0)  # (1, d)\n",
    "        angle_rates = 1 / (10000 ** (dimensions / d))  # (1, d)\n",
    "        angles = positions * angle_rates  # (T, d)\n",
    "        # Apply the sine and cosine functions\n",
    "        pos_embedding = torch.empty((T, d))\n",
    "        pos_embedding[:, 0::2] = torch.sin(angles[:, 0::2])  # Apply sine to even indices\n",
    "        pos_embedding[:, 1::2] = torch.cos(angles[:, 1::2])  # Apply cosine to odd indices\n",
    "        self.time_embedding = pos_embedding.to(device)\n",
    "\n",
    "        #Collecting feature map numbers and Setting up the projection maps\n",
    "        downsampling_features = []\n",
    "        upsampling_features = []\n",
    "        bottleneck_features = A[1][1][1]\n",
    "        for block in A[0]:\n",
    "            downsampling_features.append(block[1][1])\n",
    "        for block in A[2]:\n",
    "            upsampling_features.append(block[1][0]//2)\n",
    "\n",
    "        # Creating projection layers as nn.ModuleList\n",
    "        self.downsampling_projections = nn.ModuleList([nn.Linear(d, out_features) for out_features in downsampling_features])\n",
    "        self.upsampling_projections = nn.ModuleList([nn.Linear(d, out_features) for out_features in upsampling_features])\n",
    "        self.bottleneck_projections = nn.ModuleList([nn.Linear(d, bottleneck_features) for j in range(A[1][0])])\n",
    "\n",
    "        # Create the self-attention\n",
    "        self.down_attention = SelfAttention_Image(512)\n",
    "        self.up_attention = SelfAttention_Image(512)\n",
    "\n",
    "        # A[0] contains all info about down_blocks\n",
    "        # A[0] = [down_block_1,down_block_2,...]\n",
    "        # down_block_1 = [2,[3,64,3,1,0],nn.ReLU(),2]\n",
    "        # A[1] contains information about BottleNeck layer\n",
    "        # A[1] = [2,[512,1024,3,1,0],nn.ReLU()]\n",
    "        # A[2] contains all information about up_blocks in same way as A[0]\n",
    "        # A[2] = [up_block_1,up_block_2,...]\n",
    "        # up_block_1 = [2,[1024,512,3,1,0],nn.ReLU(),[1024,512,2,2,0]]\n",
    "        # A[3] = [64,3,1,1,0] is the 1x1 final_conv\n",
    "\n",
    "        # Downsampling path\n",
    "        for i in range(len(A[0])):\n",
    "            block = nn.ModuleList()\n",
    "            num_conv_layers = A[0][i][0]\n",
    "            conv_specs = A[0][i][1]\n",
    "            activation = A[0][i][2]\n",
    "            i_feature = conv_specs[0]\n",
    "            o_feature = conv_specs[1]\n",
    "            k = conv_specs[2]\n",
    "            s = conv_specs[3]\n",
    "            p = conv_specs[4]\n",
    "            for _ in range(num_conv_layers):\n",
    "                conv = nn.Conv2d(i_feature, o_feature, kernel_size = k , stride = s, padding = p)\n",
    "                block.append(nn.Sequential(conv, nn.BatchNorm2d(o_feature), activation))\n",
    "                # Update #channels for next conv_layer\n",
    "                i_feature = o_feature\n",
    "            self.down_blocks.append(block)\n",
    "            # self.down_blocks.append(nn.MaxPool2d(A[0][i][3]))\n",
    "            self.down_blocks.append(nn.AvgPool2d(A[0][i][3]))\n",
    "\n",
    "        # Bottleneck layer\n",
    "        i_feature = A[1][1][0]\n",
    "        o_feature = A[1][1][1]\n",
    "        k = A[1][1][2]\n",
    "        s = A[1][1][3]\n",
    "        p = A[1][1][4]\n",
    "        for _ in range(A[1][0]):\n",
    "            bottleneck_conv = nn.Conv2d(i_feature, o_feature, k, stride=s, padding=p)\n",
    "            self.bottleneck.append(nn.Sequential(bottleneck_conv, nn.BatchNorm2d(o_feature), A[1][2]))\n",
    "            # Update it\n",
    "            i_feature = o_feature\n",
    "\n",
    "        # Upsampling path\n",
    "        for i in range(len(A[2])):\n",
    "            # Transposed Conv for upsampling instead of max-pooling\n",
    "            up_A = A[2][i][3]\n",
    "            up = nn.ConvTranspose2d(up_A[0], up_A[1], kernel_size=up_A[2], stride=up_A[3] , padding=up_A[4])\n",
    "            self.up_blocks.append(up)\n",
    "            block = nn.ModuleList()\n",
    "            num_conv_layers = A[2][i][0]\n",
    "            conv_specs = A[2][i][1]\n",
    "            activation = A[2][i][2]\n",
    "            i_feature = conv_specs[0]\n",
    "            o_feature = conv_specs[1]\n",
    "            k = conv_specs[2]\n",
    "            s = conv_specs[3]\n",
    "            p = conv_specs[4]\n",
    "            for _ in range(num_conv_layers):\n",
    "                conv = nn.Conv2d(i_feature, o_feature, kernel_size = k , stride = s, padding = p)\n",
    "                block.append(nn.Sequential(conv, nn.BatchNorm2d(o_feature), activation))\n",
    "                # Update #channels for next conv_layer\n",
    "                i_feature = o_feature\n",
    "            self.up_blocks.append(block)\n",
    "\n",
    "    # self.down_blocks is an instance of nn.ModuleList()\n",
    "    # It stores all the downsampling blocks as individual modules + MaxPooling\n",
    "    # Each downsampling block is also nn.ModuleList containing a sequence of {ConvLayers + BN + Activation}\n",
    "    def forward(self, x, t):\n",
    "        # x is of shape (bs,C,H,W)\n",
    "        # t of shape (bs,) and take care of indices since t in [1,2,..T]\n",
    "        time_embedding = self.time_embedding[t-1]\n",
    "        # To store the outputs for skip connections\n",
    "        encoder_outputs = []\n",
    "\n",
    "        # Downsampling path\n",
    "        for i in range(0, len(self.down_blocks), 2):\n",
    "            block = self.down_blocks[i]\n",
    "            # block is nn.ModuleList()\n",
    "            for layer in block:\n",
    "                # Each layer is nn.Sequential()\n",
    "                x = layer(x)\n",
    "            projected_embedding = self.downsampling_projections[i//2](time_embedding).unsqueeze(-1).unsqueeze(-1)\n",
    "            projected_embedding = projected_embedding.expand_as(x)\n",
    "            x = x + projected_embedding\n",
    "            # Perform attention!\n",
    "            if (int(x.size(2)) == 16):\n",
    "                x = self.down_attention(x)\n",
    "            encoder_outputs.append(x)\n",
    "            # Perform MaxPooling\n",
    "            x = self.down_blocks[i+1](x)\n",
    "\n",
    "        # Bottleneck\n",
    "        for j,layer in enumerate(self.bottleneck):\n",
    "            x = layer(x)\n",
    "            projected_embedding = self.bottleneck_projections[j](time_embedding).unsqueeze(-1).unsqueeze(-1)\n",
    "            projected_embedding = projected_embedding.expand_as(x)\n",
    "            x = x + projected_embedding\n",
    "\n",
    "        # Upsampling path\n",
    "        for i in range(0, len(self.up_blocks), 2):\n",
    "            # Upsample the feature map\n",
    "            up_conv = self.up_blocks[i]\n",
    "            x = up_conv(x)\n",
    "            # Retrieve the corresponding feature map from downsampling for skip connection\n",
    "            skip_connection = encoder_outputs[-(i//2 + 1)]\n",
    "            # Resize skip connection to match x dimensions (using crop or interpolation)\n",
    "            skip_connection = F.interpolate(skip_connection, size=x.shape[2:], mode='bilinear', align_corners=False)\n",
    "            # Concatenate skip connection with upsampled feature map after incorporating time_embedding\n",
    "            projected_embedding = self.upsampling_projections[i//2](time_embedding).unsqueeze(-1).unsqueeze(-1)\n",
    "            projected_embedding = projected_embedding.expand_as(x)\n",
    "            x = x + projected_embedding\n",
    "            if (int(x.size(2)) == 16):\n",
    "                x = self.up_attention(x)\n",
    "            x = torch.cat([x, skip_connection], dim=1)\n",
    "            block = self.up_blocks[i+1]\n",
    "            for layer in block:\n",
    "                x = layer(x)\n",
    "\n",
    "        # Final output\n",
    "        x = self.final_conv(x)\n",
    "        return x\n",
    "\n",
    "    # We will use this to prevent calculating gradients wrt parameters of UNet_DDPM\n",
    "    def set_requires_grad(self, requires_grad):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = requires_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lWXTGFkSCAZo"
   },
   "outputs": [],
   "source": [
    "# Just checking!\n",
    "U_check = UNet_DDPM_1(A_Original , 1000, 64 , \"cpu\")\n",
    "a = torch.randn(7, 3, 572, 572)\n",
    "t0 = torch.randint(low=0, high=1001, size=(7,))\n",
    "b = U_check(a , t0)\n",
    "print(b.shape)\n",
    "TP1 = sum(p.numel() for p in U_check.parameters())\n",
    "print(TP1)\n",
    "# o/p is [7, 2, 388, 388] as expected!\n",
    "# 311,68,386\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "onVlPNgLEZqv"
   },
   "source": [
    "**General Idea of a DDPM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hkbbPpPhOtvB"
   },
   "outputs": [],
   "source": [
    "# We start with a Markovian Hierarchichal VAE := MHVAE\n",
    "# A VDM := Variational Diffusion Model has 3 key restrictions:\n",
    "# 1) latent_dim = data_dim (meaning images here)\n",
    "# 2) Structure of latent-encoder is not learnt; Predefined as Linear Gaussian [Centered at o/p of previous time-step]\n",
    "# 3) Gaussian parameters of the latent encoders vary over time in such a way that the distribution of\n",
    "# the latent at final timestep T, i.e p(x_T) is N(0,I) meaning each pixel is indepndently std Normal\n",
    "\n",
    "# From now on use the convention xt for latent at timestep [1,2,..T] and x0 for i/p image\n",
    "# {Mean ; SD} of each Latent Encoder is set before training as HyperParameters [Alpha replace by \"a\"]\n",
    "# We typically use the default parameterisation where x_t = root(a_t)*x_{t-1} + root((1-a_t))*e ; where e is N(0,I)\n",
    "# This form of {a_t} make the Encoder Variance preserving\n",
    "# {a_t} can also be learnt but generally in practice kept fixed!\n",
    "# It is a steady noisification of an image input over time\n",
    "# Only interested in learning the conditionals p_theta(x_{t-1}|x_t)\n",
    "\n",
    "# Both for image prediction and error prediction UNET is only being used as final o/p has dim as i/p image\n",
    "# Note THAT FOR DDPM Training images going to be in [-1,1]\n",
    "\n",
    "class DDPM_Denoiser(nn.Module):\n",
    "    def __init__(self, T, beta_start, beta_end , A , embedding_dim , device):\n",
    "        super(DDPM_Denoiser, self).__init__()\n",
    "        # Define {alpha_t := a_t = 1 - beta_t} and {b_t := a_t \"bar\" = alphas_cumprod}\n",
    "        # {alpha_t} decrease for noising and thereby {beta_t} increase\n",
    "        self.T = T\n",
    "        # Define a linear beta schedule (common practice is 1e-4 to 0.02)\n",
    "        # Alphas are supposed to be defined from 1 to T only! So be careful with indices!\n",
    "        self.betas = torch.linspace(beta_start, beta_end, steps=self.T).to(device)  # (T,)\n",
    "        self.alphas = (1 - self.betas).to(device)  # (T,)\n",
    "        self.alphas_cumprod = (torch.cumprod(self.alphas, dim=0)).to(device)  # (T,)\n",
    "\n",
    "        # Store the sqrt directly for ease of computation\n",
    "        self.sqrt_alphas = torch.sqrt(self.alphas)\n",
    "        self.sqrt_betas = torch.sqrt(self.betas)\n",
    "        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n",
    "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1 - self.alphas_cumprod)\n",
    "\n",
    "        # Store the following for easy inference/training\n",
    "        # mean_q_t;0  and sigma_squared_q are used [1 to T] but indexed [0,T-1]\n",
    "        self.mean_q_t = torch.zeros(self.T).to(device)  # (T,)\n",
    "        self.mean_q_t[0] = 0\n",
    "        for i in range(1,self.T):\n",
    "            self.mean_q_t[i] = (1 - self.alphas_cumprod[i-1]) * self.sqrt_alphas[i] / (1 - self.alphas_cumprod[i])\n",
    "        self.mean_q_0 = torch.zeros(self.T).to(device)  # (T,)\n",
    "        self.mean_q_0[0] = 1\n",
    "        for i in range(1,self.T):\n",
    "            self.mean_q_0[i] = (self.betas[i]) * self.sqrt_alphas_cumprod[i-1] / (1 - self.alphas_cumprod[i])\n",
    "        self.sigma_squared_q = torch.zeros(self.T).to(device)  # (T,)\n",
    "        self.sigma_squared_q[0] = 0\n",
    "        for i in range(1,self.T):\n",
    "            self.sigma_squared_q[i] = (self.betas[i]) * (1 - self.alphas_cumprod[i-1]) / (1 - self.alphas_cumprod[i])\n",
    "        self.sigma_q = torch.sqrt(self.sigma_squared_q)\n",
    "        self.e_q = (self.betas/self.sqrt_one_minus_alphas_cumprod)\n",
    "        self.unet = UNet_DDPM_1(A , self.T , embedding_dim , device).to(device)\n",
    "\n",
    "    # Forward pass via the Unet\n",
    "    def forward(self, x, t):\n",
    "        # x is of shape (bs, C, H, W)\n",
    "        # t is of shape (bs,) containing the time steps [int]\n",
    "        return self.unet(x,t)\n",
    "\n",
    "    # Helper functions inside the Denoiser Class itself\n",
    "    def q_sample(self, encoding, t):\n",
    "        # ENSURE t of shape (bs,) in range[1,...T]\n",
    "        batch_size = encoding.shape[0]\n",
    "        mean = self.sqrt_alphas[t-1].view(batch_size, 1, 1, 1) * encoding\n",
    "        std = self.sqrt_betas[t-1].view(batch_size, 1, 1, 1)\n",
    "        noise = torch.randn_like(encoding)\n",
    "        # We are assuming std is same for all dimensions\n",
    "        next_encoding = mean + (std * noise)\n",
    "        return (next_encoding)\n",
    "\n",
    "    def q_sample_from_x0(self, original_image, t):\n",
    "        # ENSURE t of shape (bs,) in range[1,2,..T]\n",
    "        batch_size = original_image.shape[0]\n",
    "        mean = self.sqrt_alphas_cumprod[t-1].view(batch_size, 1, 1, 1) * original_image\n",
    "        std = self.sqrt_one_minus_alphas_cumprod[t-1].view(batch_size, 1, 1, 1)\n",
    "        noise = torch.randn_like(original_image)\n",
    "        # We are assuming std is same for all dimensions\n",
    "        encoding_t = mean + (std * noise)\n",
    "        return (encoding_t,noise)\n",
    "\n",
    "    # We will use this to prevent calculating gradients wrt parameters of DDPM_Denoiser\n",
    "    def set_requires_grad(self, requires_grad):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = requires_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eujlJgJDPIMr"
   },
   "outputs": [],
   "source": [
    "# Formula for CNN convolutions\n",
    "# output_size = ({input_size - kernel_size + 2*padding}/stride) + 1\n",
    "# Formula for Transpose Conv\n",
    "# output_size = {(input_size - 1) * stride} - (2*padding) + kernel_size}\n",
    "\n",
    "# Trial-1 [498,035]\n",
    "# We us padding to preserve sizes properly!\n",
    "# A_small = [\n",
    "#     # A[0]: Downsampling blocks\n",
    "#     [\n",
    "#         # Down block 1\n",
    "#         [2, [3, 16, 3, 1, 1], nn.ReLU(), 2], #(128 to 64)\n",
    "#         # Down block 2\n",
    "#         [2, [16, 32, 3, 1, 1], nn.ReLU(), 2], #(64 to 32)\n",
    "#         # Down block 3\n",
    "#         [2, [32, 64, 3, 1, 1], nn.ReLU(), 2], #(32 to 16)\n",
    "#     ],\n",
    "\n",
    "#     # A[1]: Bottleneck layer\n",
    "#     [2, [64, 128, 3, 1, 1], nn.ReLU()], #(16 to 16)\n",
    "\n",
    "#     # A[2]: Upsampling blocks\n",
    "#     [\n",
    "#         # Up block 1\n",
    "#         [2, [128, 64, 3, 1, 1], nn.ReLU(), [128, 64, 2, 2, 0]], #(16 to 32)\n",
    "#         # Up block 2\n",
    "#         [2, [64, 32, 3, 1, 1], nn.ReLU(), [64, 32, 2, 2, 0]], #(32 to 64)\n",
    "#         # Up block 3\n",
    "#         [2, [32, 16, 3, 1, 1], nn.ReLU(), [32, 16, 2, 2, 0]], #(64 to 128)\n",
    "#     ],\n",
    "\n",
    "#     # A[3]: Final 1x1 convolution\n",
    "#     [16, 3, 1, 1, 0]\n",
    "# ]\n",
    "\n",
    "# Trial -2 [7,828,451]\n",
    "# Larger Architecture for the Diffusion Model\n",
    "# Increased capacity with more filters and additional layers\n",
    "# A_small = [\n",
    "#     # A[0]: Downsampling blocks\n",
    "#     [\n",
    "#         # Down block 1\n",
    "#         [2, [3, 32, 3, 1, 1], nn.ReLU(), 2],  #(128 to 64)\n",
    "#         # Down block 2\n",
    "#         [2, [32, 64, 3, 1, 1], nn.ReLU(), 2],  #(64 to 32)\n",
    "#         # Down block 3\n",
    "#         [2, [64, 128, 3, 1, 1], nn.ReLU(), 2],  #(32 to 16)\n",
    "#         # Down block 4 (additional layer)\n",
    "#         [2, [128, 256, 3, 1, 1], nn.ReLU(), 2], #(16 to 8)\n",
    "#     ],\n",
    "\n",
    "#     # A[1]: Bottleneck layer\n",
    "#     [2, [256, 512, 3, 1, 1], nn.ReLU()],  #(8 to 8)\n",
    "\n",
    "#     # A[2]: Upsampling blocks\n",
    "#     [\n",
    "#         # Up block 1\n",
    "#         [2, [512, 256, 3, 1, 1], nn.ReLU(), [512, 256, 2, 2, 0]], #(8 to 16)\n",
    "#         # Up block 2\n",
    "#         [2, [256, 128, 3, 1, 1], nn.ReLU(), [256, 128, 2, 2, 0]], #(16 to 32)\n",
    "#         # Up block 3\n",
    "#         [2, [128, 64, 3, 1, 1], nn.ReLU(), [128, 64, 2, 2, 0]], #(32 to 64)\n",
    "#         # Up block 4 (additional layer)\n",
    "#         [2, [64, 32, 3, 1, 1], nn.ReLU(), [64, 32, 2, 2, 0]], #(64 to 128)\n",
    "#     ],\n",
    "\n",
    "#     # A[3]: Final 1x1 convolution\n",
    "#     [32, 3, 1, 1, 0]\n",
    "# ]\n",
    "\n",
    "# Trial- 3 [31,291,331]\n",
    "A_small = [\n",
    "    # A[0]: Downsampling blocks\n",
    "    [\n",
    "        # Down block 1\n",
    "        [2, [3, 64, 3, 1, 1], nn.ReLU(), 2],  #(128 to 64)\n",
    "        # Down block 2\n",
    "        [2, [64, 128, 3, 1, 1], nn.ReLU(), 2],  #(64 to 32)\n",
    "        # Down block 3\n",
    "        [2, [128, 256, 3, 1, 1], nn.ReLU(), 2],  #(32 to 16)\n",
    "        # Down block 4 (additional layer)\n",
    "        [2, [256, 512, 3, 1, 1], nn.ReLU(), 2], #(16 to 8)\n",
    "    ],\n",
    "\n",
    "    # A[1]: Bottleneck layer\n",
    "    [2, [512, 1024, 3, 1, 1], nn.ReLU()],  #(8 to 8)\n",
    "\n",
    "    # A[2]: Upsampling blocks\n",
    "    [\n",
    "        # Up block 1\n",
    "        [2, [1024, 512, 3, 1, 1], nn.ReLU(), [1024, 512, 2, 2, 0]], #(8 to 16)\n",
    "        # Up block 2\n",
    "        [2, [512, 256, 3, 1, 1], nn.ReLU(), [512, 256, 2, 2, 0]], #(16 to 32)\n",
    "        # Up block 3\n",
    "        [2, [256, 128, 3, 1, 1], nn.ReLU(), [256, 128, 2, 2, 0]], #(32 to 64)\n",
    "        # Up block 4 (additional layer)\n",
    "        [2, [128, 64, 3, 1, 1], nn.ReLU(), [128, 64, 2, 2, 0]], #(64 to 128)\n",
    "    ],\n",
    "\n",
    "    # A[3]: Final 1x1 convolution\n",
    "    [64, 3, 1, 1, 0]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MMMV7hxRPLwL"
   },
   "outputs": [],
   "source": [
    "# Rough checking if code so far behaves as expected\n",
    "DDPM_check = DDPM_Denoiser(1000 , 1e-4 , 0.02 , A_small , 128 , device).to(device)\n",
    "r1 = torch.randn((7,3,128,128)).to(device)\n",
    "# The low value is inclusive, and the high value is exclusive\n",
    "r3 = torch.randint(low=1, high=1001, size=(7,)).to(device)\n",
    "r2 = DDPM_check(r1,r3)\n",
    "print(r2.size())\n",
    "TP1 = sum(p.numel() for p in DDPM_check.parameters())\n",
    "print(TP1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5E0pDiFycrE8"
   },
   "source": [
    "**Interpretation-1 [Predicting Original Image from Denoised version]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jPwe9MJUXxjI"
   },
   "source": [
    "**Observations:**\n",
    "* In the UNet paper they state that it relies on DataAugmentations. This was also observed while training the DDPM where augmentation improved image quality but took more number of epochs\n",
    "* They also suggest to initialize weights such that each feature map has unit variance which is done by default in pytorch using \"Kaiming Initialization\" that is specifically for ReLU activations\n",
    "* It was difficult to make \"higher timesteps denoising\" good as the #timesteps increases to 1000. But it was relatively better at T = 500\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q4Ichf1kR858"
   },
   "outputs": [],
   "source": [
    "# log p(x) = ELBO + {Intractable term} where ELBO = A - B - C\n",
    "# A := Reconstruction Term = E_{q(x1|x0)}[log p_theta(x0|x1)]\n",
    "# This is analogous to ELBO of VAE can be done using MSE-Likelihood + MonteCarlo\n",
    "# B := Prior-Matching Term = D_kl(q(xT|x0) || p(xT))\n",
    "# B has no trainable parameters and equal to zero under our assumptions!\n",
    "# C := Denoising-Matching Term = sum{t=2 to T}[E_{q(xt|x0)}[D_kl(q(x_{t-1}|xt,x0) || p_theta(x_{t-1}|x_t))]]\n",
    "# The bulk of the optimisation lies in C [minimize C,maximize A,maximize ELBO]\n",
    "\n",
    "# The difficulty to optimize C in general case is the reason to use simple Gaussian Transitions to make it analytic!\n",
    "# Use b_t = a_t \"bar\"\n",
    "# q(xt|x0) = N(xt ; root(b_t) ; (1 - b_t)I)\n",
    "# Using above q(x_{t-1}|xt,x0) = Constant * N(x_{t-1} ; mu_q(xt,x0) ; sigma^2_q(t)I)\n",
    "# In above formula we have\n",
    "# 1) mu_q(xt,x0) = [{root(a_t) * (1 - b_{t-1}) * xt} + {root(b_{t-1}) * (1 - a_{t}) * x0}] / [1 - b_t]\n",
    "# 2) sigma^2_q(t) = (1 - a_t) * (1 - b_{t-1}) / [1 - b_t]\n",
    "# Mean is function of (xt,x0) and variance is essentially fixed but depends on timestep\n",
    "# Therefore p_theta(x_{t-1}|x_t) is MODELLED AS A GAUSSIAN WITH SAME COVARINCE\n",
    "# The only thing going to be parameterised using a NN is mu_theta(xt,t)\n",
    "# Note That x0 information is technically conveyed via the variable \"t of timestep\"!\n",
    "\n",
    "# Under above assumptions the D_kl term of C for each timestep becomes the following:\n",
    "# D_kl(.,.) = 1/(2*sigma^2_q(t)) * [mu_theta(xt,t) - mu_q(xt,x0)] ^ 2\n",
    "# Now this can be FURTHER SIMPLIFIED in METHOD 1\n",
    "# To match both the \"mu\" knowing the form of mu_q(xt,x0) the coefficient of xt can be fixed for mu_theta\n",
    "# What we can NOW learn is simply x0 = x_theta(xt,t) assuming mu_theta is \"same form\" as \"mu_q(xt,x0)\"\n",
    "# Now it simplifies to Objective_1 = 1/(2*sigma^2_q(t)) * {b_{t-1} * (1 - a_t)^2/(1 - bt)^2 } * [mu_theta(xt,t) - x0] ^ 2\n",
    "# The A term also combines to give sum{t=1 to T} * 1/(2*sigma^2_q(t)) * (mu_q_0 ^ 2) * [E_{q(xt|x0)}[x_theta(xt) - x0]^2]\n",
    "# Therefore, optimizing a VDM boils down to learning a neural network to predict the original ground truth\n",
    "# image from an arbitrarily noisified version of it with some weighting factor over time-steps\n",
    "\n",
    "# Furthermore, minimizing the summation involved in C; aross all noise levels\n",
    "# can be approximated by minimizing the expectation over all timesteps\n",
    "# Sample a random timestep \"t\" uniformly from [1 to T]\n",
    "# Now fixing this t..get many xt by sampling from q(xt|x0) using encoding\n",
    "# then apply Objective_1 to each and take average to compute inner expectation\n",
    "\n",
    "# The DDPM Image-Prediction Model\n",
    "lr_ddpm , m , time_embedding_dim = 1e-4, 128 , 512\n",
    "T = 1000\n",
    "b_1 = 1e-4\n",
    "b_T = 0.02\n",
    "ddpm = DDPM_Denoiser(T , b_1 , b_T , A_small , time_embedding_dim , device).to(device)\n",
    "ddpm_loss = []\n",
    "ddpm_params = list(ddpm.parameters())\n",
    "ddpm_optimizer = optim.Adam(ddpm_params, lr=lr_ddpm, betas=(0.7, 0.99), eps=1e-8, weight_decay=0)\n",
    "# ddpm_optimizer = optim.RMSprop(ddpm_params, lr=lr_ddpm, alpha=0.99, eps=1e-8, weight_decay=0)\n",
    "# ddpm_optimizer = optim.SGD(ddpm_params, lr=lr_ddpm, momentum=0.9, weight_decay=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jyRsR7YzQdPS"
   },
   "outputs": [],
   "source": [
    "# Manual changing if needed\n",
    "lr_ddpm , m = 5e-5, 64\n",
    "ddpm_optimizer = optim.Adam(ddpm_params, lr=lr_ddpm, betas=(0.5, 0.98), eps=1e-8, weight_decay=0)\n",
    "# ddpm_optimizer = optim.RMSprop(ddpm_params, lr=lr_ddpm, alpha=0.99, eps=1e-8, weight_decay=0)\n",
    "# ddpm_optimizer = optim.SGD(ddpm_params, lr=lr_ddpm, momentum=0.9, weight_decay=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IgQ6sZLlepWT"
   },
   "outputs": [],
   "source": [
    "# DDPM Method 1 training algorithm\n",
    "num_epochs = 10\n",
    "mini_batch_epochs = 200\n",
    "epsilon = 1e-15\n",
    "mse_loss = nn.MSELoss(reduction='none')\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for _ in range(mini_batch_epochs):\n",
    "        ddpm.set_requires_grad(True)\n",
    "        ddpm_optimizer.zero_grad()\n",
    "\n",
    "        # Sample m random images from the training dataset\n",
    "        x0 = sample_train(m).to(device)\n",
    "        # 1) Sampling a random int timestep from 1 to T for each image\n",
    "        t = torch.randint(low=1, high=ddpm.T+1, size=(m,)).to(device)  # Shape: (m,)\n",
    "        # Compute time-dependent weights for each timestep in the batch\n",
    "        # time_weights = torch.tensor([0.5 * (1.0 / (ddpm.sigma_squared_q[j-1] + epsilon)) * (ddpm.mean_q_0[j-1]) ** 2 for j in t] , device=x0.device)\n",
    "        time_weights = torch.tensor([1 for j in t] , device=x0.device)\n",
    "        # 2) Use q_sample_from_x0 to get the perturbed images\n",
    "        perturbed_images = (ddpm.q_sample_from_x0(x0, t))[0].to(device)  # Shape: (m, C, H, W)\n",
    "        # 3) Pass the perturbed images and the timesteps to the DDPM\n",
    "        denoised_images = ddpm(perturbed_images, t)  # Shape: (m, C, H, W)\n",
    "        # 4) Apply the loss function appropriate for DDPM\n",
    "        loss = mse_loss(denoised_images, x0).mean(dim=(1, 2, 3)).view(-1, 1)\n",
    "        time_weights = time_weights.view(x0.shape[0],1)\n",
    "        weighted_loss = torch.mean(loss * time_weights,dim=0)\n",
    "        weighted_loss.backward()\n",
    "        ddpm_optimizer.step()\n",
    "        ddpm_loss.append(weighted_loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3It0ti6Dzdlj"
   },
   "outputs": [],
   "source": [
    "# Plot DDPM loss\n",
    "plt.figure(figsize=(8, 6))\n",
    "ddpm_steps = range(len(ddpm_loss))\n",
    "plt.plot(ddpm_steps, ddpm_loss, label='DDPM_loss', color='green')\n",
    "plt.xlabel('Mini-Batch Epochs')\n",
    "plt.ylabel('Loss Value')\n",
    "plt.title('Denoising Matching Term [PredictingImage]')\n",
    "plt.legend()\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "02hwa9WUUpnO"
   },
   "outputs": [],
   "source": [
    "# Inference\n",
    "N_timesteps = ddpm.T\n",
    "N_images = 100\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Step 1: Initialize x_T with white noise from N(0, I)\n",
    "    # Currently t = T\n",
    "    x_t = torch.randn(N_images, 3, 128, 128).to(device)\n",
    "    # Step 2: Iterate from {t = T to 1} to obtain x0\n",
    "    for t in range(N_timesteps,0,-1):\n",
    "        # Create a tensor for the current timestep\n",
    "        t_tensor = torch.full((N_images,), t, dtype=torch.int64).to(device)\n",
    "        # Step 3: Get the denoised output x_theta(xt)\n",
    "        x_theta_xt = ddpm(x_t, t_tensor)\n",
    "        # Generate noise z from N(0, I)\n",
    "        z = torch.randn_like(x_t)\n",
    "        # Step 4: Update x_t based on the INFERENCE FORMULA\n",
    "        # Note that due to indices starting from zero we need to do a \"t-1\"\n",
    "        x_t_minus_1 = ((ddpm.mean_q_t[t-1] * x_t) + (ddpm.mean_q_0[t-1] * x_theta_xt) + (ddpm.sigma_q[t-1] * z))\n",
    "        # Prepare x_t for the next iteration\n",
    "        # x_t = torch.clamp(x_t_minus_1, -1, 1)\n",
    "        x_t = x_t_minus_1\n",
    "\n",
    "    images = (torch.clamp(x_t,-1,1).detach().cpu().numpy())\n",
    "    # Create a figure for the grid of images\n",
    "    fig, axes = plt.subplots(nrows=10, ncols=10, figsize=(15, 15))\n",
    "    # Loop through the 100 images and display them in the grid\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        img = (images[i].transpose(1, 2, 0) + 1)/2\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SE2GNLyj-YKm"
   },
   "outputs": [],
   "source": [
    "# Visualize how the IMAGES ARE GENERATED\n",
    "N_timesteps = ddpm.T\n",
    "N_images = 8\n",
    "timesteps_to_plot = torch.arange(N_timesteps, 0, -100).tolist()\n",
    "\n",
    "# Initialize a list to store the images at the specified timesteps\n",
    "images_at_timesteps = [[] for _ in range(N_images)]\n",
    "with torch.no_grad():\n",
    "    # Step 1: Initialize x_T with white noise from N(0, I)\n",
    "    x_t = torch.randn(N_images, 3, 128, 128).to(device)\n",
    "    # Step 2: Iterate from {t = T to 1} to obtain x0\n",
    "    for t in range(N_timesteps, 0, -1):\n",
    "        # Create a tensor for the current timestep\n",
    "        t_tensor = torch.full((N_images,), t, dtype=torch.int64).to(device)\n",
    "        # Step 3: Get the denoised output x_theta(xt)\n",
    "        x_theta_xt = ddpm(x_t, t_tensor)\n",
    "        # Generate noise z from N(0, I)\n",
    "        z = torch.randn_like(x_t)\n",
    "        # Step 4: Update x_t based on the INFERENCE FORMULA\n",
    "        x_t_minus_1 = ((ddpm.mean_q_t[t-1] * x_t) +\n",
    "                        (ddpm.mean_q_0[t-1] * x_theta_xt) +\n",
    "                        (ddpm.sigma_q[t-1] * z))\n",
    "        # Store images at the specified timesteps\n",
    "        if t in timesteps_to_plot:\n",
    "            idx = timesteps_to_plot.index(t)\n",
    "            for i in range(N_images):\n",
    "                images_at_timesteps[i].append(x_t[i].clone())  # Store a copy of the image\n",
    "        # Prepare x_t for the next iteration\n",
    "        x_t = x_t_minus_1\n",
    "\n",
    "# Create a figure for the grid of images\n",
    "fig, axes = plt.subplots(nrows=N_images, ncols=len(timesteps_to_plot)+1, figsize=(15, 15))\n",
    "# Loop through the stored images and display them in the grid\n",
    "for i in range(N_images):\n",
    "    # Plot the final image (after all timesteps)\n",
    "    final_image = (torch.clamp(x_t[i], -1, 1).detach().cpu().numpy().transpose(1, 2, 0) + 1) / 2\n",
    "    axes[i, -1].imshow(final_image)\n",
    "    axes[i, -1].axis('off')\n",
    "    axes[i, -1].set_title('Final Image')\n",
    "    for j in range(len(timesteps_to_plot)):\n",
    "        img = (torch.clamp(images_at_timesteps[i][j], -1, 1).detach().cpu().numpy().transpose(1, 2, 0) + 1) / 2\n",
    "        axes[i, j].imshow(img)\n",
    "        axes[i, j].axis('off')\n",
    "# Set titles for each column to represent the timesteps\n",
    "for j, t in enumerate(timesteps_to_plot):\n",
    "    axes[0, j].set_title(f'Timestep {t}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I_co73Bizdw3"
   },
   "outputs": [],
   "source": [
    "# Visualize the DENOISING ON THE TRAINING IMAGES\n",
    "N_timesteps = ddpm.T\n",
    "s = 8\n",
    "old_images = torch.empty(s,3,128,128)\n",
    "new_images = torch.empty(s,9,3,128, 128)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(s):\n",
    "        # Sample a single training image #bs = 10\n",
    "        x0 = sample_train(1)\n",
    "        old_images[i] = x0[0]\n",
    "        x0 = x0.repeat(9, 1, 1, 1).to(device)\n",
    "        # 1) Sampling a int timestep from 100 to 1000 for each image\n",
    "        t = torch.linspace(100, N_timesteps, steps=9).long().to(device)\n",
    "        # 2) Use q_sample_from_x0 to get the perturbed images\n",
    "        perturbed_images = (ddpm.q_sample_from_x0(x0, t))[0].to(device)\n",
    "        # 3) Pass the perturbed images and the timesteps to the DDPM\n",
    "        denoised_images = ddpm(perturbed_images, t)\n",
    "        new_images[i] = denoised_images.detach().cpu()\n",
    "\n",
    "# Create a figure for the s x 10 grid\n",
    "fig, axes = plt.subplots(nrows=s, ncols=10, figsize=(15, s * 2))\n",
    "for i in range(s):\n",
    "    # Plot the i-th old image\n",
    "    old_img = (old_images[i].cpu().numpy().transpose(1, 2, 0) + 1) / 2  # Normalize [-1,1] to [0,1]\n",
    "    axes[i, 0].imshow(old_img)\n",
    "    axes[i, 0].axis('off')\n",
    "    # Plot the 10 new images in new_images[i]\n",
    "    for j in range(9):\n",
    "        new_img = (new_images[i, j].cpu().numpy().transpose(1, 2, 0) + 1) / 2  # Normalize [-1,1] to [0,1]\n",
    "        axes[i, j + 1].imshow(new_img)\n",
    "        axes[i, j + 1].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p4IB4pJrwJmG"
   },
   "outputs": [],
   "source": [
    "# DONT FORGET TO SAVE THE WEIGHTS!\n",
    "torch.save(ddpm,\"/home/sahapthank/models/1e_60.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ibt1J428ErjA"
   },
   "source": [
    "**Interpretation-2 [Predicting the Noise]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EHhltv7yEwlr"
   },
   "outputs": [],
   "source": [
    "# We can use the reparametrisation trick to get the most common implementation\n",
    "# Using the q(xt|x0) transitions , express x0 in terms of xt to get\n",
    "# mu_q(xt,x0) = {1/root(a_t) * x_t} - {(1-a_t)/root[(1-b_t)*a_t] * e0} where e0 is N(0,I)\n",
    "# Therefore simply set the NN to predict e_theta(xt,t) as proxy for e0\n",
    "# Now Objective_2 = 1/(2*sigma^2_q(t)) * {(1 - a_t)^2 * b_t-1/(1 - bt) } * [e_theta(xt,t) - e0] ^ 2\n",
    "\n",
    "# The DDPM Noise-Prediction Model\n",
    "lr_ddpm , m , time_embedding_dim = 1e-4, 64 , 512\n",
    "T = 500\n",
    "b_1 = 1e-4\n",
    "b_T = 0.02\n",
    "ddpm = DDPM_Denoiser(T , b_1 , b_T , A_small , time_embedding_dim , device).to(device)\n",
    "# ddpm = torch.load(\"/home/sahapthank/models/2a_100.pth\")\n",
    "ddpm_loss = []\n",
    "ddpm_params = list(ddpm.parameters())\n",
    "ddpm_optimizer = optim.Adam(ddpm_params, lr=lr_ddpm, betas=(0.7, 0.99), eps=1e-8, weight_decay=0)\n",
    "# ddpm_optimizer = optim.RMSprop(ddpm_params, lr=lr_ddpm, alpha=0.99, eps=1e-8, weight_decay=0)\n",
    "# ddpm_optimizer = optim.SGD(ddpm_params, lr=lr_ddpm, momentum=0.9, weight_decay=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gNtaPMKSytMx"
   },
   "outputs": [],
   "source": [
    "# Manual changing if needed\n",
    "lr_ddpm , m = 2e-4, 64\n",
    "ddpm_optimizer = optim.Adam(ddpm_params, lr=lr_ddpm, betas=(0.7, 0.99), eps=1e-8, weight_decay=0)\n",
    "# ddpm_optimizer = optim.RMSprop(ddpm_params, lr=lr_ddpm, alpha=0.99, eps=1e-8, weight_decay=0)\n",
    "# ddpm_optimizer = optim.SGD(ddpm_params, lr=lr_ddpm, momentum=0.9, weight_decay=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cHuldSVaywvD"
   },
   "outputs": [],
   "source": [
    "# DDPM Method 2 training algorithm\n",
    "num_epochs = 30\n",
    "mini_batch_epochs = 200\n",
    "epsilon = 1e-15\n",
    "mse_loss = nn.MSELoss(reduction='none')\n",
    "\n",
    "# The true noise is the one that got added to x_t while getting it from x0\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for _ in range(mini_batch_epochs):\n",
    "        ddpm.set_requires_grad(True)\n",
    "        ddpm_optimizer.zero_grad()\n",
    "\n",
    "        # Sample m random images from the training dataset\n",
    "        x0 = sample_train(m).to(device)\n",
    "        # 1) Sampling a random int timestep from 1 to T for each image\n",
    "        t = torch.randint(low=1, high=ddpm.T+1, size=(m,)).to(device)  # Shape: (m,)\n",
    "        # Compute time-dependent weights for each timestep in the batch\n",
    "        # time_weights = torch.tensor([0.5 * (1.0 / (ddpm.sigma_squared_q[j-1] + epsilon)) * (ddpm.mean_q_0[j-1]) ** 2 for j in t] , device=x0.device)\n",
    "        time_weights = torch.tensor([1 for j in t] , device=x0.device)\n",
    "        # 2) Use q_sample_from_x0 to get the perturbed images\n",
    "        S = ddpm.q_sample_from_x0(x0, t)\n",
    "        perturbed_images = (S[0]).to(device)  # Shape: (m, C, H, W)\n",
    "        true_noise = (S[1]).to(device)\n",
    "        # 3) Pass the perturbed images and the timesteps to the DDPM\n",
    "        predicted_noise = ddpm(perturbed_images, t)  # Shape: (m, C, H, W)\n",
    "        # 4) Apply the loss function appropriate for DDPM\n",
    "        loss = mse_loss(predicted_noise, true_noise).mean(dim=(1, 2, 3)).view(-1, 1)\n",
    "        time_weights = time_weights.view(x0.shape[0],1)\n",
    "        weighted_loss = torch.mean(loss * time_weights,dim=0)*3*128*128\n",
    "        weighted_loss.backward()\n",
    "        ddpm_optimizer.step()\n",
    "        ddpm_loss.append(weighted_loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_fp0NnrUyzP8"
   },
   "outputs": [],
   "source": [
    "# Plot DDPM loss\n",
    "plt.figure(figsize=(8, 6))\n",
    "ddpm_steps = range(len(ddpm_loss))\n",
    "plt.plot(ddpm_steps, ddpm_loss, label='DDPM_loss', color='green')\n",
    "plt.xlabel('Mini-Batch Epochs')\n",
    "plt.ylabel('Loss Value')\n",
    "plt.title('Denoising Matching Term [PredictingErrors]')\n",
    "plt.legend()\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WMmFqZPxyesN"
   },
   "source": [
    "![200.1.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8weYZKsxy2EM"
   },
   "outputs": [],
   "source": [
    "# Inference\n",
    "N_timesteps = ddpm.T\n",
    "N_images = 100\n",
    "with torch.no_grad():\n",
    "    # Step 1: Initialize x_T with white noise from N(0, I) ; Currently t = T\n",
    "    x_t = torch.randn(N_images, 3, 128, 128).to(device)\n",
    "    # Step 2: Iterate from {t = T to 1} to obtain x0\n",
    "    for t in range(N_timesteps,0,-1):\n",
    "        # Create a tensor for the current timestep\n",
    "        t_tensor = torch.full((N_images,), t, dtype=torch.int64).to(device)\n",
    "        # Step 3: Get the predicted_noise : e_theta_(xt)\n",
    "        e_theta_xt = ddpm(x_t, t_tensor)\n",
    "        # Generate noise z from N(0, I)\n",
    "        z = torch.randn_like(x_t)\n",
    "        # Step 4: Update x_t based on the INFERENCE FORMULA\n",
    "        # Note that due to indices starting from zero we need to do a \"t-1\"\n",
    "        x_t_minus_1 = (((1/ddpm.sqrt_alphas[t-1]) * ((x_t) - (ddpm.e_q[t-1] * e_theta_xt))) + (ddpm.sigma_q[t-1] * z))\n",
    "        # Prepare x_t for the next iteration\n",
    "        # x_t = torch.clamp(x_t_minus_1, -1, 1)\n",
    "        x_t = x_t_minus_1\n",
    "\n",
    "    images = (torch.clamp(x_t,-1,1).detach().cpu().numpy())\n",
    "    # Create a figure for the grid of images\n",
    "    fig, axes = plt.subplots(nrows=10, ncols=10, figsize=(15, 15))\n",
    "    # Loop through the 100 images and display them in the grid\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        img = (images[i].transpose(1, 2, 0) + 1)/2\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nrWj2O5AQQmu"
   },
   "outputs": [],
   "source": [
    "# Visualize how the IMAGES ARE GENERATED\n",
    "N_timesteps = ddpm.T\n",
    "N_images = 8\n",
    "timesteps_to_plot = torch.arange(N_timesteps, 0, -50).tolist()\n",
    "\n",
    "# Initialize a list to store the images at the specified timesteps\n",
    "images_at_timesteps = [[] for _ in range(N_images)]\n",
    "with torch.no_grad():\n",
    "    # Step 1: Initialize x_T with white noise from N(0, I)\n",
    "    x_t = torch.randn(N_images, 3, 128, 128).to(device)\n",
    "    # Step 2: Iterate from {t = T to 1} to obtain x0\n",
    "    for t in range(N_timesteps, 0, -1):\n",
    "        # Create a tensor for the current timestep\n",
    "        t_tensor = torch.full((N_images,), t, dtype=torch.int64).to(device)\n",
    "        # Step 3: Get the denoised output x_theta(xt)\n",
    "        e_theta_xt = ddpm(x_t, t_tensor)\n",
    "        # Generate noise z from N(0, I)\n",
    "        z = torch.randn_like(x_t)\n",
    "        # Step 4: Update x_t based on the INFERENCE FORMULA\n",
    "        x_t_minus_1 = (((1/ddpm.sqrt_alphas[t-1]) * ((x_t) - (ddpm.e_q[t-1] * e_theta_xt))) + (ddpm.sigma_q[t-1] * z))\n",
    "        # Store images at the specified timesteps\n",
    "        if t in timesteps_to_plot:\n",
    "            idx = timesteps_to_plot.index(t)\n",
    "            for i in range(N_images):\n",
    "                images_at_timesteps[i].append(x_t[i].clone())  # Store a copy of the image\n",
    "        # Prepare x_t for the next iteration\n",
    "        x_t = x_t_minus_1\n",
    "\n",
    "# Create a figure for the grid of images\n",
    "fig, axes = plt.subplots(nrows=N_images, ncols=len(timesteps_to_plot)+1, figsize=(15, 15))\n",
    "# Loop through the stored images and display them in the grid\n",
    "for i in range(N_images):\n",
    "    # Plot the final image (after all timesteps)\n",
    "    final_image = (torch.clamp(x_t[i], -1, 1).detach().cpu().numpy().transpose(1, 2, 0) + 1) / 2\n",
    "    axes[i, -1].imshow(final_image)\n",
    "    axes[i, -1].axis('off')\n",
    "    axes[i, -1].set_title('Final Image')\n",
    "    for j in range(len(timesteps_to_plot)):\n",
    "        img = (torch.clamp(images_at_timesteps[i][j], -1, 1).detach().cpu().numpy().transpose(1, 2, 0) + 1) / 2\n",
    "        axes[i, j].imshow(img)\n",
    "        axes[i, j].axis('off')\n",
    "# Set titles for each column to represent the timesteps\n",
    "for j, t in enumerate(timesteps_to_plot):\n",
    "    axes[0, j].set_title(f'Timestep {t}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e8Vm71Zb7X1Z"
   },
   "outputs": [],
   "source": [
    "# DONT FORGET TO SAVE THE WEIGHTS!\n",
    "torch.save(ddpm,\"/home/sahapthank/models/2b_500.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UycpH7CszBZL"
   },
   "source": [
    "**[Q4] Stable Diffusion on VQVAE LatentSpace**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5gCyektf3aZs"
   },
   "outputs": [],
   "source": [
    "# VQ-VAE is the SoTa lets see if we can do it properly!\n",
    "# Learning useful representations without supervision remains a key challenge in ML\n",
    "# (VQ-VAE) differs from VAEs in two key ways:\n",
    "# 1) encoder network outputs discrete, rather than continuous, codes; and the prior\n",
    "# 2) is learnt rather than static.\n",
    "# Using the VQ method allows the\n",
    "# model to circumvent issues of posterior collapse  where the latents are ignored\n",
    "# when they are paired with a powerful autoregressive decoder  typically observed in the VAE framework.\n",
    "\n",
    "# For speech discrete encodings k are 1D and each z_q corresponds to e_q in R^d\n",
    "# For images (d1,d2) 2D encoding where each (vector along pixel) gets mapped to an embedding vector\n",
    "# There exist K embedding vectors!\n",
    "# Basically a CNN is used to convert to z_e(x) = (D,d1,d2) and each {z_e(x)}i,j as a D-dim vectors\n",
    "# We use nearest neighbours and map it to e_i in R^D!\n",
    "# We now get z_q(x) = (D,d1,d2) which using a Transpose_CNN we get final image\n",
    "\n",
    "# Note that there is no real gradient defined for discretisation using nearest neighbour\n",
    "# however we approximate the gradient similar to the straight-through estimator\n",
    "# and just copy gradients from decoder input z_q(x) to encoder output z_e(x)\n",
    "# Since the output representation of the encoder and the input to the decoder share the same D dimensional space,\n",
    "# the gradients contain useful information for how the encoder has to change its output\n",
    "\n",
    "class VQ_VAE_Encoder_0(nn.Module):\n",
    "    def __init__(self, architecture):\n",
    "        # Has only Convolutions, architecture = [cnn_architecture]\n",
    "        # cnn_architecture = [conv_params, cnn_batchnorm, cnn_activation_fn]\n",
    "        # conv_params = [[in_channels, output_channels, kernel_size, stride, padding], ...]\n",
    "        # cnn_batchnorm = [False, True, ...]\n",
    "        # cnn_activation = [nn.ReLU(), nn.LeakyReLU(), ...]\n",
    "        super(VQ_VAE_Encoder_0, self).__init__()\n",
    "        assert len(architecture[0]) == len(architecture[1]) == len(architecture[2])\n",
    "        self.conv_params = architecture[0]\n",
    "        self.cnn_batchnorm = architecture[1]\n",
    "        self.cnn_activation = architecture[2]\n",
    "\n",
    "        layers_0 = []\n",
    "        for j,i in enumerate(self.conv_params):\n",
    "            (layers_0).append(nn.Conv2d(in_channels=i[0],out_channels=i[1],kernel_size=i[2],stride=i[3],padding=i[4]))\n",
    "            if (self.cnn_batchnorm)[j]:\n",
    "                (layers_0).append(nn.BatchNorm2d(i[1]))\n",
    "            (layers_0).append(self.cnn_activation[j])\n",
    "        # Stack convolutional layers in a Sequential block\n",
    "        self.cnn = nn.Sequential(*(layers_0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        z_e = self.cnn(x)\n",
    "        return z_e\n",
    "    # We will use this to prevent calculating gradients wrt parameters of Encoder\n",
    "    def set_requires_grad(self, requires_grad):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = requires_grad\n",
    "\n",
    "class VQ_VAE_Decoder_0(nn.Module):\n",
    "    def __init__(self, architecture):\n",
    "        # The architecture is almost same as DCGAN using transpose_convolutions only\n",
    "        super(VQ_VAE_Decoder_0, self).__init__()\n",
    "        assert len(architecture[0]) == len(architecture[1]) == len(architecture[2])\n",
    "        self.transpose_conv_params = architecture[0]\n",
    "        self.use_batchnorm = architecture[1]\n",
    "        self.activation_fn = architecture[2]\n",
    "\n",
    "        layers = []\n",
    "        # Starting with input latent vector of 2D image!\n",
    "        for j, i in enumerate(self.transpose_conv_params):\n",
    "            layers.append(nn.ConvTranspose2d(in_channels=i[0], out_channels=i[1], kernel_size=i[2], stride=i[3], padding=i[4]))\n",
    "            if self.use_batchnorm[j]:\n",
    "                layers.append(nn.BatchNorm2d(i[1]))\n",
    "            layers.append(self.activation_fn[j])\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    # Sampling images from Decoder\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    # We will use this to prevent calculating gradients wrt parameters of Decoder\n",
    "    def set_requires_grad(self, requires_grad):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = requires_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YTZW0FBx3bFE"
   },
   "outputs": [],
   "source": [
    "# Since we assume a uniform prior for z, the KL term that usually appears in the ELBO is constant\n",
    "# w.r.t. the encoder parameters and can thus be ignored for training\n",
    "# Note that in VAE KL is only used for encoder training!\n",
    "# VQVAE has 3 loss terms for the reconstruction\n",
    "# A = logp(x|z_q(x)) := reconstruction {Encoder and Decoder}\n",
    "# B = l2 ((sg(z_e(x)),e)) := VQ Objective {Embedding Vectors only}\n",
    "# C =  beta * l2 ((z_e(x),sg(e))) := CommitmentLoss {Encoder only}\n",
    "\n",
    "# When we use N = k1*k2 dicrete latents (In paper they use (32,32) for ImageNet)\n",
    "# The loss terms are averaged for each latent (1/N factor)\n",
    "# One term for each latent arises in {B,C}\n",
    "# Whilst training the VQ-VAE, the prior is kept constant and uniform.\n",
    "# Each region of the image (represented by a latent code) has an equal probability\n",
    "# of being encoded by any of the embedding vectors from the codebook.\n",
    "\n",
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_loss_beta):\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "        # Codebook containing K embedding vectors\n",
    "        # Each vector is of dim D\n",
    "        # D would be the final #output channels in CNN\n",
    "        # Underscore is used for inplace operation\n",
    "        # Initialize with unform weights from [-1/K,1/K]\n",
    "        self.K = num_embeddings\n",
    "        self.D = embedding_dim\n",
    "        self.embedding = nn.Embedding(self.K, self.D) #(K,D)\n",
    "        self.embedding.weight.data.uniform_(-1/self.K, 1/self.K)\n",
    "        self.beta = commitment_loss_beta\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # convert inputs z_e(x) from (bs,c,h,w) to (bs,h,w,c)\n",
    "        inputs = inputs.permute(0, 2, 3, 1).contiguous()\n",
    "        input_shape = inputs.shape\n",
    "        # Flatten input to (bs*h*w,c=D)\n",
    "        flat_input = inputs.view(-1, self.D)\n",
    "        # Calculate distances of {each pixel_vector with each embedding vector}\n",
    "        # Final shape is (bs*h*w = N,K)\n",
    "        distances = (torch.sum(flat_input**2, dim=1, keepdim=True)\n",
    "                    + (torch.sum((self.embedding.weight)**2, dim=1))\n",
    "                    - (2 * torch.matmul(flat_input, self.embedding.weight.t())))\n",
    "\n",
    "        # Encoding\n",
    "        # Using one-hot encodings allows gradients to flow correctly during backpropagation.\n",
    "        # If we directly assign embeddings based on indices\n",
    "        # the computation graph might not accurately reflect the necessary operations for gradient calculation\n",
    "        # especially when dealing with operations like straight-through estimators\n",
    "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1) #(N,1)\n",
    "        encodings = torch.zeros(encoding_indices.shape[0], self.K, device=inputs.device)\n",
    "        # Keep shape same but put ones in the appropriate positions and 0-elsewhere\n",
    "        encodings.scatter_(1,encoding_indices,1) #(N,K)\n",
    "        # Quantize to (N,D) and unflatten to (bs,h,w,c)\n",
    "        quantized = torch.matmul(encodings, self.embedding.weight).view(input_shape)\n",
    "        # Calculate the Loss associated\n",
    "        # Averaging over {batch and EmbeddingVectors is done by default}\n",
    "        # MSE btw {z_e(x),sg(e)}\n",
    "        commitment_loss = F.mse_loss(quantized.detach(),inputs)\n",
    "        # MSE btw {sg(z_e(x)),e}\n",
    "        vq_objective = F.mse_loss(quantized,inputs.detach())\n",
    "        loss_B_C = (vq_objective) + (self.beta * commitment_loss)\n",
    "        # Trick for straight-through estimation\n",
    "        quantized = inputs + (quantized - inputs).detach()\n",
    "        # convert quantized back to (bs,c,h,w) and return (Loss,z_q(x)),B_loss,C_loss)\n",
    "        return (loss_B_C, quantized.permute(0, 3, 1, 2).contiguous(),commitment_loss.detach(),vq_objective.detach())\n",
    "\n",
    "    # We will use this to prevent calculating gradients wrt parameters of VQ\n",
    "    def set_requires_grad(self, requires_grad):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = requires_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FbwlBUQQzG-N"
   },
   "outputs": [],
   "source": [
    "# Use the class definitions of VQVAE before loading\n",
    "E = torch.load(\"models/vqvae/E_2b_700.pth\").to(device)\n",
    "D = torch.load(\"models/vqvae/D_2b_700.pth\").to(device)\n",
    "vq = torch.load(\"models/vqvae/vq_2b_700.pth\").to(device)\n",
    "E.set_requires_grad(False)\n",
    "D.set_requires_grad(False)\n",
    "vq.set_requires_grad(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KTiOHGFj2v0a"
   },
   "outputs": [],
   "source": [
    "# Get all the latent vectors\n",
    "# Note that VQVAE Encoder was trained using images in [0,1] space\n",
    "all_latent_vectors = torch.zeros(6499*2,64,16,16)\n",
    "batch_size = 64\n",
    "for i in range(0, 6499*2, batch_size):\n",
    "    with torch.no_grad():\n",
    "        x = butterfly_images_augmented_2[i:min(i+batch_size,6499*2)].to(device)\n",
    "        z_e = E(x)\n",
    "        z_q = vq(z_e)[1]\n",
    "        # Calculate the end index for slicing\n",
    "        end_index = min(i + batch_size, 6499*2)\n",
    "        # Insert the batch of latents into the correct slice of T\n",
    "        all_latent_vectors[i:end_index] = z_q\n",
    "all_latent_vectors = all_latent_vectors.view(-1,64,16,16)\n",
    "print(all_latent_vectors.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kuJVLHe_BbCu"
   },
   "outputs": [],
   "source": [
    "# Trial- 3 [31,291,331]\n",
    "A_small = [\n",
    "    # A[0]: Downsampling blocks\n",
    "    [\n",
    "        [3, [64, 128, 3, 1, 1], nn.ReLU(), 2],  #(16 to 8)\n",
    "        [2, [128, 256, 3, 1, 1], nn.ReLU(), 2],  #(8 to 4)\n",
    "    ],\n",
    "\n",
    "    # A[1]: Bottleneck layer\n",
    "    [3, [256, 512, 3, 1, 1], nn.ReLU()],  #(4 to 4)\n",
    "\n",
    "    # A[2]: Upsampling blocks\n",
    "    [\n",
    "        [2, [512,256, 3, 1, 1], nn.ReLU(), [512,256, 2, 2, 0]], #(4 to 8)\n",
    "        [3, [256,128, 3, 1, 1], nn.ReLU(), [256,128, 2, 2, 0]], #(8 to 16)\n",
    "    ],\n",
    "\n",
    "    # A[3]: Final 1x1 convolution\n",
    "    [128, 64, 1, 1, 0]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s4Ei-3b1DVAd"
   },
   "outputs": [],
   "source": [
    "# We can use the reparametrisation trick to get the most common implementation\n",
    "# Using the q(xt|x0) transitions , express x0 in terms of xt to get\n",
    "# mu_q(xt,x0) = {1/root(a_t) * x_t} - {(1-a_t)/root[(1-b_t)*a_t] * e0} where e0 is N(0,I)\n",
    "# Therefore simply set the NN to predict e_theta(xt,t) as proxy for e0\n",
    "# Now Objective_2 = 1/(2*sigma^2_q(t)) * {(1 - a_t)^2 * b_t-1/(1 - bt) } * [e_theta(xt,t) - e0] ^ 2\n",
    "\n",
    "# The DDPM Noise-Prediction Model\n",
    "lr_ddpm , m , time_embedding_dim = 1e-4, 64 , 512\n",
    "T = 500\n",
    "b_1 = 1e-4\n",
    "b_T = 0.02\n",
    "ddpm = DDPM_Denoiser(T , b_1 , b_T , A_small , time_embedding_dim , device).to(device)\n",
    "# ddpm = torch.load(\"/home/sahapthank/models/2a_100.pth\")\n",
    "ddpm_loss = []\n",
    "ddpm_params = list(ddpm.parameters())\n",
    "ddpm_optimizer = optim.Adam(ddpm_params, lr=lr_ddpm, betas=(0.7, 0.99), eps=1e-8, weight_decay=0)\n",
    "# ddpm_optimizer = optim.RMSprop(ddpm_params, lr=lr_ddpm, alpha=0.99, eps=1e-8, weight_decay=0)\n",
    "# ddpm_optimizer = optim.SGD(ddpm_params, lr=lr_ddpm, momentum=0.9, weight_decay=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8LB_TBegDV24"
   },
   "outputs": [],
   "source": [
    "# Manual changing if needed\n",
    "lr_ddpm , m = 5e-5, 64\n",
    "ddpm_optimizer = optim.Adam(ddpm_params, lr=lr_ddpm, betas=(0.7, 0.99), eps=1e-8, weight_decay=0)\n",
    "# ddpm_optimizer = optim.RMSprop(ddpm_params, lr=lr_ddpm, alpha=0.99, eps=1e-8, weight_decay=0)\n",
    "# ddpm_optimizer = optim.SGD(ddpm_params, lr=lr_ddpm, momentum=0.9, weight_decay=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JkKva6oDDIzC"
   },
   "outputs": [],
   "source": [
    "training_images = all_latent_vectors\n",
    "# VQVAE was trained using [0,1] images as i/p and o/p\n",
    "# Encoded vectors actually fit in [-1,1]\n",
    "# So UNET ddpm has to be final layer nn.Tanh()\n",
    "# This is a change from previous training\n",
    "\n",
    "# DDPM Method 2 training algorithm\n",
    "num_epochs = 20\n",
    "mini_batch_epochs = 200\n",
    "epsilon = 1e-15\n",
    "mse_loss = nn.MSELoss(reduction='none')\n",
    "\n",
    "# The true noise is the one that got added to x_t while getting it from x0\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for _ in range(mini_batch_epochs):\n",
    "        ddpm.set_requires_grad(True)\n",
    "        ddpm_optimizer.zero_grad()\n",
    "\n",
    "        # Sample m random images from the training dataset\n",
    "        x0 = sample_train(m).to(device)\n",
    "        # 1) Sampling a random int timestep from 1 to T for each image\n",
    "        t = torch.randint(low=1, high=ddpm.T+1, size=(m,)).to(device)  # Shape: (m,)\n",
    "        # Compute time-dependent weights for each timestep in the batch\n",
    "        # time_weights = torch.tensor([0.5 * (1.0 / (ddpm.sigma_squared_q[j-1] + epsilon)) * (ddpm.mean_q_0[j-1]) ** 2 for j in t] , device=x0.device)\n",
    "        time_weights = torch.tensor([1 for j in t] , device=x0.device)\n",
    "        # 2) Use q_sample_from_x0 to get the perturbed images\n",
    "        S = ddpm.q_sample_from_x0(x0, t)\n",
    "        perturbed_images = (S[0]).to(device)  # Shape: (m, C, H, W)\n",
    "        true_noise = (S[1]).to(device)\n",
    "        # 3) Pass the perturbed images and the timesteps to the DDPM\n",
    "        predicted_noise = ddpm(perturbed_images, t)  # Shape: (m, C, H, W)\n",
    "        # 4) Apply the loss function appropriate for DDPM\n",
    "        loss = mse_loss(predicted_noise, true_noise).mean(dim=(1, 2, 3)).view(-1, 1)\n",
    "        time_weights = time_weights.view(x0.shape[0],1)\n",
    "        weighted_loss = torch.mean(loss * time_weights,dim=0)*3*128*128\n",
    "        weighted_loss.backward()\n",
    "        ddpm_optimizer.step()\n",
    "        ddpm_loss.append(weighted_loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T1MT6GY2ECrx"
   },
   "outputs": [],
   "source": [
    "# Inference\n",
    "N_timesteps = ddpm.T\n",
    "N_images = 100\n",
    "with torch.no_grad():\n",
    "    # Step 1: Initialize x_T with white noise from N(0, I) ; Currently t = T\n",
    "    x_t = torch.randn(N_images, 64, 16, 16).to(device)\n",
    "    # Step 2: Iterate from {t = T to 1} to obtain x0\n",
    "    for t in range(N_timesteps,0,-1):\n",
    "        # Create a tensor for the current timestep\n",
    "        t_tensor = torch.full((N_images,), t, dtype=torch.int64).to(device)\n",
    "        # Step 3: Get the predicted_noise : e_theta_(xt)\n",
    "        e_theta_xt = ddpm(x_t, t_tensor)\n",
    "        # Generate noise z from N(0, I)\n",
    "        z = torch.randn_like(x_t)\n",
    "        # Step 4: Update x_t based on the INFERENCE FORMULA\n",
    "        # Note that due to indices starting from zero we need to do a \"t-1\"\n",
    "        x_t_minus_1 = (((1/ddpm.sqrt_alphas[t-1]) * ((x_t) - (ddpm.e_q[t-1] * e_theta_xt))) + (ddpm.sigma_q[t-1] * z))\n",
    "        # Prepare x_t for the next iteration\n",
    "        # x_t = torch.clamp(x_t_minus_1, -1, 1)\n",
    "        x_t = x_t_minus_1\n",
    "\n",
    "    x_t = D(x_t)\n",
    "    images = (torch.clamp(x_t,0,1).detach().cpu().numpy())\n",
    "    # Create a figure for the grid of images\n",
    "    fig, axes = plt.subplots(nrows=10, ncols=10, figsize=(15, 15))\n",
    "    # Loop through the 100 images and display them in the grid\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        img = (images[i].transpose(1, 2, 0))\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sJIt0OfbHpeF"
   },
   "outputs": [],
   "source": [
    "# Visualize how the IMAGES ARE GENERATED\n",
    "N_timesteps = ddpm.T\n",
    "N_images = 8\n",
    "timesteps_to_plot = torch.arange(N_timesteps, 0, -50).tolist()\n",
    "\n",
    "# Initialize a list to store the images at the specified timesteps\n",
    "images_at_timesteps = [[] for _ in range(N_images)]\n",
    "with torch.no_grad():\n",
    "    # Step 1: Initialize x_T with white noise from N(0, I)\n",
    "    x_t = torch.randn(N_images, 64, 16, 16).to(device)\n",
    "    # Step 2: Iterate from {t = T to 1} to obtain x0\n",
    "    for t in range(N_timesteps, 0, -1):\n",
    "        # Create a tensor for the current timestep\n",
    "        t_tensor = torch.full((N_images,), t, dtype=torch.int64).to(device)\n",
    "        # Step 3: Get the denoised output x_theta(xt)\n",
    "        e_theta_xt = ddpm(x_t, t_tensor)\n",
    "        # Generate noise z from N(0, I)\n",
    "        z = torch.randn_like(x_t)\n",
    "        # Step 4: Update x_t based on the INFERENCE FORMULA\n",
    "        x_t_minus_1 = (((1/ddpm.sqrt_alphas[t-1]) * ((x_t) - (ddpm.e_q[t-1] * e_theta_xt))) + (ddpm.sigma_q[t-1] * z))\n",
    "        # Store images at the specified timesteps\n",
    "        if t in timesteps_to_plot:\n",
    "            y_t = D(x_t)\n",
    "            idx = timesteps_to_plot.index(t)\n",
    "            for i in range(N_images):\n",
    "                images_at_timesteps[i].append(y_t[i].clone())  # Store a copy of the image\n",
    "        # Prepare x_t for the next iteration\n",
    "        x_t = x_t_minus_1\n",
    "\n",
    "# Create a figure for the grid of images\n",
    "x_t = D(x_t)\n",
    "fig, axes = plt.subplots(nrows=N_images, ncols=len(timesteps_to_plot)+1, figsize=(15, 15))\n",
    "# Loop through the stored images and display them in the grid\n",
    "for i in range(N_images):\n",
    "    # Plot the final image (after all timesteps)\n",
    "    final_image = (torch.clamp(x_t[i], 0, 1).detach().cpu().numpy().transpose(1, 2, 0))\n",
    "    axes[i, -1].imshow(final_image)\n",
    "    axes[i, -1].axis('off')\n",
    "    axes[i, -1].set_title('Final Image')\n",
    "    for j in range(len(timesteps_to_plot)):\n",
    "        img = (torch.clamp(images_at_timesteps[i][j], 0, 1).detach().cpu().numpy().transpose(1, 2, 0))\n",
    "        axes[i, j].imshow(img)\n",
    "        axes[i, j].axis('off')\n",
    "# Set titles for each column to represent the timesteps\n",
    "for j, t in enumerate(timesteps_to_plot):\n",
    "    axes[0, j].set_title(f'Timestep {t}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQiKsT1FzZnk"
   },
   "source": [
    "**[Q10] Knowledge Distillation on ResNet50**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XFvkPdnjA0wT"
   },
   "source": [
    "* Animal Datset [train in [0,1] range] used for training ResNet50 is used to train a smaller CNN/MLP using knowledge distillation\n",
    "* The loss function has 2 components with higher priority to \"distillation loss\" where small model tries to match the \"logits/softmax\" of the ResNet [temp of softmax is varied to find suitable soft targets] . MSE loss is used here.\n",
    "* The other component , \"supervised-loss\" is given low weightage to predict the correct class labels for training dataset. These act as \"hard targets\". The same high temperature is\n",
    "used when training the distilled model, but after it has been trained it uses a temperature of 1 for inference.\n",
    "* Typically, the small model cannot exactly match the soft targets and erring in the direction of the\n",
    "correct answer turns out to be helpful.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "brMioHBfpQh_"
   },
   "outputs": [],
   "source": [
    "# animal images is [0,1] (90,60,3,128,128)\n",
    "# labels is (5400,3,128,128) [0,0,..60 times,1,1,,,60 times,...89,...60 times]\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "data = animal_images.reshape(-1,3,128,128)\n",
    "data = F.interpolate(data, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "# Set up labels and transformation pipeline\n",
    "Labels = torch.tensor([i for i in range(90) for _ in range(60)])\n",
    "# Normalize for ResNet-50\n",
    "transform = transforms.Compose([transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "data = torch.stack([transform(image) for image in data])\n",
    "dataset = TensorDataset(data, Labels)\n",
    "\n",
    "# For Reproducability :)\n",
    "torch.manual_seed(42)\n",
    "train_size = int(0.7 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jvQZPoY2zdFR"
   },
   "outputs": [],
   "source": [
    "# Distilled Model / Student Model\n",
    "class DistilledModel(nn.Module):\n",
    "    def __init__(self, architecture):\n",
    "        # architecture = [cnn_architecture, nn_architecute]\n",
    "        # nn_architecture = [nn_layers, nn_batch_norm , nn_activation]\n",
    "        # cnn_architecture = [conv_params, cnn_batchnorm, cnn_activation_fn]\n",
    "        # conv_params = [[in_channels, output_channels, kernel_size, stride, padding], ...]\n",
    "        # nn/cnn_batchnorm = [False, True, ...]\n",
    "        # nn/cnn_activation = [nn.ReLU(), nn.LeakyReLU(), ...]\n",
    "        super(DistilledModel, self).__init__()\n",
    "        assert len(architecture[0][0]) == len(architecture[0][1]) == len(architecture[0][2])\n",
    "        assert len(architecture[1][0]) == len(architecture[1][1]) == len(architecture[1][2])\n",
    "        self.conv_params = architecture[0][0]\n",
    "        self.cnn_batchnorm = architecture[0][1]\n",
    "        self.cnn_activation = architecture[0][2]\n",
    "        self.nn_layers = architecture[1][0]\n",
    "        self.nn_batchnorm = architecture[1][1]\n",
    "        self.nn_activation = architecture[1][2]\n",
    "        layers_0 = []\n",
    "        for j,i in enumerate(self.conv_params):\n",
    "            (layers_0).append(nn.Conv2d(in_channels=i[0],out_channels=i[1],kernel_size=i[2],stride=i[3],padding=i[4]))\n",
    "            if (self.cnn_batchnorm)[j]:\n",
    "                (layers_0).append(nn.BatchNorm2d(i[1]))\n",
    "            (layers_0).append(self.cnn_activation[j])\n",
    "        layers_1 = []\n",
    "        for i in range(len(self.nn_layers)):\n",
    "            in_dim, out_dim = self.nn_layers[i]\n",
    "            (layers_1).append(nn.Linear(in_dim, out_dim))\n",
    "            if self.nn_batchnorm[i]:\n",
    "                (layers_1).append(nn.BatchNorm1d(out_dim))\n",
    "            (layers_1).append(self.nn_activation[i])\n",
    "        # Stack convolutional layers in a Sequential block\n",
    "        # Create 1 FFNN\n",
    "        self.cnn = nn.Sequential(*(layers_0))\n",
    "        self.nn = nn.Sequential(*(layers_1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.cnn(x)\n",
    "        features = torch.flatten(features, start_dim=1)\n",
    "        logits = self.nn(features)\n",
    "        return logits\n",
    "    # We will use this to prevent calculating gradients wrt parameters of DistilledModel\n",
    "    def set_requires_grad(self, requires_grad):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = requires_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fo12yWYRHTMM"
   },
   "outputs": [],
   "source": [
    "# Architecture for the distilled model\n",
    "# Formula for CNN convolutions\n",
    "# output_size = ({input_size - kernel_size + 2*padding}/stride) + 1\n",
    "\n",
    "# TRIAL 1 [1M parameters]\n",
    "conv_params_3 = [[3,16,4,2,1],[16,16,4,2,1],[16,32,4,2,1],[32,32,4,2,1]]\n",
    "# CONVERT from [3,128,128] = 49,152 to [32,8,8] = 2048\n",
    "cnn_batchnorm_3 = [True for _ in range(4)]\n",
    "cnn_activation_3 = [nn.LeakyReLU(0.2, inplace=False) for _ in range(4)]\n",
    "classifier_cnn = [conv_params_3, cnn_batchnorm_3, cnn_activation_3]\n",
    "nn_layers_3 = [[2048,512],[512,128],[128,90]]\n",
    "nn_batchnorm_3 = [True for _ in range(2)] + [False]\n",
    "nn_activation_3 = [nn.ReLU() for _ in range(2)] + [nn.Identity()]\n",
    "# We will use soft-max while doing the CE loss\n",
    "classifier_nn = [nn_layers_3, nn_batchnorm_3, nn_activation_3]\n",
    "classifier_arch = [classifier_cnn , classifier_nn]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UMcwxeIFIRbL"
   },
   "outputs": [],
   "source": [
    "Classifier_check = DistilledModel(classifier_arch)\n",
    "r1 = torch.randn((7,3,128,128))\n",
    "r2 = Classifier_check(r1)\n",
    "print(r2.size())\n",
    "TP1 = sum(p.numel() for p in Classifier_check.parameters())\n",
    "print(TP1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ctus_v4IIYfg"
   },
   "outputs": [],
   "source": [
    "# Initialize model, loss function, and optimizer\n",
    "lr_d =1e-4\n",
    "D = DistilledModel(classifier_arch).to(device)\n",
    "# temp for softmax is a hyperparameter\n",
    "distillation_temp = 25\n",
    "R = torch.load(\"/home/sahapthank/models/resnet50_1.pth\").to(device)\n",
    "for param in R.parameters():\n",
    "    param.requires_grad = False\n",
    "Distillation_Loss = []\n",
    "Supervised_Loss = []\n",
    "Total_Loss = []\n",
    "D_params = list(D.parameters())\n",
    "D_optimizer = optim.Adam(D_params, lr=lr_d, betas=(0.8, 0.98), eps=1e-8, weight_decay=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O768O6MqE9z7"
   },
   "outputs": [],
   "source": [
    "# Manual adjusting\n",
    "lr_d = 1e-5\n",
    "D_optimizer = optim.Adam(D_params, lr=lr_d, betas=(0.8, 0.98), eps=1e-8, weight_decay=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v4cSlEsFJEls"
   },
   "outputs": [],
   "source": [
    "# Training the distilled Model [on same train split of ResNet50]\n",
    "# CNN training algorithm\n",
    "num_epochs = 30\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs,labels in train_loader:\n",
    "        # Move data to the same device as the model\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        x1 = inputs\n",
    "        D.set_requires_grad(True)\n",
    "        D_optimizer.zero_grad()\n",
    "        y1 = R(x1)\n",
    "        scaled_logits_1 = (y1 / distillation_temp)\n",
    "        x2 = F.interpolate(inputs, size=(128, 128), mode='bilinear', align_corners=False)\n",
    "        y2 = D(x2)\n",
    "        scaled_logits_2 = (y2 / distillation_temp)\n",
    "        # For distillation loss same [distillation_temp used for both]\n",
    "        # For KL-Loss log is taken for the student_model\n",
    "        p_prob = F.softmax(scaled_logits_1,dim=1)\n",
    "        q_log_prob = F.log_softmax(scaled_logits_2,dim=1)\n",
    "        loss_D = torch.sum((-p_prob) * (q_log_prob),dim=1).mean()\n",
    "        # For supervised Loss [temp is kept 1 only!]\n",
    "        loss_S = criterion(y2,labels)\n",
    "        weight = 1e-4\n",
    "        loss = (loss_D * (distillation_temp ** 2)) + (weight * loss_S)\n",
    "        loss.backward()\n",
    "        D_optimizer.step()\n",
    "        D_optimizer.zero_grad()\n",
    "\n",
    "    Distillation_Loss.append(loss_D.item())\n",
    "    Supervised_Loss.append(loss_S.item())\n",
    "    Total_Loss.append((loss_D + loss_S).item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gNeRSsfkNuGd"
   },
   "outputs": [],
   "source": [
    "epochs = range(len(Distillation_Loss))  # Use indices as x-axis values\n",
    "# Create a figure with three subplots (1 row, 3 columns)\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))\n",
    "# Plot Distillation Loss in the first subplot\n",
    "ax1.plot(epochs, Distillation_Loss, label='Distillation Loss', color='blue')\n",
    "ax1.set_xlabel('Index')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Distillation Loss')\n",
    "ax1.legend()\n",
    "# Plot Supervised Loss in the second subplot\n",
    "ax2.plot(epochs, Supervised_Loss, label='Supervised Loss', color='green')\n",
    "ax2.set_xlabel('Index')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.set_title('Supervised Loss')\n",
    "ax2.legend()\n",
    "# Plot Total Loss in the third subplot\n",
    "ax3.plot(epochs, Total_Loss, label='Total Loss', color='red')\n",
    "ax3.set_xlabel('Index')\n",
    "ax3.set_ylabel('Loss')\n",
    "ax3.set_title('Total Loss')\n",
    "ax3.legend()\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qFKHX3nNNuPV"
   },
   "outputs": [],
   "source": [
    "# Determine the final classification accuracy on TRAINING SET\n",
    "Correct = 0\n",
    "D.set_requires_grad(False)\n",
    "# We iterate through in batches\n",
    "# Due to BN layers bs must be > 1\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in train_loader:\n",
    "        labels = labels.to(device)\n",
    "        x2 = F.interpolate(inputs, size=(128, 128), mode='bilinear', align_corners=False).to(device)\n",
    "        y = D(x2)\n",
    "        # Get predicted classes\n",
    "        predictions = torch.argmax(y,dim=1)\n",
    "        # Check how many predictions match the ground truth labels\n",
    "        Correct += (predictions == labels).sum().item()\n",
    "# Final accuracy calculation\n",
    "accuracy = Correct * 100 / len(train_dataset)\n",
    "print(\"Final Accuracy of CNN on TRAIN:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Sxdq7b2NzA2"
   },
   "outputs": [],
   "source": [
    "# Determine the final classification accuracy on TESTING SET\n",
    "Correct = 0\n",
    "D.set_requires_grad(False)\n",
    "# We iterate through in batches\n",
    "# Due to BN layers bs must be > 1\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        labels = labels.to(device)\n",
    "        x2 = F.interpolate(inputs, size=(128, 128), mode='bilinear', align_corners=False).to(device)\n",
    "        y = D(x2)\n",
    "        # Get predicted classes\n",
    "        predictions = torch.argmax(y,dim=1)\n",
    "        # Check how many predictions match the ground truth labels\n",
    "        Correct += (predictions == labels).sum().item()\n",
    "# Final accuracy calculation\n",
    "accuracy = Correct * 100 / len(test_dataset)\n",
    "print(\"Final Accuracy of CNN on TEST:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vkN_-r57Dl3x"
   },
   "outputs": [],
   "source": [
    "torch.save(D,\"/models/KD.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WLnI8YBxN3B6"
   },
   "source": [
    "**Results of Distillation [1M small model , temp = 25]**\n",
    "* Accuracy of Teacher ResNet50 Model on TestSet: 41%\n",
    "* Accuracy of Distilled Model on TestSet without SupervisedLoss: 28.93% [Train:96.22%]\n",
    "* Accuracy of Distilled Model on TestSet with SupervisedLoss: 29.79% [Train:97.38%]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

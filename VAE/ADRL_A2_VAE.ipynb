{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xvlqxhxdEt7D"
   },
   "source": [
    "**GROUP 1 : ASSIGNMENT 2 : VAE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "jNKPvBZ4EEdU"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import InterpolationMode\n",
    "import os\n",
    "import zipfile\n",
    "import random\n",
    "from PIL import Image\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from scipy.linalg import sqrtm\n",
    "from torchvision import transforms, models\n",
    "# import pandas as pd\n",
    "# import json\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "vMCYQkMxFGmG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# Dont forget to use the GPU using .to(device)!\n",
    "# Use anaconda3/bin/python 3.12.4 base environment while using FIST server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r0x3Ug-yFN2l"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-YIHWSDyqfW"
   },
   "source": [
    "**Organising Data for Butterfly and Animal dataset and also DataAugmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4UobCH9lyqQt"
   },
   "outputs": [],
   "source": [
    "# Butterfly dataset\n",
    "# train has 6499 images from 75 classes\n",
    "# test has 2786 images\n",
    "\n",
    "# Convert the image to a tensor and normalize it between 0 and 1\n",
    "transform = transforms.Compose([transforms.Resize((128, 128)),transforms.ToTensor()])\n",
    "# Load CSV containing filenames and labels\n",
    "df = pd.read_csv(r\"C:\\ADRL data\\Butterfly dataset\\Training_set.csv\")\n",
    "N = len(df)\n",
    "# Create a mapping of distinct labels to indices\n",
    "class_names = sorted(df['label'].unique())\n",
    "class_to_idx = {class_name: idx for idx, class_name in enumerate(class_names)}\n",
    "\n",
    "# Save the class_to_idx dictionary to a JSON file\n",
    "with open(r\"C:\\ADRL data\\Butterfly dataset\\class_to_idx.json\", 'w') as f:\n",
    "    json.dump(class_to_idx, f)\n",
    "with open(r\"C:\\ADRL data\\Butterfly dataset\\class_to_idx.json\", 'r') as f:\n",
    "    loaded_class_to_idx = json.load(f)\n",
    "print(loaded_class_to_idx)\n",
    "\n",
    "# Initialize a tensor to store the images\n",
    "# Shape: (N, 3 (channels), 128 (H), 128 (W))\n",
    "image_tensor = torch.zeros((N, 3, 128, 128))\n",
    "# Initialize a tensor to store class indices\n",
    "# Shape: (N,)\n",
    "class_indices = torch.zeros(N, dtype=torch.long)  # Store class indices\n",
    "\n",
    "# Process each image and store it in the appropriate class index\n",
    "for i, row in df.iterrows():\n",
    "    print(i)\n",
    "    img_path = os.path.join(r\"C:\\ADRL data\\Butterfly dataset\\train\", row['filename'])  # Assuming images are in the 'train' folder\n",
    "    label = row['label']\n",
    "\n",
    "    # Load the image\n",
    "    img = Image.open(img_path).convert('RGB')  # Ensure image is in RGB mode\n",
    "    # (3,224,224)\n",
    "\n",
    "    # Apply transformations\n",
    "    img_tensor = transform(img)\n",
    "    # Get class index\n",
    "    class_idx = class_to_idx[label]\n",
    "\n",
    "    # Store the image tensor and class index\n",
    "    image_tensor[i] = img_tensor\n",
    "    class_indices[i] = class_idx\n",
    "\n",
    "torch.save(image_tensor, r\"C:\\ADRL data\\Butterfly dataset\\butterfly_training_images.pth\")\n",
    "torch.save(class_indices, r\"C:\\ADRL data\\Butterfly dataset\\butterfly_training_classindices.pth\")\n",
    "# Now, image_tensor contains the (N, 3, 128, 128) tensor.\n",
    "print(f'Successfully created tensor with shape: {image_tensor.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WDa7HEd3AAcp"
   },
   "outputs": [],
   "source": [
    "# Loading the dataset in [0,1] range\n",
    "animal_images = torch.load(\"/home/sahapthank/models/training_images.pt\")\n",
    "butterfly_images = torch.load(\"/home/sahapthank/models/butterfly_training_images.pth\" , weights_only = True)\n",
    "butterfly_labels = torch.load(\"/home/sahapthank/models/butterfly_training_classindices.pth\")\n",
    "anime_images = torch.load(\"/home/sahapthank/models/anime_images.pth\")\n",
    "animal_images_augmented = torch.load(\"/home/sahapthank/models/animal_images_augmented.pth\")\n",
    "butterfly_images_augmented = torch.load(\"/home/sahapthank/models/butterfly_images_augmented.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA AUGMENTATION FOR BUTTERFLY\n",
    "horizontal_flip = transforms.RandomHorizontalFlip(p=1)    # Randomly flip the image horizontally\n",
    "rotation = transforms.RandomRotation(degrees=20)    # Random rotation with expanding size\n",
    "color_jitter = transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)  # Random color adjustments\n",
    "\n",
    "tensor_shape = (6499,3,128,128)\n",
    "flipped_images = torch.zeros(tensor_shape)\n",
    "rotated_images = torch.zeros(tensor_shape)\n",
    "color_jittered_images = torch.zeros(tensor_shape)\n",
    "\n",
    "for i in range(6499):\n",
    "    flipped_images[i] = horizontal_flip(butterfly_images[i])\n",
    "    rotated_images[i] = rotation(butterfly_images[i])\n",
    "    color_jittered_images[i]= color_jitter(butterfly_images[i])\n",
    "\n",
    "butterfly_images_augmented = torch.stack([butterfly_images,flipped_images,rotated_images,color_jittered_images])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "47WjDexBA8Ld"
   },
   "outputs": [],
   "source": [
    "# DATA AUGMENTATION FOR ANIMAL\n",
    "horizontal_flip = transforms.RandomHorizontalFlip(p=1)    # Randomly flip the image horizontally\n",
    "rotation = transforms.RandomRotation(degrees=20)    # Random rotation with expanding size\n",
    "color_jitter = transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)  # Random color adjustments\n",
    "\n",
    "tensor_shape = (90,60,3,128,128)\n",
    "flipped_images = torch.zeros(tensor_shape)\n",
    "rotated_images = torch.zeros(tensor_shape)\n",
    "color_jittered_images = torch.zeros(tensor_shape)\n",
    "\n",
    "for i in range(90):\n",
    "    for j in range(60):\n",
    "        flipped_images[i][j] = horizontal_flip(animal_images[i][j])\n",
    "        rotated_images[i][j] = rotation(animal_images[i][j])\n",
    "        color_jittered_images[i][j] = color_jitter(animal_images[i][j])\n",
    "\n",
    "animal_images_augmented = torch.stack([animal_images,flipped_images,rotated_images,color_jittered_images])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KqYGny8SFPxz"
   },
   "source": [
    "**[Q1] Vanilla VAE implementation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SJzgWs0z_pOB"
   },
   "source": [
    "**Some implementation Observations**\n",
    "* In the ENCODER we use {1 NN} that use the features extracted from CNN layers to predict mean and log(variance). Since this can be any real number using ReLU as final activation saturates the learning after 50 steps and variance learnt becomes 0 collapsing all images!\n",
    "* Therefore to cover the entire range space of (-inf,inf) there is no last activation function and nn.Identity is used!\n",
    "* Since VAE is trained using Pixel-based MSE_loss it is better to keep the images in [0,1] as opposed to [-1,1] while using GAN. Therefore Tanh is replaced by Sigmoid in the last layer of DECODER\n",
    "* Without batch normalisation even prior to training just after initialisation the DECODER gives images which are just \"black\" meaning most probably \"nan\". Therefore batch-normalisation is crucial similar to Generator of GAN in all layers except the last one\n",
    "* KL divergence becoming close to zero very quickly and saturating..so we define kl_weight to manually control weightage given to it and focus first on reconstruction loss only\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8fXT2u1GFPBd"
   },
   "outputs": [],
   "source": [
    "# In a vanilla VAE, the encoder does following:\n",
    "# 1) Input: Image of shape (bs,3,128,128)\n",
    "# 2) Outputs 2 things for a fixed (x_i) when z is d-dimensional\n",
    "# Mean vector (μ) of shape (bs,d) of {q_phi(z|x_i)}\n",
    "# Log variance vector (log(σ²)) of shape (bs,d) since co-variance is assumed to be diagonal [dimensions are indep]\n",
    "# This is done to prevent the model from learning negative values of variance\n",
    "\n",
    "class Vanilla_VAE_Encoder(nn.Module):\n",
    "    def __init__(self, architecture):\n",
    "        # architecture = [cnn_architecture, nn_architecute]\n",
    "        # nn_architecture = [nn_layers, nn_batch_norm , nn_activation]\n",
    "        # cnn_architecture = [conv_params, cnn_batchnorm, cnn_activation_fn]\n",
    "        # conv_params = [[in_channels, output_channels, kernel_size, stride, padding], ...]\n",
    "        # nn/cnn_batchnorm = [False, True, ...]\n",
    "        # nn/cnn_activation = [nn.ReLU(), nn.LeakyReLU(), ...]\n",
    "\n",
    "        super(Vanilla_VAE_Encoder, self).__init__()\n",
    "        assert len(architecture[0][0]) == len(architecture[0][1]) == len(architecture[0][2])\n",
    "        assert len(architecture[1][0]) == len(architecture[1][1]) == len(architecture[1][2])\n",
    "        self.conv_params = architecture[0][0]\n",
    "        self.cnn_batchnorm = architecture[0][1]\n",
    "        self.cnn_activation = architecture[0][2]\n",
    "        self.nn_layers = architecture[1][0]\n",
    "        self.nn_batchnorm = architecture[1][1]\n",
    "        self.nn_activation = architecture[1][2]\n",
    "\n",
    "        layers_0 = []\n",
    "        for j,i in enumerate(self.conv_params):\n",
    "            (layers_0).append(nn.Conv2d(in_channels=i[0],out_channels=i[1],kernel_size=i[2],stride=i[3],padding=i[4]))\n",
    "            if (self.cnn_batchnorm)[j]:\n",
    "                (layers_0).append(nn.BatchNorm2d(i[1]))\n",
    "            (layers_0).append(self.cnn_activation[j])\n",
    "\n",
    "        layers_1 = []\n",
    "        for i in range(len(self.nn_layers)):\n",
    "            in_dim, out_dim = self.nn_layers[i]\n",
    "            (layers_1).append(nn.Linear(in_dim, out_dim))\n",
    "            if self.nn_batchnorm[i]:\n",
    "                (layers_1).append(nn.BatchNorm1d(out_dim))\n",
    "            (layers_1).append(self.nn_activation[i])\n",
    "\n",
    "        # Stack convolutional layers in a Sequential block\n",
    "        # Create a single FFNN for mu and sigma o/p is \"2d\" dimension\n",
    "        self.cnn = nn.Sequential(*(layers_0))\n",
    "        self.nn = nn.Sequential(*(layers_1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.cnn(x)\n",
    "        # Flatten the tensor for the fully connected layers\n",
    "        features = torch.flatten(features, start_dim=1)\n",
    "        c = self.nn(features) #(bs,2d)\n",
    "        d = int(c.size(1)/2)\n",
    "        mu = c[:, :d]  #(bs,d)\n",
    "        logvar = c[:, d:] #(bs,d)\n",
    "        return mu,logvar\n",
    "\n",
    "    # We will use this to prevent calculating gradients wrt parameters of Encoder\n",
    "    def set_requires_grad(self, requires_grad):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = requires_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IEpzdihQFPE0"
   },
   "outputs": [],
   "source": [
    "# The decoders input is z = (bs,d) and outputs image y similar to Generator of DCGAN\n",
    "# While inference we only need the Decoder and can pass any random z to get an image\n",
    "# While trying to optimize using MSE loss it is implicitly assumed that the decoder outputs the mean image\n",
    "# Note that VAE work on PIXEL-BASED LOSSES so better in range [0,1]\n",
    "\n",
    "class Vanilla_VAE_Decoder_TransposeConv(nn.Module):\n",
    "    def __init__(self, architecture):\n",
    "        # The architecture is almost same as DCGAN\n",
    "        # Suppose we use transpose_convolutions then we first use 1 Linear layer to convert to higher dim\n",
    "        # Then we apply series of transpose convolutions!\n",
    "\n",
    "        super(Vanilla_VAE_Decoder_TransposeConv, self).__init__()\n",
    "        assert len(architecture[0]) == len(architecture[1]) == len(architecture[2])\n",
    "        self.transpose_conv_params = architecture[0]\n",
    "        self.use_batchnorm = architecture[1]\n",
    "        self.activation_fn = architecture[2]\n",
    "        layers = []\n",
    "        # Starting with input latent vector (e.g., size d = 100)\n",
    "        for j, i in enumerate(self.transpose_conv_params):\n",
    "            layers.append(nn.ConvTranspose2d(in_channels=i[0], out_channels=i[1], kernel_size=i[2], stride=i[3], padding=i[4]))\n",
    "            if self.use_batchnorm[j]:\n",
    "                layers.append(nn.BatchNorm2d(i[1]))\n",
    "            layers.append(self.activation_fn[j])\n",
    "        # nn.Sigmoid() is used as the final activation function to [0,1]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    # Sampling images from the generator G\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    # We will use this to prevent calculating gradients wrt parameters of Decoder\n",
    "    def set_requires_grad(self, requires_grad):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = requires_grad\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zhCbnQVsOEab"
   },
   "outputs": [],
   "source": [
    "# Reparameterisation trick\n",
    "# This acts as g_phi(e,x_i) as in the paper\n",
    "# g is a differentiable transformation\n",
    "def reparam_Gaussian(mu,sigma,L):\n",
    "    # General method is using Cholensky Decomposition\n",
    "    # But here lets use diagonal matrix to simplify things\n",
    "    # Ensure mu,sigma are of shape (bs,d)\n",
    "    bs = mu.size(0)\n",
    "    d = mu.size(1)\n",
    "    e = torch.distributions.MultivariateNormal(torch.zeros((d,)), torch.eye(d)).sample([bs, L]).to(device)\n",
    "    # e is of shape (bs,L,d)\n",
    "    # To broadcast we make it mu,sigma (bs,1,d)\n",
    "    # (sigma * e ) is done elementwise\n",
    "    mu = mu.unsqueeze(1)\n",
    "    sigma = sigma.unsqueeze(1)\n",
    "    z = (mu + (sigma * e))\n",
    "    return z #(bs,L,d)\n",
    "\n",
    "# Sample from a Gaussian for inference\n",
    "def sample_gaussian(mean, covariance_matrix , n_samples):\n",
    "    # mean should be of size ([d])\n",
    "    # variance is the covariance matrix of size ([d,d]) must be +ve semidefinite\n",
    "    z = torch.distributions.MultivariateNormal(mean,covariance_matrix)\n",
    "    return z.sample([n_samples])\n",
    "    # returns of size ([n_samples,d])\n",
    "\n",
    "# Sample from Real_images [BUTTERFLY]\n",
    "def sample_real_butterfly(n_samples):\n",
    "    i_indices = torch.randint(0, 6499, (n_samples,)).tolist()\n",
    "    sampled_images = []\n",
    "    for i in i_indices:\n",
    "        sampled_images.append(butterfly_images[i])\n",
    "    sampled_images_tensor = torch.stack(sampled_images)\n",
    "    return sampled_images_tensor\n",
    "\n",
    "# Sample from Real_images [ANIMAL]\n",
    "def sample_real_animal(n_samples):\n",
    "    i_indices = torch.randint(0, 90, (n_samples,)).tolist() # chooses class\n",
    "    j_indices = torch.randint(0, 60, (n_samples,)).tolist() # chooses image in class\n",
    "    sampled_images = []\n",
    "    for i, j in zip(i_indices, j_indices):\n",
    "        sampled_images.append(animal_images[i, j])\n",
    "    sampled_images_tensor = torch.stack(sampled_images)\n",
    "    return sampled_images_tensor\n",
    "\n",
    "# Sample from Real_images [ANIMAL] including augmented\n",
    "def sample_real_augmented_animal(n_samples):\n",
    "    i_indices = torch.randint(0, 90, (n_samples,)).tolist() # chooses class\n",
    "    j_indices = torch.randint(0, 60, (n_samples,)).tolist() # chooses image in class\n",
    "    k_indices = torch.randint(0, 4, (n_samples,)).tolist() # chooses augmentation type\n",
    "    sampled_images = []\n",
    "    for i, j , k in zip(i_indices, j_indices , k_indices):\n",
    "        sampled_images.append(animal_images_augmented[k, i, j])\n",
    "    sampled_images_tensor = torch.stack(sampled_images)\n",
    "    return sampled_images_tensor\n",
    "\n",
    "# Sample from Real_images [BUTTERFLY] including augmented\n",
    "def sample_real_augmented_butterfly(n_samples):\n",
    "    i_indices = torch.randint(0, 6499, (n_samples,)).tolist() # chooses image\n",
    "    k_indices = torch.randint(0, 4, (n_samples,)).tolist() # chooses augmentation type\n",
    "    sampled_images = []\n",
    "    for i, k in zip(i_indices, k_indices):\n",
    "        sampled_images.append(butterfly_images_augmented[k, i])\n",
    "    sampled_images_tensor = torch.stack(sampled_images)\n",
    "    return sampled_images_tensor\n",
    "\n",
    "# Sample from Real_images [ANIME]\n",
    "def sample_real_anime(n_samples):\n",
    "    i_indices = torch.randint(0, 21551, (n_samples,)).tolist()\n",
    "    sampled_images = []\n",
    "    for i in i_indices:\n",
    "        sampled_images.append(anime_images[i])\n",
    "    sampled_images_tensor = torch.stack(sampled_images)\n",
    "    return sampled_images_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YjShdSfS05AI"
   },
   "outputs": [],
   "source": [
    "# Formula for CNN convolutions\n",
    "# output_size = ({input_size - kernel_size + 2*padding}/stride) + 1\n",
    "# Formula for Transpose Conv\n",
    "# Assume input size is [d_1,1,1] gets converted to [3,128,128]\n",
    "# output_size = {(input_size - 1) * stride} - (2*padding) + kernel_size}\n",
    "\n",
    "# TRIAL 1\n",
    "# d_1 = 128\n",
    "# conv_params_1 = [[3,16,4,2,1],[16,16,4,2,1],[16,32,4,2,1],[32,64,4,2,1]]\n",
    "# cnn_batchnorm_1 = [False]+ [True for _ in range(3)]\n",
    "# cnn_activation_1 = [nn.LeakyReLU(0.2, inplace=False) for _ in range(4)]\n",
    "# nn_layers_1 = [[4096,1024],[1024,2*d_1]]\n",
    "# nn_batchnorm_1 = [True for _ in range(1)] + [False]\n",
    "# nn_activation_1 = [nn.ReLU() for _ in range(1)] + [nn.Identity()]\n",
    "# cnn_arch_1 = [conv_params_1, cnn_batchnorm_1, cnn_activation_1]\n",
    "# nn_arch_1 = [nn_layers_1, nn_batchnorm_1, nn_activation_1]\n",
    "# E_arch_1 = [cnn_arch_1, nn_arch_1]\n",
    "# transpose_conv_params_1 = [[d_1,512,4,1,0],[512,256,4,2,1],[256,128,4,2,1],[128,64,4,2,1],[64,32,4,2,1],[32,3,4,2,1]]\n",
    "# use_batchnorm_1 = [True for _ in range(5)] + [False]\n",
    "# activation_fn_1 = [nn.ReLU(inplace = False) for _ in range(5)] + [nn.Sigmoid()]\n",
    "# D_arch_1 = [transpose_conv_params_1, use_batchnorm_1, activation_fn_1]\n",
    "\n",
    "# TRIAL 2\n",
    "# TRIAL 2 [17,481,424 3,838,371]\n",
    "# d_1 = 128\n",
    "# conv_params_1 = [[3,16,4,2,1],[16,32,4,2,1],[32,64,4,2,1],[64,128,4,2,1]]\n",
    "# cnn_batchnorm_1 = [True for _ in range(4)]\n",
    "# cnn_activation_1 = [nn.LeakyReLU(0.2, inplace=False) for _ in range(4)]\n",
    "# nn_layers_1 = [[8192,2048],[2048,2*d_1]]\n",
    "# nn_batchnorm_1 = [True for _ in range(1)] + [False]\n",
    "# nn_activation_1 = [nn.ReLU() for _ in range(1)] + [nn.Identity()]\n",
    "# cnn_arch_1 = [conv_params_1, cnn_batchnorm_1, cnn_activation_1]\n",
    "# nn_arch_1 = [nn_layers_1, nn_batchnorm_1, nn_activation_1]\n",
    "# E_arch_1 = [cnn_arch_1, nn_arch_1]\n",
    "# transpose_conv_params_1 = [[d_1,512,4,1,0],[512,256,4,2,1],[256,128,4,2,1],[128,64,4,2,1],[64,32,4,2,1],[32,3,4,2,1]]\n",
    "# use_batchnorm_1 = [True for _ in range(5)] + [False]\n",
    "# activation_fn_1 = [nn.ReLU(inplace = False) for _ in range(5)] + [nn.Sigmoid()]\n",
    "# D_arch_1 = [transpose_conv_params_1, use_batchnorm_1, activation_fn_1]\n",
    "\n",
    "# TRIAL 3\n",
    "d_1 = 128\n",
    "conv_params_1 = [[3,16,4,2,1],[16,32,4,2,1],[32,64,4,2,1],[64,128,4,2,1],[128,256,4,2,1],[256,512,4,1,0]]\n",
    "cnn_batchnorm_1 = [True for _ in range(6)]\n",
    "cnn_activation_1 = [nn.LeakyReLU(0.2, inplace=False) for _ in range(6)]\n",
    "nn_layers_1 = [[512,512],[512,2*d_1]]\n",
    "nn_batchnorm_1 = [True for _ in range(1)] + [False]\n",
    "nn_activation_1 = [nn.ReLU() for _ in range(1)] + [nn.Identity()]\n",
    "cnn_arch_1 = [conv_params_1, cnn_batchnorm_1, cnn_activation_1]\n",
    "nn_arch_1 = [nn_layers_1, nn_batchnorm_1, nn_activation_1]\n",
    "E_arch_1 = [cnn_arch_1, nn_arch_1]\n",
    "transpose_conv_params_1 = [[d_1,512,4,1,0],[512,256,4,2,1],[256,128,4,2,1],[128,64,4,2,1],[64,32,4,2,1],[32,3,4,2,1]]\n",
    "use_batchnorm_1 = [True for _ in range(5)] + [False]\n",
    "activation_fn_1 = [nn.ReLU(inplace = False) for _ in range(5)] + [nn.Sigmoid()]\n",
    "D_arch_1 = [transpose_conv_params_1, use_batchnorm_1, activation_fn_1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1S_VKtMtSIe-"
   },
   "outputs": [],
   "source": [
    "# Vanilla VAE is based on Auto-Encoding Variational Bayes paper\n",
    "# Approximates the log-likelihood of the data = KL(P_r,P_theta)\n",
    "# EM fails when we use NN to model q(z|x) due to intractability so we use Variational method\n",
    "# A = E_{q_phi(z|x)}(log p_theta(x|z)) is the reconstruction-term\n",
    "# B = D_KL(q_phi(z|x) || p(z)) is the prior-matching term\n",
    "# C = D_KL(q_phi(z|x) || p_theta(z|x))\n",
    "# We get l_theta(x) := log(p_theta(x)) = A - B + C = ELBO + C = F_theta(q_phi) + C\n",
    "# C is intractable so (A-B) is the one optimised to get tighter bounds for l_theta(x)\n",
    "# For ease of computations q_phi(z|x) and p_theta(z) are assumed to be Gaussians!\n",
    "# p_theta(z) is actually fixed to be N(0,I) but can be modified to make it learnable\n",
    "# q_phi(z|x) is N(z,mu_phi(x),sigma_squared_phi(x)I)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "# l is for #samples used while reparameterising\n",
    "lr_VAE, m, d = 1e-4, 64, 128\n",
    "E_arch = E_arch_1\n",
    "D_arch = D_arch_1\n",
    "E = Vanilla_VAE_Encoder(E_arch).to(device)\n",
    "D = Vanilla_VAE_Decoder_TransposeConv(D_arch).to(device)\n",
    "elbo = []\n",
    "A_loss = []\n",
    "B_loss = []\n",
    "vae_params = list(E.parameters()) + list(D.parameters())\n",
    "vae_optimizer = optim.Adam(vae_params, lr=lr_VAE, betas=(0.8, 0.98), eps=1e-8, weight_decay=0)\n",
    "# vae_optimizer = optim.RMSprop(vae_params, lr=lr_VAE, alpha=0.99, eps=1e-8, weight_decay=0)\n",
    "# vae_optimizer = optim.SGD(vae_params, lr=lr_VAE, momentum=0.9, weight_decay=0)\n",
    "# scheduler = lr_scheduler.ReduceLROnPlateau(vae_optimizer, mode='min', factor=0.7, patience=5, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OYv2MvTakkZ6"
   },
   "outputs": [],
   "source": [
    "# For manual changing during training\n",
    "lr_VAE, m, d = 1e-4, 64, 128\n",
    "vae_optimizer = optim.Adam(vae_params, lr=lr_VAE, betas=(0.8, 0.98), eps=1e-8, weight_decay=0)\n",
    "# vae_optimizer = optim.RMSprop(vae_params, lr=lr_VAE, alpha=0.99, eps=1e-8, weight_decay=0)\n",
    "# vae_optimizer = optim.SGD(vae_params, lr=lr_VAE, momentum=0.9, weight_decay=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SAdgANWSgaFN"
   },
   "outputs": [],
   "source": [
    "# We use the \"2nd Loss function mentioned in the paper := L_b\"\n",
    "# Since B is KL btw 2 gaussians and is known analytically [like regularisation]\n",
    "# A is estimated using Monte-Carlo\n",
    "# Vanilla VAE training algorithm\n",
    "num_epochs = 50\n",
    "mini_batch_epochs = 101\n",
    "kl_weight = 1e-5 * (1)\n",
    "L = 3\n",
    "mse_loss = nn.MSELoss()\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for _ in range(mini_batch_epochs):\n",
    "        D.set_requires_grad(True)\n",
    "        E.set_requires_grad(True)\n",
    "        vae_optimizer.zero_grad()\n",
    "\n",
    "        x = sample_real_butterfly(m).to(device)\n",
    "        mu_x, log_sigma_squared_x = E(x)\n",
    "        sigma_squared_x = torch.exp(log_sigma_squared_x)\n",
    "        sigma_x = torch.sqrt(sigma_squared_x)\n",
    "\n",
    "        # First of all the prior term B can be computed\n",
    "        # Each x_i we get a scalar\n",
    "        # We do a sum over dimensions d\n",
    "        # So final shape of B is (bs,1)\n",
    "        B = (kl_weight) * (0.5) * torch.sum(((-1) - (log_sigma_squared_x) + (mu_x.pow(2)) + (sigma_squared_x)), dim=1, keepdim=True)\n",
    "\n",
    "        # We need to sample \"L-many z_j\" for each \"x_i\"\n",
    "        # Using the z we need to get y = (bs,L,3,128,128) as images!\n",
    "        z = (reparam_Gaussian(mu_x, sigma_x, L)).to(device)\n",
    "        D_output = []\n",
    "        for j in range(L):\n",
    "            # Select the jth sample from z\n",
    "            z_j = (z[:, j, :]).unsqueeze(-1).unsqueeze(-1)  # Shape (bs,d,1,1)\n",
    "            D_images = D(z_j)  # Shape (bs, 3, 128, 128)\n",
    "            D_output.append(D_images)\n",
    "        y = torch.stack(D_output, dim=1).to(device) #(bs,L,3,128,128)\n",
    "\n",
    "        # Finally the reconstruction loss term A has to be computed\n",
    "        # MSE pixel-wise broadcasts and averages over {bs,L,pixels}\n",
    "        x_r = x.unsqueeze(1).repeat(1, L, 1, 1, 1).to(device)\n",
    "        A = mse_loss(y,x_r)\n",
    "\n",
    "        # We need to maximize the ELBO\n",
    "        loss_VAE = ((B.mean()) + A)\n",
    "        loss_VAE.backward()\n",
    "        vae_optimizer.step()\n",
    "        vae_optimizer.zero_grad()\n",
    "\n",
    "    # .item() detaches tensor automatically\n",
    "    B1 = (B.mean().item()/(kl_weight))\n",
    "    elbo.append(-(B1 + A.item()))\n",
    "    A_loss.append(A.item())\n",
    "    B_loss.append(B1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5yme9nRFCMaO"
   },
   "outputs": [],
   "source": [
    "# Create a figure with three subplots (1 row, 3 columns)\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))\n",
    "# Plot A_loss in the first subplot\n",
    "A_steps = range(len(A_loss))\n",
    "ax1.plot(A_steps, A_loss, label='A_loss', color='blue')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('A')\n",
    "ax1.set_title('Reconstruction_Loss')\n",
    "ax1.legend()\n",
    "# Plot B_loss in the second subplot\n",
    "B_steps = range(len(B_loss))\n",
    "ax2.plot(B_steps, B_loss, label='B_loss', color='orange')\n",
    "ax2.set_xlabel('Epochs')\n",
    "ax2.set_ylabel('B')\n",
    "ax2.set_title('KL_Divg')\n",
    "ax2.legend()\n",
    "# Plot ELBO in the third subplot\n",
    "vae_steps = range(len(elbo))\n",
    "ax3.plot(vae_steps, elbo, label='ELBO', color='green')\n",
    "ax3.set_xlabel('Epochs')\n",
    "ax3.set_ylabel('ELBO')\n",
    "ax3.set_title('ELBO')\n",
    "ax3.legend()\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BXh9-nSK7I4-"
   },
   "outputs": [],
   "source": [
    "# Inference\n",
    "d = 128\n",
    "s = 100\n",
    "with torch.no_grad():\n",
    "    z = (sample_gaussian(torch.zeros((d,)),torch.eye(d),s)).unsqueeze(-1).unsqueeze(-1).to(device)\n",
    "    y = (D(z))\n",
    "    images = y.detach().cpu().numpy()\n",
    "    # Create a figure for the grid of images\n",
    "    fig, axes = plt.subplots(nrows=10, ncols=10, figsize=(15, 15))\n",
    "    # Loop through the 100 images and display them in the grid\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        img = images[i].transpose(1, 2, 0)\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tz1dv982L6LT"
   },
   "outputs": [],
   "source": [
    "# Plotting reconstructed images when fed with training images\n",
    "s = 100\n",
    "with torch.no_grad():\n",
    "    x = sample_real_butterfly(s).to(device)\n",
    "    mu_x, log_sigma_squared_x = E(x)\n",
    "    sigma_squared_x = torch.exp(log_sigma_squared_x)\n",
    "    sigma_x = torch.sqrt(sigma_squared_x)\n",
    "    z = (reparam_Gaussian(mu_x, sigma_x, 1)).to(device)\n",
    "    z = z.squeeze(1).unsqueeze(-1).unsqueeze(-1)\n",
    "    y = D(z)\n",
    "\n",
    "# Assuming both sets have the same number of images\n",
    "num_images = min(x.size(0), y.size(0))\n",
    "images_0 = x.detach().cpu().numpy()\n",
    "images_1 = y.detach().cpu().numpy()\n",
    "# Create a figure for the grid of images\n",
    "fig, axes = plt.subplots(nrows=num_images//10, ncols=20, figsize=(30, 15))\n",
    "\n",
    "# Loop through the images and display them side by side\n",
    "for i in range(num_images):\n",
    "    # Show the first set of images\n",
    "    img_0 = images_0[i].transpose(1, 2, 0)\n",
    "    axes[i // 10, (i % 10) * 2].imshow(img_0)\n",
    "    axes[i // 10, (i % 10) * 2].axis('off')\n",
    "    # Show the second set of images\n",
    "    img_1 = images_1[i].transpose(1, 2, 0)\n",
    "    axes[i // 10, (i % 10) * 2 + 1].imshow(img_1)\n",
    "    axes[i // 10, (i % 10) * 2 + 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ph8p2wvfigNm"
   },
   "source": [
    "**Some other techniques were tried**\n",
    "1. KL cost annealing https://arxiv.org/pdf/1511.06349 Section 3.1\n",
    "2. Cyclical KL Annealing Schedule https://aclanthology.org/N19-1021.pdf\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lf0F33KEQk7z"
   },
   "source": [
    "**[Q2] CNN-based Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fNhEK4NAQkQx"
   },
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, architecture):\n",
    "        # architecture = [cnn_architecture, nn_architecute]\n",
    "        # nn_architecture = [nn_layers, nn_batch_norm , nn_activation]\n",
    "        # cnn_architecture = [conv_params, cnn_batchnorm, cnn_activation_fn]\n",
    "        # conv_params = [[in_channels, output_channels, kernel_size, stride, padding], ...]\n",
    "        # nn/cnn_batchnorm = [False, True, ...]\n",
    "        # nn/cnn_activation = [nn.ReLU(), nn.LeakyReLU(), ...]\n",
    "\n",
    "        super(Classifier, self).__init__()\n",
    "        assert len(architecture[0][0]) == len(architecture[0][1]) == len(architecture[0][2])\n",
    "        assert len(architecture[1][0]) == len(architecture[1][1]) == len(architecture[1][2])\n",
    "        self.conv_params = architecture[0][0]\n",
    "        self.cnn_batchnorm = architecture[0][1]\n",
    "        self.cnn_activation = architecture[0][2]\n",
    "        self.nn_layers = architecture[1][0]\n",
    "        self.nn_batchnorm = architecture[1][1]\n",
    "        self.nn_activation = architecture[1][2]\n",
    "\n",
    "        layers_0 = []\n",
    "        for j,i in enumerate(self.conv_params):\n",
    "            (layers_0).append(nn.Conv2d(in_channels=i[0],out_channels=i[1],kernel_size=i[2],stride=i[3],padding=i[4]))\n",
    "            if (self.cnn_batchnorm)[j]:\n",
    "                (layers_0).append(nn.BatchNorm2d(i[1]))\n",
    "            (layers_0).append(self.cnn_activation[j])\n",
    "\n",
    "        layers_1 = []\n",
    "        for i in range(len(self.nn_layers)):\n",
    "            in_dim, out_dim = self.nn_layers[i]\n",
    "            (layers_1).append(nn.Linear(in_dim, out_dim))\n",
    "            if self.nn_batchnorm[i]:\n",
    "                (layers_1).append(nn.BatchNorm1d(out_dim))\n",
    "            (layers_1).append(self.nn_activation[i])\n",
    "\n",
    "        # Stack convolutional layers in a Sequential block\n",
    "        # Create 1 FFNN\n",
    "        self.cnn = nn.Sequential(*(layers_0))\n",
    "        self.nn = nn.Sequential(*(layers_1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.cnn(x)\n",
    "        features = torch.flatten(features, start_dim=1)\n",
    "        logits = self.nn(features)\n",
    "        return logits\n",
    "\n",
    "    # We will use this to prevent calculating gradients wrt parameters of Classifier\n",
    "    def set_requires_grad(self, requires_grad):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = requires_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8wAU-FwVjVoa"
   },
   "outputs": [],
   "source": [
    "# Formula for CNN convolutions\n",
    "# output_size = ({input_size - kernel_size + 2*padding}/stride) + 1\n",
    "\n",
    "# TRIAL 1\n",
    "conv_params_3 = [[3,16,4,2,1],[16,16,4,2,1],[16,32,4,2,1],[32,32,4,2,1]]\n",
    "# CONVERT from [3,128,128] = 49,152 to [32,8,8] = 2048\n",
    "cnn_batchnorm_3 = [True for _ in range(4)]\n",
    "cnn_activation_3 = [nn.LeakyReLU(0.2, inplace=False) for _ in range(4)]\n",
    "classifier_cnn = [conv_params_3, cnn_batchnorm_3, cnn_activation_3]\n",
    "nn_layers_3 = [[2048,75]]\n",
    "nn_batchnorm_3 = [True for _ in range(0)] + [False]\n",
    "nn_activation_3 = [nn.ReLU() for _ in range(0)] + [nn.Identity()]\n",
    "# We will use soft-max while doing the CE loss\n",
    "classifier_nn = [nn_layers_3, nn_batchnorm_3, nn_activation_3]\n",
    "classifier_arch = [classifier_cnn , classifier_nn]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bB6RD3zqjboZ"
   },
   "outputs": [],
   "source": [
    "Classifier_check = Classifier(classifier_arch)\n",
    "r1 = torch.randn((7,3,128,128))\n",
    "r2 = Classifier_check(r1)\n",
    "print(r2.size())\n",
    "TP1 = sum(p.numel() for p in Classifier_check.parameters())\n",
    "print(TP1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WvgnG6kEjcgf"
   },
   "outputs": [],
   "source": [
    "# Initialize model, loss function, and optimizer\n",
    "lr_CNN, m = 1e-4, 64\n",
    "C = Classifier(classifier_arch).to(device)\n",
    "CE_loss = []\n",
    "C_params = list(C.parameters())\n",
    "C_optimizer = optim.Adam(C_params, lr=lr_CNN, betas=(0.8, 0.98), eps=1e-8, weight_decay=0)\n",
    "\n",
    "# Since butterfly images test set has no labels we create a random split over training\n",
    "all_indices = set(range(6499))\n",
    "train_split = set(random.sample(range(6499), 5000))\n",
    "test_split = list(all_indices - train_split)\n",
    "train_split = list(train_split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C1BCVjYCjfRc"
   },
   "outputs": [],
   "source": [
    "# CNN training algorithm\n",
    "num_epochs = 10\n",
    "mini_batch_epochs = 77\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for _ in range(mini_batch_epochs):\n",
    "        C.set_requires_grad(True)\n",
    "        C_optimizer.zero_grad()\n",
    "\n",
    "        j_indices = torch.randint(0,5000, (m,)).tolist()\n",
    "        i_indices = [train_split[t] for t in j_indices]\n",
    "        sampled_images = []\n",
    "        Labels = []\n",
    "        for i in i_indices:\n",
    "            sampled_images.append(butterfly_images[i])\n",
    "            Labels.append(butterfly_labels[i])\n",
    "\n",
    "        x = torch.stack(sampled_images).to(device)\n",
    "        Labels = torch.tensor(Labels).to(device)\n",
    "        y = C(x)\n",
    "        loss = criterion(y,Labels)\n",
    "        loss.backward()\n",
    "        C_optimizer.step()\n",
    "        C_optimizer.zero_grad()\n",
    "\n",
    "    CE_loss.append(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QmL9i44fjiI_"
   },
   "outputs": [],
   "source": [
    "# Plot the losses\n",
    "plt.plot(CE_loss, label='Cross-Entropy Loss')\n",
    "# Adding titles and labels\n",
    "plt.title('Cross-Entropy Loss Over Time')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kMfSJLqljyGq"
   },
   "outputs": [],
   "source": [
    "# Determine the final classification accuracy on TRAINING SET\n",
    "batch_size = 64\n",
    "Correct = 0\n",
    "\n",
    "# We iterate through in batches\n",
    "# Due to BN layers bs must be > 1\n",
    "for i in range(0, 5000, batch_size):\n",
    "    with torch.no_grad():\n",
    "        batch_indices = train_split[i:i+batch_size]\n",
    "        # Create batches of butterfly_images and labels\n",
    "        batch_images = butterfly_images[batch_indices].to(device)\n",
    "        batch_labels = butterfly_labels[batch_indices].to(device)\n",
    "        # Classification using the images\n",
    "        y = C(batch_images)\n",
    "        # Get predicted classes\n",
    "        predictions = torch.argmax(y, dim=1)\n",
    "        # Check how many predictions match the ground truth labels\n",
    "        Correct += (predictions == batch_labels).sum().item()\n",
    "\n",
    "# Final accuracy calculation\n",
    "accuracy = Correct * 100 / 5000\n",
    "print(\"Final Accuracy of CNN on TRAIN:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RJrVuys6kd9d"
   },
   "outputs": [],
   "source": [
    "# Determine the final classification accuracy on TEST SET\n",
    "batch_size = 64\n",
    "Correct = 0\n",
    "\n",
    "# We iterate through in batches\n",
    "# Due to BN layers bs must be > 1\n",
    "for i in range(0, 1499, batch_size):\n",
    "    with torch.no_grad():\n",
    "        batch_indices = test_split[i:i+batch_size]\n",
    "        # Create batches of butterfly_images and labels\n",
    "        batch_images = butterfly_images[batch_indices].to(device)\n",
    "        batch_labels = butterfly_labels[batch_indices].to(device)\n",
    "        # Classification using the images\n",
    "        y = C(batch_images)\n",
    "        # Get predicted classes\n",
    "        predictions = torch.argmax(y, dim=1)\n",
    "        # Check how many predictions match the ground truth labels\n",
    "        Correct += (predictions == batch_labels).sum().item()\n",
    "\n",
    "# Final accuracy calculation\n",
    "accuracy = Correct * 100 / 1499\n",
    "print(\"Final Accuracy of CNN on TEST:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u1qRUzmJXSh4"
   },
   "source": [
    "**Accuracy of Classifier directly on the Butterfly Images**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_tDw1hhaQ0Qv"
   },
   "source": [
    "* After 25 epochs: Final Accuracy of CNN on TRAIN: 97.82%\n",
    "* After 25 epochs: Final Accuracy of CNN on TEST: 56.43%\n",
    "* #CNN parameters = 183,403\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MeSofcr8Ng5B"
   },
   "source": [
    "**[Q3] Posterior Inference and MLP classification using Latents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "odEXHUkOOMZ_"
   },
   "outputs": [],
   "source": [
    "class Classifier_MLP(nn.Module):\n",
    "    def __init__(self, architecture):\n",
    "        # architecture = nn_architecture\n",
    "        # nn_architecture = [nn_layers, nn_batch_norm , nn_activation]\n",
    "        # nn_batchnorm = [False, True, ...]\n",
    "        # nn_activation = [nn.ReLU(), nn.LeakyReLU(), ...]\n",
    "\n",
    "        super(Classifier_MLP, self).__init__()\n",
    "        assert len(architecture[0]) == len(architecture[1]) == len(architecture[2])\n",
    "        self.nn_layers = architecture[0]\n",
    "        self.nn_batchnorm = architecture[1]\n",
    "        self.nn_activation = architecture[2]\n",
    "\n",
    "        layers_1 = []\n",
    "        for i in range(len(self.nn_layers)):\n",
    "            in_dim, out_dim = self.nn_layers[i]\n",
    "            (layers_1).append(nn.Linear(in_dim, out_dim))\n",
    "            if self.nn_batchnorm[i]:\n",
    "                (layers_1).append(nn.BatchNorm1d(out_dim))\n",
    "            (layers_1).append(self.nn_activation[i])\n",
    "\n",
    "        # Create 1 FFNN\n",
    "        self.nn = nn.Sequential(*(layers_1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (self.nn(x))\n",
    "\n",
    "    # We will use this to prevent calculating gradients wrt parameters of Classifier\n",
    "    def set_requires_grad(self, requires_grad):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = requires_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "un_CT_kQOMmI"
   },
   "outputs": [],
   "source": [
    "d_latent = 128\n",
    "\n",
    "#TRIAL_ 1 [43,211]\n",
    "nn_layers_4 = [[d_latent,d_latent],[d_latent,d_latent],[d_latent,75]]\n",
    "nn_batchnorm_4 = [True for _ in range(2)] + [False]\n",
    "nn_activation_4 = [nn.ReLU() for _ in range(2)] + [nn.Identity()]\n",
    "# We will use soft-max while doing the CE loss\n",
    "classifier_arch = [nn_layers_4, nn_batchnorm_4, nn_activation_4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hD0ROnzeUo7Y"
   },
   "outputs": [],
   "source": [
    "Classifier_check = Classifier_MLP(classifier_arch)\n",
    "r1 = torch.randn((7,d_latent))\n",
    "r2 = Classifier_check(r1)\n",
    "print(r2.size())\n",
    "TP1 = sum(p.numel() for p in Classifier_check.parameters())\n",
    "print(TP1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qmwV5q1qUs-W"
   },
   "outputs": [],
   "source": [
    "# Initialize model, loss function, and optimizer\n",
    "lr_NN, m = 1e-4, 64\n",
    "C = Classifier_MLP(classifier_arch).to(device)\n",
    "CE_loss = []\n",
    "C_params = list(C.parameters())\n",
    "C_optimizer = optim.Adam(C_params, lr=lr_NN, betas=(0.8, 0.98), eps=1e-8, weight_decay=0)\n",
    "E_c = torch.load(\"/home/sahapthank/saha_adrl/E_1f_New_400.pth\").to(device)\n",
    "\n",
    "# Since butterfly images test set has no labels we create a random split over training\n",
    "all_indices = set(range(6499))\n",
    "train_split = set(random.sample(range(6499), 5000))\n",
    "test_split = list(all_indices - train_split)\n",
    "train_split = list(train_split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5EipPBPUU43Y"
   },
   "outputs": [],
   "source": [
    "# We train NN on z which are latents of the Encoder\n",
    "num_epochs = 10\n",
    "mini_batch_epochs = 77\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for _ in range(mini_batch_epochs):\n",
    "        E_c.set_requires_grad(False)\n",
    "        C.set_requires_grad(True)\n",
    "        C_optimizer.zero_grad()\n",
    "\n",
    "        j_indices = torch.randint(0,5000, (m,)).tolist()\n",
    "        i_indices = [train_split[t] for t in j_indices]\n",
    "        sampled_images = []\n",
    "        Labels = []\n",
    "        for i in i_indices:\n",
    "            sampled_images.append(butterfly_images[i])\n",
    "            Labels.append(butterfly_labels[i])\n",
    "\n",
    "        x = torch.stack(sampled_images).to(device)\n",
    "        Labels = torch.tensor(Labels).to(device)\n",
    "        mu_x, log_sigma_squared_x = E_c(x)\n",
    "        sigma_squared_x = torch.exp(log_sigma_squared_x)\n",
    "        sigma_x = torch.sqrt(sigma_squared_x)\n",
    "        z = (reparam_Gaussian(mu_x, sigma_x, 1)).squeeze(1).to(device)\n",
    "        y = C(z)\n",
    "\n",
    "        loss = criterion(y,Labels)\n",
    "        loss.backward()\n",
    "        C_optimizer.step()\n",
    "        C_optimizer.zero_grad()\n",
    "\n",
    "    CE_loss.append(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n5Jo0-hFVO41"
   },
   "outputs": [],
   "source": [
    "# Plot the losses\n",
    "plt.plot(CE_loss, label='Cross-Entropy Loss')\n",
    "# Adding titles and labels\n",
    "plt.title('Cross-Entropy Loss Over Time')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oKtibbQMVUdR"
   },
   "outputs": [],
   "source": [
    "# Determine the final classification accuracy on TRAINING SET\n",
    "batch_size = 64\n",
    "Correct = 0\n",
    "\n",
    "# We iterate through in batches\n",
    "# Due to BN layers bs must be > 1\n",
    "for i in range(0, 5000, batch_size):\n",
    "    with torch.no_grad():\n",
    "        # Create batches of butterfly_images and labels\n",
    "        batch_indices = train_split[i:i+batch_size]\n",
    "        batch_images = butterfly_images[batch_indices].to(device)\n",
    "        batch_labels = butterfly_labels[batch_indices].to(device)\n",
    "\n",
    "        # Forward pass through Encoder\n",
    "        mu_x, log_sigma_squared_x = E_c(batch_images)\n",
    "        sigma_squared_x = torch.exp(log_sigma_squared_x)\n",
    "        sigma_x = torch.sqrt(sigma_squared_x)\n",
    "        z = (reparam_Gaussian(mu_x, sigma_x, 1)).squeeze(1).to(device)\n",
    "        y = C(z)\n",
    "\n",
    "        # Get predicted classes\n",
    "        predictions = torch.argmax(y, dim=1)\n",
    "        # Check how many predictions match the ground truth labels\n",
    "        Correct += (predictions == batch_labels).sum().item()\n",
    "\n",
    "# Final accuracy calculation\n",
    "accuracy = Correct * 100 / 5000\n",
    "print(\"Final Accuracy of MLP based on VAE Latents on TRAIN:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YOX-Q6M0VY05"
   },
   "outputs": [],
   "source": [
    "# Determine the final classification accuracy on TEST SET\n",
    "batch_size = 64\n",
    "Correct = 0\n",
    "\n",
    "# We iterate through in batches\n",
    "# Due to BN layers batch_size must be > 1\n",
    "for i in range(0, 1499, batch_size):\n",
    "    with torch.no_grad():\n",
    "        batch_indices = test_split[i:i+batch_size]\n",
    "        # Create batches of butterfly_images and labels\n",
    "        batch_images = butterfly_images[batch_indices].to(device)\n",
    "        batch_labels = butterfly_labels[batch_indices].to(device)\n",
    "\n",
    "        # Forward pass through Encoder\n",
    "        mu_x, log_sigma_squared_x = E_c(batch_images)\n",
    "        sigma_squared_x = torch.exp(log_sigma_squared_x)\n",
    "        sigma_x = torch.sqrt(sigma_squared_x)\n",
    "        z = (reparam_Gaussian(mu_x, sigma_x, 1)).squeeze(1).to(device)\n",
    "        y = C(z)\n",
    "\n",
    "        # Get predicted classes\n",
    "        predictions = torch.argmax(y, dim=1)\n",
    "        # Check how many predictions match the ground truth labels\n",
    "        Correct += (predictions == batch_labels).sum().item()\n",
    "\n",
    "# Final accuracy calculation\n",
    "accuracy = Correct * 100 / 1499\n",
    "print(\"Final Accuracy of MLP based on VAE Latents on TEST:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rfv16J7bbg0V"
   },
   "source": [
    "**Accuracy of a simple MLP [3-layers] on VAE Latents**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tm9bcJ9nXwqh"
   },
   "source": [
    "The VAE latents were not that good. But even with 1/4 parameters of the CNN , accuracy is reasonable highlighting the ability of VAE to compress information.\n",
    "* Final Accuracy of MLP based on VAE Latents on TRAIN: 62.48%\n",
    "* Final Accuracy of MLP based on VAE Latents on TEST: 32.68%\n",
    "* #MLP parameters  = 43,211\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MXxHqds-Imle"
   },
   "source": [
    "**[Q7 and Q8] VQ-VAE Implementation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PzFqINPw5-Rn"
   },
   "source": [
    "**Some implementation Observations**\n",
    "* In VQ-VAE training we vaey the architecture and the beta value.\n",
    "* Also as expected uniform sampling for generation does not give any good images! Fitting GMM improved it but still image quality is bad [this might be due to fitting using only diagonal covariances]. Fitting more gaussian components on the entire latents increased recognisable images but it seems the entire space is not mapped.\n",
    "* This could be due to using lot of embedding vectors and small encoer having only 40k parameters. So we change K,D and experiment.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XljfoC5PtIol"
   },
   "outputs": [],
   "source": [
    "# VQ-VAE is the SoTa lets see if we can do it properly!\n",
    "# Learning useful representations without supervision remains a key challenge in ML\n",
    "# (VQ-VAE) differs from VAEs in two key ways:\n",
    "# 1) encoder network outputs discrete, rather than continuous, codes; and the prior\n",
    "# 2) is learnt rather than static.\n",
    "# Using the VQ method allows the\n",
    "# model to circumvent issues of “posterior collapse” — where the latents are ignored\n",
    "# when they are paired with a powerful autoregressive decoder — typically observed in the VAE framework.\n",
    "\n",
    "# For speech discrete encodings k are 1D and each z_q corresponds to e_q in R^d\n",
    "# For images (d1,d2) 2D encoding where each (vector along pixel) gets mapped to an embedding vector\n",
    "# There exist K embedding vectors!\n",
    "# Basically a CNN is used to convert to z_e(x) = (D,d1,d2) and each {z_e(x)}i,j as a D-dim vectors\n",
    "# We use nearest neighbours and map it to e_i in R^D!\n",
    "# We now get z_q(x) = (D,d1,d2) which using a Transpose_CNN we get final image\n",
    "\n",
    "# Note that there is no real gradient defined for discretisation using nearest neighbour\n",
    "# however we approximate the gradient similar to the straight-through estimator\n",
    "# and just copy gradients from decoder input z_q(x) to encoder output z_e(x)\n",
    "# Since the output representation of the encoder and the input to the decoder share the same D dimensional space,\n",
    "# the gradients contain useful information for how the encoder has to change its output\n",
    "\n",
    "class VQ_VAE_Encoder_0(nn.Module):\n",
    "    def __init__(self, architecture):\n",
    "        # Has only Convolutions, architecture = [cnn_architecture]\n",
    "        # cnn_architecture = [conv_params, cnn_batchnorm, cnn_activation_fn]\n",
    "        # conv_params = [[in_channels, output_channels, kernel_size, stride, padding], ...]\n",
    "        # cnn_batchnorm = [False, True, ...]\n",
    "        # cnn_activation = [nn.ReLU(), nn.LeakyReLU(), ...]\n",
    "\n",
    "        super(VQ_VAE_Encoder_0, self).__init__()\n",
    "        assert len(architecture[0]) == len(architecture[1]) == len(architecture[2])\n",
    "        self.conv_params = architecture[0]\n",
    "        self.cnn_batchnorm = architecture[1]\n",
    "        self.cnn_activation = architecture[2]\n",
    "\n",
    "        layers_0 = []\n",
    "        for j,i in enumerate(self.conv_params):\n",
    "            (layers_0).append(nn.Conv2d(in_channels=i[0],out_channels=i[1],kernel_size=i[2],stride=i[3],padding=i[4]))\n",
    "            if (self.cnn_batchnorm)[j]:\n",
    "                (layers_0).append(nn.BatchNorm2d(i[1]))\n",
    "            (layers_0).append(self.cnn_activation[j])\n",
    "\n",
    "        # Stack convolutional layers in a Sequential block\n",
    "        self.cnn = nn.Sequential(*(layers_0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        z_e = self.cnn(x)\n",
    "        return z_e\n",
    "\n",
    "    # We will use this to prevent calculating gradients wrt parameters of Encoder\n",
    "    def set_requires_grad(self, requires_grad):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = requires_grad\n",
    "\n",
    "class VQ_VAE_Decoder_0(nn.Module):\n",
    "    def __init__(self, architecture):\n",
    "        # The architecture is almost same as DCGAN using transpose_convolutions only\n",
    "\n",
    "        super(VQ_VAE_Decoder_0, self).__init__()\n",
    "        assert len(architecture[0]) == len(architecture[1]) == len(architecture[2])\n",
    "        self.transpose_conv_params = architecture[0]\n",
    "        self.use_batchnorm = architecture[1]\n",
    "        self.activation_fn = architecture[2]\n",
    "\n",
    "        layers = []\n",
    "        # Starting with input latent vector of 2D image!\n",
    "        for j, i in enumerate(self.transpose_conv_params):\n",
    "            layers.append(nn.ConvTranspose2d(in_channels=i[0], out_channels=i[1], kernel_size=i[2], stride=i[3], padding=i[4]))\n",
    "            if self.use_batchnorm[j]:\n",
    "                layers.append(nn.BatchNorm2d(i[1]))\n",
    "            layers.append(self.activation_fn[j])\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    # Sampling images from Decoder\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    # We will use this to prevent calculating gradients wrt parameters of Decoder\n",
    "    def set_requires_grad(self, requires_grad):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = requires_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PXe4LWYKtIsu"
   },
   "outputs": [],
   "source": [
    "# Since we assume a uniform prior for z, the KL term that usually appears in the ELBO is constant\n",
    "# w.r.t. the encoder parameters and can thus be ignored for training\n",
    "# Note that in VAE KL is only used for encoder training!\n",
    "# VQVAE has 3 loss terms for the reconstruction\n",
    "# A = logp(x|z_q(x)) := reconstruction {Encoder and Decoder}\n",
    "# B = l2 ((sg(z_e(x)),e)) := VQ Objective {Embedding Vectors only}\n",
    "# C =  beta * l2 ((z_e(x),sg(e))) := CommitmentLoss {Encoder only}\n",
    "\n",
    "# When we use N = k1*k2 dicrete latents (In paper they use (32,32) for ImageNet)\n",
    "# The loss terms are averaged for each latent (1/N factor)\n",
    "# One term for each latent arises in {B,C}\n",
    "# Whilst training the VQ-VAE, the prior is kept constant and uniform.\n",
    "# Each region of the image (represented by a latent code) has an equal probability\n",
    "# of being encoded by any of the embedding vectors from the codebook.\n",
    "\n",
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_loss_beta):\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "        # Codebook containing K embedding vectors\n",
    "        # Each vector is of dim D\n",
    "        # D would be the final #output channels in CNN\n",
    "        # Underscore is used for inplace operation\n",
    "        # Initialize with unform weights from [-1/K,1/K]\n",
    "        self.K = num_embeddings\n",
    "        self.D = embedding_dim\n",
    "        self.embedding = nn.Embedding(self.K, self.D) #(K,D)\n",
    "        self.embedding.weight.data.uniform_(-1/self.K, 1/self.K)\n",
    "        self.beta = commitment_loss_beta\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # convert inputs z_e(x) from (bs,c,h,w) to (bs,h,w,c)\n",
    "        inputs = inputs.permute(0, 2, 3, 1).contiguous()\n",
    "        input_shape = inputs.shape\n",
    "        # Flatten input to (bs*h*w,c=D)\n",
    "        flat_input = inputs.view(-1, self.D)\n",
    "        # Calculate distances of {each pixel_vector with each embedding vector}\n",
    "        # Final shape is (bs*h*w = N,K)\n",
    "        distances = (torch.sum(flat_input**2, dim=1, keepdim=True)\n",
    "                    + (torch.sum((self.embedding.weight)**2, dim=1))\n",
    "                    - (2 * torch.matmul(flat_input, self.embedding.weight.t())))\n",
    "\n",
    "        # Encoding\n",
    "        # Using one-hot encodings allows gradients to flow correctly during backpropagation.\n",
    "        # If we directly assign embeddings based on indices\n",
    "        # the computation graph might not accurately reflect the necessary operations for gradient calculation\n",
    "        # especially when dealing with operations like straight-through estimators\n",
    "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1) #(N,1)\n",
    "        encodings = torch.zeros(encoding_indices.shape[0], self.K, device=inputs.device)\n",
    "        # Keep shape same but put ones in the appropriate positions and 0-elsewhere\n",
    "        encodings.scatter_(1,encoding_indices,1) #(N,K)\n",
    "\n",
    "        # Quantize to (N,D) and unflatten to (bs,h,w,c)\n",
    "        quantized = torch.matmul(encodings, self.embedding.weight).view(input_shape)\n",
    "\n",
    "        # Calculate the Loss associated\n",
    "        # Averaging over {batch and EmbeddingVectors is done by default}\n",
    "        # MSE btw {z_e(x),sg(e)}\n",
    "        commitment_loss = F.mse_loss(quantized.detach(),inputs)\n",
    "        # MSE btw {sg(z_e(x)),e}\n",
    "        vq_objective = F.mse_loss(quantized,inputs.detach())\n",
    "        loss_B_C = (vq_objective) + (self.beta * commitment_loss)\n",
    "\n",
    "        # Trick for straight-through estimation\n",
    "        quantized = inputs + (quantized - inputs).detach()\n",
    "\n",
    "        # convert quantized back to (bs,c,h,w) and return (Loss,z_q(x)),B_loss,C_loss)\n",
    "        return (loss_B_C, quantized.permute(0, 3, 1, 2).contiguous(),commitment_loss.detach(),vq_objective.detach())\n",
    "\n",
    "    # We will use this to prevent calculating gradients wrt parameters of VQ\n",
    "    def set_requires_grad(self, requires_grad):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = requires_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Gvw8x7TGs-z"
   },
   "outputs": [],
   "source": [
    "# Formula for CNN convolutions\n",
    "# output_size = ({input_size - kernel_size + 2*padding}/stride) + 1\n",
    "# Formula for Transpose Conv\n",
    "# output_size = {(input_size - 1) * stride} - (2*padding) + kernel_size}\n",
    "\n",
    "# # TRIAL 1 [E = 43,488 V = 32,768 D = 853,315]\n",
    "# conv_params_2 = [[3,16,4,2,1],[16,16,1,1,0],[16,32,4,2,1],[32,32,1,1,0],[32,64,4,2,1]]\n",
    "# # CONVERT TO [64,16,16] so {D = 64}\n",
    "# cnn_batchnorm_2 = [True for _ in range(5)]\n",
    "# cnn_activation_2 = [nn.LeakyReLU(0.2, inplace=False) for _ in range(5)]\n",
    "# vq_E_arch_2 = [conv_params_2, cnn_batchnorm_2, cnn_activation_2]\n",
    "# transpose_conv_params_2 = [[64,256,4,2,1],[256,128,4,2,1],[128,32,4,2,1],[32,3,1,1,0]]\n",
    "# use_batchnorm_2 = [True for _ in range(3)] + [False]\n",
    "# activation_fn_2 = [nn.ReLU(inplace = False) for _ in range(3)] + [nn.Sigmoid()]\n",
    "# vq_D_arch_2 = [transpose_conv_params_2, use_batchnorm_2, activation_fn_2]\n",
    "\n",
    "# TRIAL 2 [E = 47,776 V = 32,768 D = 937,859]\n",
    "# Somebody please add this using different architectures to try...it is boring!\n",
    "conv_params_2 = [[3,16,4,2,1],[16,16,1,1,0],[16,32,4,2,1],[32,32,1,1,0],[32,64,4,2,1],[64,64,1,1,0]]\n",
    "# CONVERT TO [64,16,16] so D = 64\n",
    "cnn_batchnorm_2 = [True for _ in range(6)]\n",
    "cnn_activation_2 = [nn.LeakyReLU(0.2, inplace=False) for _ in range(6)]\n",
    "vq_E_arch_2 = [conv_params_2, cnn_batchnorm_2, cnn_activation_2]\n",
    "transpose_conv_params_2 = [[64,256,4,2,1],[256,128,4,2,1],[128,128,1,1,0],[128,64,4,2,1],[64,32,1,1,0],[32,3,1,1,0]]\n",
    "use_batchnorm_2 = [True for _ in range(5)] + [False]\n",
    "activation_fn_2 = [nn.ReLU(inplace = False) for _ in range(5)] + [nn.Sigmoid()]\n",
    "vq_D_arch_2 = [transpose_conv_params_2, use_batchnorm_2, activation_fn_2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "97tHoHt-GvjI"
   },
   "outputs": [],
   "source": [
    "# Rough checking if code so far behaves as expected\n",
    "E_check = VQ_VAE_Encoder_0(vq_E_arch_2)\n",
    "D_check = VQ_VAE_Decoder_0(vq_D_arch_2)\n",
    "V_check = VectorQuantizer(512,64,2)\n",
    "r1 = torch.randn((7,3,128,128))\n",
    "r2 = E_check(r1)\n",
    "print(r2.size())\n",
    "r4 = torch.randn((7,64,16,16))\n",
    "r5 = D_check(r4)\n",
    "print(r5.size())\n",
    "print(V_check(r4)[1].size())\n",
    "TP1 = sum(p.numel() for p in E_check.parameters())\n",
    "TP2 = sum(p.numel() for p in V_check.parameters())\n",
    "TP3 = sum(p.numel() for p in D_check.parameters())\n",
    "print(TP1,TP2,TP3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TGiw8rcSGxaf"
   },
   "outputs": [],
   "source": [
    "# Initialize model, loss function, and optimizer\n",
    "lr_vq_VAE, m = 1e-4, 64\n",
    "beta = 2\n",
    "E_arch = vq_E_arch_2\n",
    "D_arch = vq_D_arch_2\n",
    "E = VQ_VAE_Encoder_0(E_arch).to(device)\n",
    "D = VQ_VAE_Decoder_0(D_arch).to(device)\n",
    "vq = VectorQuantizer(512,64,beta).to(device)\n",
    "A_loss = []\n",
    "B_loss = []\n",
    "C_loss = []\n",
    "vq_loss = []\n",
    "vae_params = list(E.parameters()) + list(D.parameters()) + list(vq.parameters())\n",
    "vae_optimizer = optim.Adam(vae_params, lr=lr_vq_VAE, betas=(0.8, 0.98), eps=1e-8, weight_decay=0)\n",
    "# vae_optimizer = optim.RMSprop(vae_params, lr=lr_VAE, alpha=0.99, eps=1e-8, weight_decay=0)\n",
    "# vae_optimizer = optim.SGD(vae_params, lr=lr_VAE, momentum=0.9, weight_decay=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B4kz25gk3FGG"
   },
   "outputs": [],
   "source": [
    "# For manual changing during training\n",
    "lr_vq_VAE, m = 1e-4, 64\n",
    "vq.beta = 2\n",
    "vae_optimizer = optim.Adam(vae_params, lr=lr_vq_VAE, betas=(0.8, 0.98), eps=1e-8, weight_decay=0)\n",
    "# vae_optimizer = optim.RMSprop(vae_params, lr=lr_VAE, alpha=0.99, eps=1e-8, weight_decay=0)\n",
    "# vae_optimizer = optim.SGD(vae_params, lr=lr_VAE, momentum=0.9, weight_decay=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xEIwzqQ2G18i"
   },
   "outputs": [],
   "source": [
    "# VQ VAE training algorithm\n",
    "num_epochs = 900\n",
    "mini_batch_epochs = 100\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for _ in range(mini_batch_epochs):\n",
    "        # start = time.time()\n",
    "        D.set_requires_grad(True)\n",
    "        E.set_requires_grad(True)\n",
    "        vq.set_requires_grad(True)\n",
    "        vae_optimizer.zero_grad()\n",
    "\n",
    "        x = sample_real_butterfly(m).to(device)\n",
    "        z_e = E(x)\n",
    "        loss_B_C , z_q , C, B = vq(z_e)\n",
    "        y = D(z_q)\n",
    "        reconstruction_loss = (F.mse_loss(x,y))\n",
    "        # Averaging over {batch+dimension} done by default\n",
    "\n",
    "        loss_vq_VAE = (loss_B_C + reconstruction_loss)\n",
    "        loss_vq_VAE.backward()\n",
    "        vae_optimizer.step()\n",
    "        vae_optimizer.zero_grad()\n",
    "\n",
    "    A_loss.append(reconstruction_loss.detach().cpu())\n",
    "    B_loss.append(B.cpu())\n",
    "    C_loss.append(C.cpu())\n",
    "    vq_loss.append(loss_vq_VAE.detach().cpu())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ywHEwbsG4E9"
   },
   "outputs": [],
   "source": [
    "# Create a figure with three subplots (1 row, 3 columns)\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(18, 6))\n",
    "\n",
    "# Plot A_loss in the first subplot\n",
    "A_steps = range(len(A_loss))\n",
    "ax1.plot(A_steps, A_loss, label='A_loss', color='blue')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('A')\n",
    "ax1.set_title('Reconstruction_Loss')\n",
    "ax1.legend()\n",
    "# Plot B_loss in the second subplot\n",
    "B_steps = range(len(B_loss))\n",
    "ax2.plot(B_steps, B_loss, label='B_loss', color='orange')\n",
    "ax2.set_xlabel('Epochs')\n",
    "ax2.set_ylabel('B')\n",
    "ax2.set_title('VQ_Objective')\n",
    "ax2.legend()\n",
    "# Plot C_loss in the third subplot\n",
    "vae_steps = range(len(C_loss))\n",
    "ax3.plot(vae_steps, C_loss, label='C_loss', color='green')\n",
    "ax3.set_xlabel('Epochs')\n",
    "ax3.set_ylabel('C')\n",
    "ax3.set_title('Commitment_Loss')\n",
    "ax3.legend()\n",
    "# Plot vq_loss in the fourth subplot\n",
    "vae_steps = range(len(vq_loss))\n",
    "ax4.plot(vae_steps, vq_loss, label='vq_loss', color='black')\n",
    "ax4.set_xlabel('Epochs')\n",
    "ax4.set_ylabel('Overall')\n",
    "ax4.set_title('vq_Loss')\n",
    "ax4.legend()\n",
    "\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qO0iPg6EG4uf"
   },
   "outputs": [],
   "source": [
    "# Plotting reconstructed images when fed with training images\n",
    "s = 100\n",
    "with torch.no_grad():\n",
    "    x = sample_real_butterfly(s).to(device)\n",
    "    z_e = E(x)\n",
    "    z_q = vq(z_e)[1]\n",
    "    y = D(z_q)\n",
    "\n",
    "# Assuming both sets have the same number of images\n",
    "num_images = min(x.size(0), y.size(0))\n",
    "images_0 = x.detach().cpu().numpy()\n",
    "images_1 = y.detach().cpu().numpy()\n",
    "# Create a figure for the grid of images\n",
    "fig, axes = plt.subplots(nrows=num_images//10, ncols=20, figsize=(30, 15))\n",
    "\n",
    "# Loop through the images and display them side by side\n",
    "for i in range(num_images):\n",
    "    # Show the first set of images\n",
    "    img_0 = images_0[i].transpose(1, 2, 0)\n",
    "    axes[i // 10, (i % 10) * 2].imshow(img_0)\n",
    "    axes[i // 10, (i % 10) * 2].axis('off')\n",
    "    # Show the second set of images\n",
    "    img_1 = images_1[i].transpose(1, 2, 0)\n",
    "    axes[i // 10, (i % 10) * 2 + 1].imshow(img_1)\n",
    "    axes[i // 10, (i % 10) * 2 + 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hb8HlvLbt7Iz"
   },
   "outputs": [],
   "source": [
    "# Inference M1 using random [assuming uniform prior]\n",
    "s = 100\n",
    "K = 512\n",
    "with torch.no_grad():\n",
    "    shape = (s,16,16,64)\n",
    "    random_indices = torch.randint(0,K,(s*16*16,1)).to(device) #(N,1)\n",
    "    # Now we need to map to embedding vectors\n",
    "    # We use the previous logic of matmul\n",
    "    random_encodings = torch.zeros(random_indices.shape[0],K,device=device)\n",
    "    random_encodings.scatter_(1,random_indices,1) #(N,K)\n",
    "    random_quantized = torch.matmul(random_encodings, vq.embedding.weight).view(shape)\n",
    "    random_quantized = random_quantized.permute(0, 3, 1, 2).contiguous()\n",
    "    # print(random_quantized.shape)\n",
    "    generated_images = D(random_quantized)\n",
    "\n",
    "    images = generated_images.detach().cpu().numpy()\n",
    "    # Create a figure for the grid of images\n",
    "    fig, axes = plt.subplots(nrows=10, ncols=10, figsize=(15, 15))\n",
    "    # Loop through the 100 images and display them in the grid\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        img = images[i].transpose(1, 2, 0)\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QSN4Gt3nwxnw"
   },
   "outputs": [],
   "source": [
    "# Inference M2 using GMM [Fit GMM on entire latent space of real_images]\n",
    "# We treat latent vectors coming from GMM to be in space of dimension [64*16*16]\n",
    "# E = torch.load(\"A2_weights/E_2a_100.pth\").to(device)\n",
    "# D = torch.load(\"A2_weights/D_2a_100.pth\").to(device)\n",
    "# vq = torch.load(\"A2_weights/vq_2a_100.pth\").to(device)\n",
    "all_latent_vectors = torch.zeros(6499,64,16,16)\n",
    "batch_size = 64\n",
    "for i in range(0, 6499, batch_size):\n",
    "    with torch.no_grad():\n",
    "        x = butterfly_images[i:min(i+batch_size,6499)].to(device)\n",
    "        z_e = E(x)\n",
    "        z_q = vq(z_e)[1]\n",
    "        # Calculate the end index for slicing\n",
    "        end_index = min(i + batch_size, 6499)\n",
    "        # Insert the batch of latents into the correct slice of T\n",
    "        all_latent_vectors[i:end_index] = z_q\n",
    "all_latent_vectors = all_latent_vectors.view(-1,64*16*16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5zHk7fKzwz8Y"
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "# Fit the GMM and generate images\n",
    "gmm = GaussianMixture(n_components=256, covariance_type='diag',max_iter=100,verbose=2, verbose_interval=1)\n",
    "gmm.fit(all_latent_vectors.cpu().numpy())\n",
    "# Access convergence results\n",
    "print(\"Converged:\", gmm.converged_)\n",
    "print(\"Number of iterations:\", gmm.n_iter_)\n",
    "print(\"Log-Likelihood:\", gmm.score(all_latent_vectors.cpu().numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LdK9dEMyxC7B"
   },
   "outputs": [],
   "source": [
    "s = 100\n",
    "random_quantized, _ = gmm.sample(s)\n",
    "random_quantized = (torch.from_numpy(random_quantized.reshape(-1, 64, 16, 16)).float()).to(device)\n",
    "with torch.no_grad():\n",
    "    generated_images = D(random_quantized)\n",
    "    images = generated_images.detach().cpu().numpy()\n",
    "    # Create a figure for the grid of images\n",
    "    fig, axes = plt.subplots(nrows=10, ncols=10, figsize=(15, 15))\n",
    "    # Loop through the 100 images and display them in the grid\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        img = images[i].transpose(1, 2, 0)\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E43bZiKJEERh"
   },
   "source": [
    "**Classifying the images using the learned latents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2VIESzxxEKF-"
   },
   "outputs": [],
   "source": [
    "# A [CNN + NN] for classification\n",
    "class Classifier_0(nn.Module):\n",
    "    def __init__(self, architecture):\n",
    "        # architecture = [cnn_architecture, nn_architecute]\n",
    "        # nn_architecture = [nn_layers, nn_batch_norm , nn_activation]\n",
    "        # cnn_architecture = [conv_params, cnn_batchnorm, cnn_activation_fn]\n",
    "        # conv_params = [[in_channels, output_channels, kernel_size, stride, padding], ...]\n",
    "        # nn/cnn_batchnorm = [False, True, ...]\n",
    "        # nn/cnn_activation = [nn.ReLU(), nn.LeakyReLU(), ...]\n",
    "\n",
    "        super(Classifier_0, self).__init__()\n",
    "        assert len(architecture[0][0]) == len(architecture[0][1]) == len(architecture[0][2])\n",
    "        assert len(architecture[1][0]) == len(architecture[1][1]) == len(architecture[1][2])\n",
    "        self.conv_params = architecture[0][0]\n",
    "        self.cnn_batchnorm = architecture[0][1]\n",
    "        self.cnn_activation = architecture[0][2]\n",
    "        self.nn_layers = architecture[1][0]\n",
    "        self.nn_batchnorm = architecture[1][1]\n",
    "        self.nn_activation = architecture[1][2]\n",
    "\n",
    "        layers_0 = []\n",
    "        for j,i in enumerate(self.conv_params):\n",
    "            (layers_0).append(nn.Conv2d(in_channels=i[0],out_channels=i[1],kernel_size=i[2],stride=i[3],padding=i[4]))\n",
    "            if (self.cnn_batchnorm)[j]:\n",
    "                (layers_0).append(nn.BatchNorm2d(i[1]))\n",
    "            (layers_0).append(self.cnn_activation[j])\n",
    "\n",
    "        layers_1 = []\n",
    "        for i in range(len(self.nn_layers)):\n",
    "            in_dim, out_dim = self.nn_layers[i]\n",
    "            (layers_1).append(nn.Linear(in_dim, out_dim))\n",
    "            if self.nn_batchnorm[i]:\n",
    "                (layers_1).append(nn.BatchNorm1d(out_dim))\n",
    "            (layers_1).append(self.nn_activation[i])\n",
    "\n",
    "        # Stack convolutional layers in a Sequential block\n",
    "        # Create 1 FFNN\n",
    "        self.cnn = nn.Sequential(*(layers_0))\n",
    "        self.nn = nn.Sequential(*(layers_1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.cnn(x)\n",
    "        features = torch.flatten(features, start_dim=1)\n",
    "        logits = self.nn(features)\n",
    "        return logits\n",
    "\n",
    "    # We will use this to prevent calculating gradients wrt parameters of Classifier_0\n",
    "    def set_requires_grad(self, requires_grad):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = requires_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aQ6NyxRNJidP"
   },
   "outputs": [],
   "source": [
    "# Formula for CNN convolutions\n",
    "# output_size = ({input_size - kernel_size + 2*padding}/stride) + 1\n",
    "\n",
    "# TRIAL 1\n",
    "conv_params_4 = [[64,64,1,1,0],[64,64,4,2,1],[64,128,4,2,1]]\n",
    "# CONVERT from [64,16,16] = 16,384 to [128,4,4] = 2048\n",
    "cnn_batchnorm_4 = [True for _ in range(3)]\n",
    "cnn_activation_4 = [nn.LeakyReLU(0.2, inplace=False) for _ in range(3)]\n",
    "classifier_0_cnn = [conv_params_4, cnn_batchnorm_4, cnn_activation_4]\n",
    "nn_layers_4 = [[2048,75]]\n",
    "nn_batchnorm_4 = [True for _ in range(0)] + [False]\n",
    "nn_activation_4 = [nn.ReLU() for _ in range(0)] + [nn.Identity()]\n",
    "# We will use soft-max while doing the CE loss\n",
    "classifier_0_nn = [nn_layers_4, nn_batchnorm_4, nn_activation_4]\n",
    "classifier_0_arch = [classifier_0_cnn , classifier_0_nn]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ie_mw5sTJjaG"
   },
   "outputs": [],
   "source": [
    "Classifier_check = Classifier_0(classifier_0_arch)\n",
    "r1 = torch.randn((7,64,16,16))\n",
    "r2 = Classifier_check(r1)\n",
    "print(r2.size())\n",
    "TP1 = sum(p.numel() for p in Classifier_check.parameters())\n",
    "print(TP1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VbLTZu_1LWpH"
   },
   "outputs": [],
   "source": [
    "# Initialize model, loss function, and optimizer\n",
    "lr_CNN, m = 1e-4, 64\n",
    "C = Classifier_0(classifier_0_arch).to(device)\n",
    "CE_loss = []\n",
    "C_params = list(C.parameters())\n",
    "C_optimizer = optim.Adam(C_params, lr=lr_CNN, betas=(0.8, 0.98), eps=1e-8, weight_decay=0)\n",
    "E_c = torch.load(\"A2_weights/E_2b_700.pth\").to(device)\n",
    "D_c = torch.load(\"A2_weights/D_2b_700.pth\").to(device)\n",
    "vq_c = torch.load(\"A2_weights/vq_2b_700.pth\").to(device)\n",
    "\n",
    "# Since butterfly images test set has no labels we create a random split over training\n",
    "all_indices = set(range(6499))\n",
    "train_split = set(random.sample(range(6499), 5000))\n",
    "test_split = list(all_indices - train_split)\n",
    "train_split = list(train_split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_WS_AIiFEjKQ"
   },
   "outputs": [],
   "source": [
    "# We train CNN on z_q(x) of shape (64,16,16)/(1,16,16)\n",
    "num_epochs = 10\n",
    "mini_batch_epochs = 77\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for _ in range(mini_batch_epochs):\n",
    "        D_c.set_requires_grad(False)\n",
    "        E_c.set_requires_grad(False)\n",
    "        vq_c.set_requires_grad(False)\n",
    "        C.set_requires_grad(True)\n",
    "        C_optimizer.zero_grad()\n",
    "\n",
    "        j_indices = torch.randint(0,5000, (m,)).tolist()\n",
    "        i_indices = [train_split[t] for t in j_indices]\n",
    "        sampled_images = []\n",
    "        Labels = []\n",
    "        for i in i_indices:\n",
    "            sampled_images.append(butterfly_images[i])\n",
    "            Labels.append(butterfly_labels[i])\n",
    "\n",
    "        x = torch.stack(sampled_images).to(device)\n",
    "        Labels = torch.tensor(Labels).to(device)\n",
    "        z_e = E_c(x)\n",
    "        z_q = vq_c(z_e)[1]\n",
    "        y = C(z_q)\n",
    "\n",
    "        loss = criterion(y,Labels)\n",
    "        loss.backward()\n",
    "        C_optimizer.step()\n",
    "        C_optimizer.zero_grad()\n",
    "\n",
    "    CE_loss.append(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A-PFKBSBPnyd"
   },
   "outputs": [],
   "source": [
    "# Plot the losses\n",
    "plt.plot(CE_loss, label='Cross-Entropy Loss')\n",
    "# Adding titles and labels\n",
    "plt.title('Cross-Entropy Loss Over Time')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VKNmYY9mOF1q"
   },
   "outputs": [],
   "source": [
    "# Determine the final classification accuracy on TRAINING SET\n",
    "batch_size = 64\n",
    "Correct = 0\n",
    "\n",
    "# We iterate through in batches\n",
    "# Due to BN layers bs must be > 1\n",
    "for i in range(0, 5000, batch_size):\n",
    "    with torch.no_grad():\n",
    "        # Create batches of butterfly_images and labels\n",
    "        batch_indices = train_split[i:i+batch_size]\n",
    "        batch_images = butterfly_images[batch_indices].to(device)\n",
    "        batch_labels = butterfly_labels[batch_indices].to(device)\n",
    "        # Forward pass through the VQ-VAE encoder and quantizer\n",
    "        z_e = E_c(batch_images)\n",
    "        z_q = vq_c(z_e)[1]\n",
    "        # Classification using the quantized latents\n",
    "        y = C(z_q)\n",
    "        # Get predicted classes\n",
    "        predictions = torch.argmax(y, dim=1)\n",
    "        # Check how many predictions match the ground truth labels\n",
    "        Correct += (predictions == batch_labels).sum().item()\n",
    "\n",
    "# Final accuracy calculation\n",
    "accuracy = Correct * 100 / 5000\n",
    "print(\"Final Accuracy of CNN based on VQ-VAE Latents:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R-zGk_SWUymn"
   },
   "outputs": [],
   "source": [
    "# Determine the final classification accuracy on TEST SET\n",
    "batch_size = 64\n",
    "Correct = 0\n",
    "\n",
    "# We iterate through in batches\n",
    "# Due to BN layers batch_size must be > 1\n",
    "for i in range(0, 1499, batch_size):\n",
    "    with torch.no_grad():\n",
    "        batch_indices = test_split[i:i+batch_size]\n",
    "        # Create batches of butterfly_images and labels\n",
    "        batch_images = butterfly_images[batch_indices].to(device)\n",
    "        batch_labels = butterfly_labels[batch_indices].to(device)\n",
    "        z_e = E_c(batch_images)\n",
    "        z_q = vq_c(z_e)[1]\n",
    "        # Classification using the quantized latents\n",
    "        y = C(z_q)\n",
    "        # Get predicted classes\n",
    "        predictions = torch.argmax(y, dim=1)\n",
    "        # Check how many predictions match the ground truth labels\n",
    "        Correct += (predictions == batch_labels).sum().item()\n",
    "\n",
    "# Final accuracy calculation\n",
    "accuracy = Correct * 100 / 1499\n",
    "print(\"Final Accuracy of CNN on TEST:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-r9e11pcLGI"
   },
   "source": [
    "**Classification Accuracy of CNN trained on VQ-VAE Latents [64 x 16 x 16 as i/p]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5lSEeyJ8Wn1U"
   },
   "source": [
    "* After 25 epochs: Final Accuracy of CNN(VQ-VAE Latents) on TRAIN : 99.86\n",
    "* After 25 epochs: Final Accuracy of CNN(VQ-VAE Latents) on TEST : 51.63\n",
    "* #CNN Parameters : 355,147"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
